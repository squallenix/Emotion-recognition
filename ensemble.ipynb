{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4891fb26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Device: cuda\n",
      "=== TRAIN TEXT MODEL ===\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "CSV must have 'text' and 'label'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAssertionError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 665\u001b[39m\n\u001b[32m    662\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m[STACKER] ALIGN_CSV not provided -> skipping stacker training.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    664\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m665\u001b[39m     \u001b[43mrun_all\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 633\u001b[39m, in \u001b[36mrun_all\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    631\u001b[39m t0 = time.time()\n\u001b[32m    632\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=== TRAIN TEXT MODEL ===\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m633\u001b[39m text_res = \u001b[43mtrain_text_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    634\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=== TRAIN FACE MODEL ===\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    635\u001b[39m face_res = train_face_model()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 170\u001b[39m, in \u001b[36mtrain_text_model\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    169\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mtrain_text_model\u001b[39m():\n\u001b[32m--> \u001b[39m\u001b[32m170\u001b[39m     df = \u001b[43mload_csvs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCSV_PATHS\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    171\u001b[39m     le = LabelEncoder()\n\u001b[32m    172\u001b[39m     df[\u001b[33m\"\u001b[39m\u001b[33mlabel\u001b[39m\u001b[33m\"\u001b[39m] = le.fit_transform(df[\u001b[33m\"\u001b[39m\u001b[33mlabel\u001b[39m\u001b[33m\"\u001b[39m].astype(\u001b[38;5;28mstr\u001b[39m))\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 156\u001b[39m, in \u001b[36mload_csvs\u001b[39m\u001b[34m(paths)\u001b[39m\n\u001b[32m    152\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mNo CSV files found. Update CSV_PATHS.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    154\u001b[39m df = pd.concat(dfs, ignore_index=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m156\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m df.columns \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mlabel\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m df.columns, \\\n\u001b[32m    157\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mCSV must have \u001b[39m\u001b[33m'\u001b[39m\u001b[33mtext\u001b[39m\u001b[33m'\u001b[39m\u001b[33m and \u001b[39m\u001b[33m'\u001b[39m\u001b[33mlabel\u001b[39m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    159\u001b[39m df = df.dropna(subset=[\u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mlabel\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m    160\u001b[39m df[\u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m] = df[\u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m].astype(\u001b[38;5;28mstr\u001b[39m).str.strip()\n",
      "\u001b[31mAssertionError\u001b[39m: CSV must have 'text' and 'label'"
     ]
    }
   ],
   "source": [
    "# multimodal_ensemble_train.py\n",
    "\"\"\"\n",
    "Multimodal training + optional stacking meta-learner.\n",
    "\n",
    "Usage:\n",
    " - Edit paths under USER CONFIG.\n",
    " - Run: python multimodal_ensemble_train.py\n",
    "Notes:\n",
    " - If ALIGN_CSV is set to a CSV that contains aligned samples across modalities with\n",
    "   columns ['label','text','image_path','audio_feat_path'] then a stacking meta-learner\n",
    "   (logistic regression) will be trained after the base models using validation predictions.\n",
    " - If ALIGN_CSV is None (default) the script will only train/evaluate each base model.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import time\n",
    "import copy\n",
    "import random\n",
    "import pickle\n",
    "from typing import List, Optional, Tuple, Dict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, models\n",
    "from PIL import Image\n",
    "\n",
    "from transformers import BertTokenizer, BertModel, get_linear_schedule_with_warmup\n",
    "from torch.optim import AdamW\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, label_binarize\n",
    "from sklearn.metrics import confusion_matrix, roc_curve, auc, precision_score, recall_score, f1_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ------------------------\n",
    "# USER CONFIG (edit paths & hyperparams)\n",
    "# ------------------------\n",
    "# Text CSVs used by text model\n",
    "CSV_PATHS = [\n",
    "    r\"K:\\Code\\Project\\Research Paper\\Emotion Detection\\archive_text\\tweet_emotions.csv\",\n",
    "    r\"K:\\Code\\Project\\Research Paper\\Emotion Detection\\archive_text\\dataset2.csv\",\n",
    "    r\"K:\\Code\\Project\\Research Paper\\Emotion Detection\\archive_text\\dataset3_excel.csv\"\n",
    "]\n",
    "# Face data roots used by face model\n",
    "ROOT_DATA_DIRS = [ r\"K:\\Code\\Project\\Research Paper\\Emotion Detection\\facial_data\\archive\" ]\n",
    "# Speech cached features path used by speech model (cached_features.pkl expected)\n",
    "SPEECH_FEATURES_PKL = \"cached_features.pkl\"  # your file produced during speech training\n",
    "\n",
    "# Optional: an alignment CSV with rows that have label + modalities (for stacking)\n",
    "# required columns if using stacking: label, content, image_path, audio_feat_path\n",
    "ALIGN_CSV = r\"K:\\Code\\Project\\Research Paper\\Emotion Detection\\meld dataset\\MELD.Raw\\self\\fusion_dataset.csv\"  # Example: r\"K:\\...\\fusion_dataset_aligned.csv\"  (set to None to skip stacking)\n",
    "\n",
    "# Models / labels save paths\n",
    "TEXT_MODEL_NAME = \"bert-base-multilingual-cased\"\n",
    "TEXT_BEST_MODEL = r\"K:\\Code\\Project\\Research Paper\\Emotion Detection\\Code\\bert_emotion_text_final.pth\"\n",
    "TEXT_LABELS_NPY = r\"K:\\Code\\Project\\Research Paper\\Emotion Detection\\Code\\label_classes.npy\"\n",
    "\n",
    "FACE_MODEL_PATH = r\"K:\\Code\\Project\\Research Paper\\Emotion Detection\\inceptionresnetv3_face_emotion.pth\"\n",
    "FACE_LABELS_NPY = r\"K:\\Code\\Project\\Research Paper\\Emotion Detection\\emotions_face.npy\"\n",
    "\n",
    "SPEECH_MODEL_PATH = \"./best_transformer_speech_model.pth\"\n",
    "\n",
    "# Hyperparams\n",
    "SEED = 42\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "BATCH_SIZE = 64\n",
    "NUM_WORKERS = 0\n",
    "PIN_MEMORY = True\n",
    "\n",
    "# Text hyperparams\n",
    "MAX_LEN = 64\n",
    "TEXT_EPOCHS = 2\n",
    "TEXT_LR = 2e-5\n",
    "TEXT_BATCH = 64\n",
    "\n",
    "# Face hyperparams\n",
    "IMG_SIZE = 299\n",
    "FACE_EPOCHS = 2\n",
    "FACE_LR = 1e-4\n",
    "FACE_BATCH = 64\n",
    "\n",
    "# Speech hyperparams\n",
    "SPEECH_EPOCHS = 2\n",
    "SPEECH_LR = 1e-4\n",
    "SPEECH_BATCH = 64\n",
    "\n",
    "# Ensemble stacking config\n",
    "STACKER_SOLVER = 'liblinear'  # logistic regression solver (scikit-learn)\n",
    "STACKER_C = 1.0\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "print(f\"[INFO] Device: {DEVICE}\")\n",
    "\n",
    "# ===========================================\n",
    "# ================ TEXT PART ================\n",
    "# ===========================================\n",
    "from transformers import BertTokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(TEXT_MODEL_NAME, use_fast=True)\n",
    "\n",
    "class SimpleTextDataset(Dataset):\n",
    "    def __init__(self, texts: List[str], labels: List[int], tokenizer, max_len: int):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self): return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        txt = self.texts[idx]\n",
    "        enc = self.tokenizer(txt, padding=\"max_length\", truncation=True, max_length=self.max_len, return_tensors=\"pt\")\n",
    "        return {\n",
    "            \"input_ids\": enc[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask\": enc[\"attention_mask\"].squeeze(0),\n",
    "            \"labels\": torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        }\n",
    "\n",
    "class BERTClassifier(nn.Module):\n",
    "    def __init__(self, model_name: str, num_classes: int, dropout: float = 0.2):\n",
    "        super().__init__()\n",
    "        self.bert = BertModel.from_pretrained(model_name)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.classifier = nn.Linear(self.bert.config.hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        out = self.bert(input_ids=input_ids, attention_mask=attention_mask, return_dict=True)\n",
    "        pooled = out.pooler_output\n",
    "        x = self.dropout(pooled)\n",
    "        return self.classifier(x)\n",
    "\n",
    "def load_csvs(paths: List[str]) -> pd.DataFrame:\n",
    "    dfs = []\n",
    "    for p in paths:\n",
    "        if os.path.exists(p):\n",
    "            dfs.append(pd.read_csv(p))\n",
    "        else:\n",
    "            print(f\"[WARN] CSV missing, skipping: {p}\")\n",
    "\n",
    "    if not dfs:\n",
    "        raise RuntimeError(\"No CSV files found. Update CSV_PATHS.\")\n",
    "\n",
    "    df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "    assert \"content\" in df.columns and \"label\" in df.columns, \\\n",
    "        \"CSV must have 'text' and 'label'\"\n",
    "\n",
    "    df = df.dropna(subset=[\"content\", \"label\"])\n",
    "    df[\"content\"] = df[\"content\"].astype(str).str.strip()\n",
    "    df = df[df[\"content\"] != \"\"]\n",
    "\n",
    "    #  Drop label \"pray\"\n",
    "    df = df[df[\"label\"] != \"pray\"]\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def train_text_model():\n",
    "    df = load_csvs(CSV_PATHS)\n",
    "    le = LabelEncoder()\n",
    "    df[\"label\"] = le.fit_transform(df[\"label\"].astype(str))\n",
    "    texts = df[\"content\"].tolist()\n",
    "    labels = df[\"label\"].tolist()\n",
    "    class_names = list(le.classes_)\n",
    "    print(f\"[TEXT] Samples: {len(texts)} | Classes: {class_names}\")\n",
    "\n",
    "    np.save(TEXT_LABELS_NPY, class_names)\n",
    "\n",
    "    idxs = list(range(len(texts)))\n",
    "    train_idx, val_idx = train_test_split(idxs, test_size=0.2, random_state=SEED, stratify=labels)\n",
    "\n",
    "    train_texts = [texts[i] for i in train_idx]\n",
    "    train_labels = [labels[i] for i in train_idx]\n",
    "    val_texts = [texts[i] for i in val_idx]\n",
    "    val_labels = [labels[i] for i in val_idx]\n",
    "\n",
    "    train_ds = SimpleTextDataset(train_texts, train_labels, tokenizer, MAX_LEN)\n",
    "    val_ds = SimpleTextDataset(val_texts, val_labels, tokenizer, MAX_LEN)\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=TEXT_BATCH, shuffle=True, num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY)\n",
    "    val_loader = DataLoader(val_ds, batch_size=TEXT_BATCH, shuffle=False, num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY)\n",
    "\n",
    "    model = BERTClassifier(TEXT_MODEL_NAME, num_classes=len(class_names)).to(DEVICE)\n",
    "    optimizer = AdamW(model.parameters(), lr=TEXT_LR, weight_decay=1e-2)\n",
    "    total_steps = len(train_loader) * TEXT_EPOCHS\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, int(0.1 * total_steps), total_steps)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    best_val = -1.0\n",
    "    train_accs, val_accs = [], []\n",
    "    all_val_probs, all_val_targets = [], []\n",
    "\n",
    "    for epoch in range(1, TEXT_EPOCHS+1):\n",
    "        model.train()\n",
    "        running_correct = 0; running_total = 0\n",
    "        pbar = tqdm(train_loader, desc=f\"[TEXT] Epoch {epoch}/{TEXT_EPOCHS}\", leave=False)\n",
    "        for batch in pbar:\n",
    "            input_ids = batch[\"input_ids\"].to(DEVICE)\n",
    "            attention_mask = batch[\"attention_mask\"].to(DEVICE)\n",
    "            labels = batch[\"labels\"].to(DEVICE)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            out = model(input_ids, attention_mask)\n",
    "            loss = criterion(out, labels)\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            if scheduler: scheduler.step()\n",
    "\n",
    "            preds = out.argmax(dim=1)\n",
    "            running_total += labels.size(0)\n",
    "            running_correct += (preds == labels).sum().item()\n",
    "            pbar.set_postfix({\"TrainAcc\": f\"{100.0*running_correct/running_total:.2f}%\"})\n",
    "\n",
    "        epoch_train_acc = 100.0 * running_correct / running_total\n",
    "        train_accs.append(epoch_train_acc)\n",
    "\n",
    "        # validation\n",
    "        model.eval()\n",
    "        v_total = 0; v_correct = 0\n",
    "        val_probs, val_targets = [], []\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                input_ids = batch[\"input_ids\"].to(DEVICE)\n",
    "                attention_mask = batch[\"attention_mask\"].to(DEVICE)\n",
    "                labels = batch[\"labels\"].to(DEVICE)\n",
    "\n",
    "                out = model(input_ids, attention_mask)\n",
    "                probs = F.softmax(out, dim=1)\n",
    "                preds = probs.argmax(dim=1)\n",
    "                v_total += labels.size(0)\n",
    "                v_correct += (preds == labels).sum().item()\n",
    "                val_probs.append(probs.cpu().numpy()); val_targets.append(labels.cpu().numpy())\n",
    "\n",
    "        epoch_val_acc = 100.0 * v_correct / v_total\n",
    "        val_accs.append(epoch_val_acc)\n",
    "        print(f\"[TEXT] Epoch {epoch}/{TEXT_EPOCHS} → Train Acc: {epoch_train_acc:.2f}% | Val Acc: {epoch_val_acc:.2f}%\")\n",
    "\n",
    "        if epoch_val_acc > best_val:\n",
    "            best_val = epoch_val_acc\n",
    "            torch.save({'model_state': model.state_dict(), 'class_names': class_names}, TEXT_BEST_MODEL)\n",
    "            print(f\"[TEXT] Best model saved (Val Acc {best_val:.2f}%) -> {TEXT_BEST_MODEL}\")\n",
    "\n",
    "        all_val_probs.append(np.vstack(val_probs))\n",
    "        all_val_targets.append(np.concatenate(val_targets))\n",
    "\n",
    "    # prepare return\n",
    "    y_score = np.vstack(all_val_probs) if all_val_probs else np.array([])\n",
    "    y_true = np.concatenate(all_val_targets) if all_val_targets else np.array([])\n",
    "    # store training curves\n",
    "    pickle.dump({'train_acc': train_accs, 'val_acc': val_accs}, open(\"text_training_curves.pkl\",\"wb\"))\n",
    "    # final metrics\n",
    "    return {'model': model, 'best_val': best_val, 'y_score': y_score, 'y_true': y_true, 'classes': class_names}\n",
    "\n",
    "# ===========================================\n",
    "# ================ FACE PART ================\n",
    "# ===========================================\n",
    "class FaceEmotionDataset(Dataset):\n",
    "    def __init__(self, image_paths, labels, transform=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self): return len(self.image_paths)\n",
    "    def __getitem__(self, idx):\n",
    "        path = self.image_paths[idx]\n",
    "        label = self.labels[idx]\n",
    "        img = Image.open(path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            img_t = self.transform(img)\n",
    "        else:\n",
    "            img_t = transforms.ToTensor()(img)\n",
    "        return img_t, label\n",
    "\n",
    "class InceptionResNetV3_Emotion(nn.Module):\n",
    "    def __init__(self, num_classes=8, pretrained=True):\n",
    "        super().__init__()\n",
    "        base = models.inception_v3(weights=models.Inception_V3_Weights.IMAGENET1K_V1 if pretrained else None, aux_logits=True)\n",
    "        in_features = base.fc.in_features\n",
    "        base.fc = nn.Sequential(nn.Linear(in_features, 512), nn.ReLU(inplace=True), nn.Dropout(0.4), nn.Linear(512, num_classes))\n",
    "        if hasattr(base, 'AuxLogits'):\n",
    "            base.AuxLogits.fc = nn.Linear(base.AuxLogits.fc.in_features, num_classes)\n",
    "        self.base = base\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.base(x)\n",
    "        if isinstance(out, tuple):\n",
    "            return out[0]\n",
    "        return out\n",
    "\n",
    "def load_face_paths(root_dirs: List[str]):\n",
    "    image_paths, labels = [], []\n",
    "    emotions = set()\n",
    "    for root in root_dirs:\n",
    "        for subdir, _, files in os.walk(root):\n",
    "            imgs = [f for f in files if f.lower().endswith(('.jpg','.jpeg','.png'))]\n",
    "            if not imgs: continue\n",
    "            emotion = os.path.basename(subdir).lower()\n",
    "            emotions.add(emotion)\n",
    "            for f in imgs:\n",
    "                image_paths.append(os.path.join(subdir, f))\n",
    "                labels.append(emotion)\n",
    "    emotions = sorted(list(emotions))\n",
    "    idx_map = {e:i for i,e in enumerate(emotions)}\n",
    "    numeric_labels = [ idx_map[l] for l in labels ]\n",
    "    return image_paths, numeric_labels, emotions\n",
    "\n",
    "def train_face_model():\n",
    "    image_paths, numeric_labels, emotions = load_face_paths(ROOT_DATA_DIRS)\n",
    "    print(f\"[FACE] Found {len(image_paths)} images across {len(emotions)} emotions: {emotions}\")\n",
    "    np.save(FACE_LABELS_NPY, emotions)\n",
    "\n",
    "    X_train, X_val, y_train, y_val = train_test_split(image_paths, numeric_labels, test_size=0.3, random_state=SEED, stratify=numeric_labels)\n",
    "\n",
    "    preprocess = transforms.Compose([transforms.Resize((IMG_SIZE, IMG_SIZE)), transforms.ToTensor(),\n",
    "                                     transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])])\n",
    "    train_ds = FaceEmotionDataset(X_train, y_train, transform=preprocess)\n",
    "    val_ds = FaceEmotionDataset(X_val, y_val, transform=preprocess)\n",
    "    train_loader = DataLoader(train_ds, batch_size=FACE_BATCH, shuffle=True, num_workers=NUM_WORKERS)\n",
    "    val_loader = DataLoader(val_ds, batch_size=FACE_BATCH, shuffle=False, num_workers=NUM_WORKERS)\n",
    "\n",
    "    model = InceptionResNetV3_Emotion(num_classes=len(emotions)).to(DEVICE)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=FACE_LR)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    best_val = -1.0\n",
    "    train_acc_hist, val_acc_hist = [], []\n",
    "    all_val_probs, all_val_targets = [], []\n",
    "\n",
    "    for epoch in range(1, FACE_EPOCHS+1):\n",
    "        model.train()\n",
    "        running_correct, running_total = 0, 0\n",
    "        pbar = tqdm(train_loader, desc=f\"[FACE] Epoch {epoch}/{FACE_EPOCHS}\", leave=False)\n",
    "        for imgs, labels in pbar:\n",
    "            imgs, labels = imgs.to(DEVICE), labels.to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(imgs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            preds = outputs.argmax(dim=1)\n",
    "            running_total += labels.size(0)\n",
    "            running_correct += (preds == labels).sum().item()\n",
    "            pbar.set_postfix({\"TrainAcc\": f\"{100.0*running_correct/running_total:.2f}%\"})\n",
    "\n",
    "        epoch_train_acc = 100.0 * running_correct / running_total\n",
    "        train_acc_hist.append(epoch_train_acc)\n",
    "\n",
    "        # val\n",
    "        model.eval()\n",
    "        val_correct, val_total = 0, 0\n",
    "        val_probs, val_targets = [], []\n",
    "        with torch.no_grad():\n",
    "            for imgs, labels in val_loader:\n",
    "                imgs, labels = imgs.to(DEVICE), labels.to(DEVICE)\n",
    "                outputs = model(imgs)\n",
    "                probs = F.softmax(outputs, dim=1)\n",
    "                preds = probs.argmax(dim=1)\n",
    "                val_total += labels.size(0)\n",
    "                val_correct += (preds == labels).sum().item()\n",
    "                val_probs.append(probs.cpu().numpy()); val_targets.append(labels.cpu().numpy())\n",
    "\n",
    "        epoch_val_acc = 100.0 * val_correct / val_total\n",
    "        val_acc_hist.append(epoch_val_acc)\n",
    "        print(f\"[FACE] Epoch {epoch}/{FACE_EPOCHS} → Train Acc: {epoch_train_acc:.2f}% | Val Acc: {epoch_val_acc:.2f}%\")\n",
    "\n",
    "        if epoch_val_acc > best_val:\n",
    "            best_val = epoch_val_acc\n",
    "            torch.save({'model_state': model.state_dict(), 'classes': emotions}, FACE_MODEL_PATH)\n",
    "            print(f\"[FACE] Best model saved (Val Acc {best_val:.2f}%) -> {FACE_MODEL_PATH}\")\n",
    "\n",
    "        all_val_probs.append(np.vstack(val_probs))\n",
    "        all_val_targets.append(np.concatenate(val_targets))\n",
    "\n",
    "    y_score = np.vstack(all_val_probs) if all_val_probs else np.array([])\n",
    "    y_true = np.concatenate(all_val_targets) if all_val_targets else np.array([])\n",
    "    pickle.dump({'train_acc': train_acc_hist, 'val_acc': val_acc_hist}, open(\"face_training_curves.pkl\",\"wb\"))\n",
    "    return {'model': model, 'best_val': best_val, 'y_score': y_score, 'y_true': y_true, 'classes': emotions}\n",
    "\n",
    "# ===========================================\n",
    "# ================ SPEECH PART ==============\n",
    "# ===========================================\n",
    "class TransformerLSTM(nn.Module):\n",
    "    def __init__(self, num_emotions, d_model=128, nhead=4, num_layers=2):\n",
    "        super().__init__()\n",
    "        self.feature_proj = nn.Linear(40, d_model)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, dim_feedforward=256,\n",
    "                                                   dropout=0.3, batch_first=True)\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.lstm = nn.LSTM(d_model, 128, batch_first=True)\n",
    "        self.fc = nn.Linear(128, num_emotions)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # expects [B, timesteps, feat_dim] or [B,1,feat_dim,timesteps]\n",
    "        if x.dim() == 4:\n",
    "            x = x.squeeze(1).permute(0,2,1)\n",
    "        x = self.feature_proj(x)\n",
    "        x = self.transformer(x)\n",
    "        x, _ = self.lstm(x)\n",
    "        x = self.dropout(x[:, -1, :])\n",
    "        return self.fc(x)\n",
    "\n",
    "def load_speech_cached(pkl_path: str):\n",
    "    if not os.path.exists(pkl_path):\n",
    "        raise FileNotFoundError(\"Speech cached features (.pkl) not found: \" + pkl_path)\n",
    "    with open(pkl_path, \"rb\") as f:\n",
    "        X, y, meta = pickle.load(f)  # X: list/array of feature arrays, y: labels numeric, meta optional\n",
    "    return X, y, meta\n",
    "\n",
    "class SpeechDataset(Dataset):\n",
    "    def __init__(self, X_list, y_list):\n",
    "        self.X = X_list\n",
    "        self.y = y_list\n",
    "    def __len__(self): return len(self.X)\n",
    "    def __getitem__(self, idx):\n",
    "        arr = np.array(self.X[idx], dtype=np.float32)\n",
    "        if arr.ndim == 1:\n",
    "            # try folding into (timesteps, feat_dim) if multiple of 40\n",
    "            if arr.size % 40 == 0:\n",
    "                arr = arr.reshape(-1, 40)\n",
    "            else:\n",
    "                arr = arr.reshape(1, -1)\n",
    "        # expected shape -> (timesteps, feat_dim)\n",
    "        return torch.tensor(arr, dtype=torch.float32), torch.tensor(self.y[idx], dtype=torch.long)\n",
    "\n",
    "def train_speech_model():\n",
    "    X, y, meta = load_speech_cached(SPEECH_FEATURES_PKL)\n",
    "    print(f\"[SPEECH] Loaded {len(X)} samples.\")\n",
    "    # split\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.15, random_state=SEED, stratify=y)\n",
    "    train_ds = SpeechDataset(X_train, y_train)\n",
    "    val_ds = SpeechDataset(X_val, y_val)\n",
    "    train_loader = DataLoader(train_ds, batch_size=SPEECH_BATCH, shuffle=True, collate_fn=None)\n",
    "    val_loader = DataLoader(val_ds, batch_size=SPEECH_BATCH, shuffle=False, collate_fn=None)\n",
    "\n",
    "    num_emotions = len(set(y))\n",
    "    model = TransformerLSTM(num_emotions=num_emotions).to(DEVICE)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=SPEECH_LR)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    best_val = -1.0\n",
    "    train_acc_hist, val_acc_hist = [], []\n",
    "    all_val_probs, all_val_targets = [], []\n",
    "\n",
    "    for epoch in range(1, SPEECH_EPOCHS+1):\n",
    "        model.train()\n",
    "        running_correct, running_total = 0, 0\n",
    "        pbar = tqdm(train_loader, desc=f\"[SPEECH] Epoch {epoch}/{SPEECH_EPOCHS}\", leave=False)\n",
    "        for Xb, yb in pbar:\n",
    "            Xb, yb = Xb.to(DEVICE), yb.to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(Xb)\n",
    "            loss = criterion(outputs, yb)\n",
    "            loss.backward(); optimizer.step()\n",
    "            preds = outputs.argmax(dim=1)\n",
    "            running_total += yb.size(0)\n",
    "            running_correct += (preds == yb).sum().item()\n",
    "            pbar.set_postfix({\"TrainAcc\": f\"{100.0*running_correct/running_total:.2f}%\"})\n",
    "\n",
    "        epoch_train_acc = 100.0 * running_correct / running_total\n",
    "        train_acc_hist.append(epoch_train_acc)\n",
    "\n",
    "        model.eval()\n",
    "        val_correct, val_total = 0, 0\n",
    "        val_probs, val_targets = [], []\n",
    "        with torch.no_grad():\n",
    "            for Xb, yb in val_loader:\n",
    "                Xb, yb = Xb.to(DEVICE), yb.to(DEVICE)\n",
    "                outputs = model(Xb)\n",
    "                probs = F.softmax(outputs, dim=1)\n",
    "                preds = probs.argmax(dim=1)\n",
    "                val_total += yb.size(0)\n",
    "                val_correct += (preds == yb).sum().item()\n",
    "                val_probs.append(probs.cpu().numpy()); val_targets.append(yb.cpu().numpy())\n",
    "\n",
    "        epoch_val_acc = 100.0 * val_correct / val_total\n",
    "        val_acc_hist.append(epoch_val_acc)\n",
    "        print(f\"[SPEECH] Epoch {epoch}/{SPEECH_EPOCHS} → Train Acc: {epoch_train_acc:.2f}% | Val Acc: {epoch_val_acc:.2f}%\")\n",
    "\n",
    "        if epoch_val_acc > best_val:\n",
    "            best_val = epoch_val_acc\n",
    "            torch.save({'model': model.state_dict(), 'emotions': list(sorted(set(y)))}, SPEECH_MODEL_PATH)\n",
    "            print(f\"[SPEECH] Best model saved (Val Acc {best_val:.2f}%) -> {SPEECH_MODEL_PATH}\")\n",
    "\n",
    "        all_val_probs.append(np.vstack(val_probs))\n",
    "        all_val_targets.append(np.concatenate(val_targets))\n",
    "\n",
    "    y_score = np.vstack(all_val_probs) if all_val_probs else np.array([])\n",
    "    y_true = np.concatenate(all_val_targets) if all_val_targets else np.array([])\n",
    "    pickle.dump({'train_acc': train_acc_hist, 'val_acc': val_acc_hist}, open(\"speech_training_curves.pkl\",\"wb\"))\n",
    "    return {'model': model, 'best_val': best_val, 'y_score': y_score, 'y_true': y_true, 'classes': sorted(list(set(y)))}\n",
    "\n",
    "# ===========================================\n",
    "# ================ STACKER (optional) ======\n",
    "# ===========================================\n",
    "def train_stacker_from_alignment(text_best_path: str, face_best_path: str, speech_best_path: str, align_csv: str):\n",
    "    \"\"\"\n",
    "    align_csv should contain: label, content, image_path, audio_feat_path\n",
    "    For each row:\n",
    "     - load text -> get probs via text model\n",
    "     - load image -> get probs via face model\n",
    "     - load audio_feat_path (.npy) -> get probs via speech model\n",
    "    Then train logistic regression on concatenated probs (stacking).\n",
    "    \"\"\"\n",
    "    if align_csv is None:\n",
    "        print(\"[STACKER] ALIGN_CSV is None -> skipping stacking.\")\n",
    "        return None\n",
    "\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    df = pd.read_csv(align_csv)\n",
    "    required = ['label','text','image_path','audio_feat_path']\n",
    "    for c in required:\n",
    "        if c not in df.columns:\n",
    "            raise RuntimeError(f\"[STACKER] Alignment CSV must contain column: {c}\")\n",
    "\n",
    "    # load models + label spaces\n",
    "    text_ck = torch.load(text_best_path, map_location=DEVICE)\n",
    "    text_classes = text_ck.get('class_names') or np.load(TEXT_LABELS_NPY, allow_pickle=True).tolist()\n",
    "    text_model = BERTClassifier(TEXT_MODEL_NAME, num_classes=len(text_classes)).to(DEVICE)\n",
    "    text_model.load_state_dict(text_ck['model_state']); text_model.eval()\n",
    "    text_token = BertTokenizer.from_pretrained(TEXT_MODEL_NAME, use_fast=True)\n",
    "\n",
    "    face_ck = torch.load(face_best_path, map_location=DEVICE)\n",
    "    face_classes = face_ck.get('classes') or np.load(FACE_LABELS_NPY, allow_pickle=True).tolist()\n",
    "    face_model = InceptionResNetV3_Emotion(num_classes=len(face_classes)).to(DEVICE)\n",
    "    face_model.load_state_dict(face_ck['model_state']); face_model.eval()\n",
    "    preprocess = transforms.Compose([transforms.Resize((IMG_SIZE,IMG_SIZE)), transforms.ToTensor(),\n",
    "                                     transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])])\n",
    "\n",
    "    speech_ck = torch.load(speech_best_path, map_location=DEVICE)\n",
    "    speech_classes = speech_ck.get('emotions')\n",
    "    speech_model = TransformerLSTM(num_emotions=len(speech_classes)).to(DEVICE)\n",
    "    speech_model.load_state_dict(speech_ck['model']); speech_model.eval()\n",
    "\n",
    "    # union classes mapping (we'll require same label names across modalities in alignment CSV)\n",
    "    labels = sorted(df['label'].unique().tolist())\n",
    "    label_to_idx = {l:i for i,l in enumerate(labels)}\n",
    "    y = df['label'].map(label_to_idx).values\n",
    "\n",
    "    X_stack = []\n",
    "    for idx, row in tqdm(df.iterrows(), total=len(df), desc=\"[STACKER] build features\"):\n",
    "        # text probs\n",
    "        text_input = text_token([row['text']], padding=True, truncation=True, max_length=MAX_LEN, return_tensors='pt')\n",
    "        t_in = text_input['input_ids'].to(DEVICE); t_att = text_input['attention_mask'].to(DEVICE)\n",
    "        with torch.no_grad():\n",
    "            t_out = text_model(t_in, t_att)\n",
    "            t_prob = F.softmax(t_out, dim=1).cpu().numpy().squeeze(0)\n",
    "\n",
    "        # image probs\n",
    "        try:\n",
    "            img = Image.open(row['image_path']).convert('RGB')\n",
    "            img_t = preprocess(img).unsqueeze(0).to(DEVICE)\n",
    "            with torch.no_grad():\n",
    "                f_out = face_model(img_t)\n",
    "                f_prob = F.softmax(f_out, dim=1).cpu().numpy().squeeze(0)\n",
    "        except Exception:\n",
    "            f_prob = np.zeros((len(face_classes),), dtype=float)\n",
    "\n",
    "        # audio probs\n",
    "        try:\n",
    "            audio_np = np.load(row['audio_feat_path'], allow_pickle=True)\n",
    "            arr = np.array(audio_np, dtype=np.float32)\n",
    "            if arr.ndim == 1 and arr.size % 40 == 0:\n",
    "                arr = arr.reshape(-1, 40)\n",
    "            X_t = torch.tensor(arr, dtype=torch.float32).unsqueeze(0).to(DEVICE)\n",
    "            with torch.no_grad():\n",
    "                s_out = speech_model(X_t)\n",
    "                s_prob = F.softmax(s_out, dim=1).cpu().numpy().squeeze(0)\n",
    "        except Exception:\n",
    "            s_prob = np.zeros((len(speech_classes),), dtype=float)\n",
    "\n",
    "        # For stacking we will concatenate probs but to keep dims manageable we map each probs vector to *label set defined by alignment CSV*\n",
    "        # If your class sets differ, you should remap them; here we will simply zero-pad/truncate to fit the label count of the CSV.\n",
    "        # Simplifying assumption: the label names in alignment CSV correspond to the CSV-level label set.\n",
    "        # Map each modality probs to length = len(labels) by simple heuristic: if class names match label names, place them; else project by sum.\n",
    "        def project_probs(src_probs, src_classes, target_labels):\n",
    "            out = np.zeros(len(target_labels), dtype=float)\n",
    "            try:\n",
    "                for i, cname in enumerate(src_classes):\n",
    "                    if cname in target_labels:\n",
    "                        out[target_labels.index(cname)] = src_probs[i]\n",
    "                if out.sum() == 0:\n",
    "                    # fallback: take argmax and place\n",
    "                    out[np.argmax(src_probs) % len(target_labels)] = src_probs.max()\n",
    "            except Exception:\n",
    "                out[:] = 0.0\n",
    "            return out\n",
    "\n",
    "        t_proj = project_probs(t_prob, text_classes, labels) if len(text_prob:=t_prob)>0 else np.zeros(len(labels))\n",
    "        f_proj = project_probs(f_prob, face_classes, labels) if len(f_prob)>0 else np.zeros(len(labels))\n",
    "        s_proj = project_probs(s_prob, speech_classes, labels) if len(s_prob)>0 else np.zeros(len(labels))\n",
    "\n",
    "        feat = np.concatenate([t_proj, f_proj, s_proj])\n",
    "        X_stack.append(feat)\n",
    "\n",
    "    X_stack = np.vstack(X_stack)\n",
    "    y = np.array(y)\n",
    "\n",
    "    # split stack data\n",
    "    X_tr, X_val_st, y_tr, y_val_st = train_test_split(X_stack, y, test_size=0.2, random_state=SEED, stratify=y)\n",
    "\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    clf = LogisticRegression(C=STACKER_C, solver=STACKER_SOLVER, max_iter=1000, multi_class='ovr')\n",
    "    clf.fit(X_tr, y_tr)\n",
    "    y_pred = clf.predict(X_val_st)\n",
    "    precision = precision_score(y_val_st, y_pred, average='macro', zero_division=0)\n",
    "    recall = recall_score(y_val_st, y_pred, average='macro', zero_division=0)\n",
    "    f1 = f1_score(y_val_st, y_pred, average='macro', zero_division=0)\n",
    "    print(f\"[STACKER] Validation metrics -> Precision: {precision:.4f}, Recall: {recall:.4f}, F1: {f1:.4f}\")\n",
    "\n",
    "    # save\n",
    "    pickle.dump({'stacker':clf, 'labels':labels}, open(\"stacker.pkl\",\"wb\"))\n",
    "    return clf\n",
    "\n",
    "# ===========================================\n",
    "# ================ RUN ALL ==================\n",
    "# ===========================================\n",
    "def run_all():\n",
    "    t0 = time.time()\n",
    "    print(\"=== TRAIN TEXT MODEL ===\")\n",
    "    text_res = train_text_model()\n",
    "    print(\"=== TRAIN FACE MODEL ===\")\n",
    "    face_res = train_face_model()\n",
    "    print(\"=== TRAIN SPEECH MODEL ===\")\n",
    "    speech_res = train_speech_model()\n",
    "    print(f\"[ALL] Finished base training in {(time.time()-t0)/60:.2f} minutes.\")\n",
    "\n",
    "    # Print per-model summary metrics if available\n",
    "    for name, res in [('TEXT', text_res), ('FACE', face_res), ('SPEECH', speech_res)]:\n",
    "        if res['y_score'].size != 0:\n",
    "            y_true = res['y_true']\n",
    "            y_score = res['y_score']\n",
    "            y_pred = y_score.argmax(axis=1)\n",
    "            prec = precision_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "            rec = recall_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "            f1 = f1_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "            print(f\"[SUMMARY {name}] Val samples: {len(y_true)} | Precision: {prec:.4f} Recall: {rec:.4f} F1: {f1:.4f}\")\n",
    "        else:\n",
    "            print(f\"[SUMMARY {name}] No validation predictions available for metrics.\")\n",
    "\n",
    "    # Optional stacking\n",
    "    if ALIGN_CSV:\n",
    "        print(\"=== TRAIN STACKER FROM ALIGNMENT CSV ===\")\n",
    "        try:\n",
    "            clf = train_stacker_from_alignment(TEXT_BEST_MODEL, FACE_MODEL_PATH, SPEECH_MODEL_PATH, ALIGN_CSV)\n",
    "            print(\"[STACKER] Trained and saved stacker.pkl\")\n",
    "        except Exception as e:\n",
    "            print(\"[STACKER] Failed:\", e)\n",
    "    else:\n",
    "        print(\"[STACKER] ALIGN_CSV not provided -> skipping stacker training.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_all()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "01f46803",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Device: cuda\n",
      "[INFO] Classes: ['anger' 'disgust' 'fear' 'joy' 'neutral' 'sadness' 'surprise']\n",
      "[INFO] Loading encoders...\n",
      "[INFO] Loaded text encoder weights (partial).\n",
      "[WARN] Could not load face checkpoint fully; using ImageNet init.\n",
      "[INFO] Loaded speech encoder weights (partial).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                   \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 400\u001b[39m\n\u001b[32m    397\u001b[39m fusion_model = FusionHead(fusion_in, hidden=FUSION_HIDDEN, num_classes=num_classes).to(DEVICE)\n\u001b[32m    399\u001b[39m \u001b[38;5;66;03m# train fusion\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m400\u001b[39m res = \u001b[43mtrain_fusion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfusion_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_enc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mface_enc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mspeech_enc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mEPOCHS\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    402\u001b[39m \u001b[38;5;66;03m# plots\u001b[39;00m\n\u001b[32m    403\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m res[\u001b[33m'\u001b[39m\u001b[33mtrain_accs\u001b[39m\u001b[33m'\u001b[39m]:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 270\u001b[39m, in \u001b[36mtrain_fusion\u001b[39m\u001b[34m(fusion_model, text_enc, face_enc, speech_enc, train_loader, val_loader, num_classes, epochs)\u001b[39m\n\u001b[32m    268\u001b[39m running_correct = \u001b[32m0\u001b[39m; running_total = \u001b[32m0\u001b[39m\n\u001b[32m    269\u001b[39m pbar = tqdm(train_loader, desc=\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m[FUSION] Epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m, leave=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m270\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpbar\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    271\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# move inputs\u001b[39;49;00m\n\u001b[32m    272\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43minput_ids\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    273\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mattention_mask\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mk:\\Code\\Project\\Research Paper\\Emotion Detection\\Code\\torch_env\\Lib\\site-packages\\tqdm\\std.py:1181\u001b[39m, in \u001b[36mtqdm.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1178\u001b[39m time = \u001b[38;5;28mself\u001b[39m._time\n\u001b[32m   1180\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1181\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1182\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[32m   1183\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[32m   1184\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mk:\\Code\\Project\\Research Paper\\Emotion Detection\\Code\\torch_env\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:732\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    729\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    730\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    731\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m732\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    733\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    734\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    735\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    736\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    737\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    738\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mk:\\Code\\Project\\Research Paper\\Emotion Detection\\Code\\torch_env\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:788\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    786\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    787\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m788\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    789\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    790\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mk:\\Code\\Project\\Research Paper\\Emotion Detection\\Code\\torch_env\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     50\u001b[39m         data = \u001b[38;5;28mself\u001b[39m.dataset.__getitems__(possibly_batched_index)\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m         data = [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     54\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 165\u001b[39m, in \u001b[36mFusionDataset.__getitem__\u001b[39m\u001b[34m(self, idx)\u001b[39m\n\u001b[32m    163\u001b[39m img_path = row[\u001b[33m'\u001b[39m\u001b[33mimage_path\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m    164\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m165\u001b[39m     img = \u001b[43mImage\u001b[49m\u001b[43m.\u001b[49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_path\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mRGB\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    166\u001b[39m     img_t = \u001b[38;5;28mself\u001b[39m.img_transform(img)\n\u001b[32m    167\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m    168\u001b[39m     \u001b[38;5;66;03m# fallback to zeros\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mk:\\Code\\Project\\Research Paper\\Emotion Detection\\Code\\torch_env\\Lib\\site-packages\\PIL\\Image.py:972\u001b[39m, in \u001b[36mImage.convert\u001b[39m\u001b[34m(self, mode, matrix, dither, palette, colors)\u001b[39m\n\u001b[32m    920\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mconvert\u001b[39m(\n\u001b[32m    921\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    922\u001b[39m     mode: \u001b[38;5;28mstr\u001b[39m | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    926\u001b[39m     colors: \u001b[38;5;28mint\u001b[39m = \u001b[32m256\u001b[39m,\n\u001b[32m    927\u001b[39m ) -> Image:\n\u001b[32m    928\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    929\u001b[39m \u001b[33;03m    Returns a converted copy of this image. For the \"P\" mode, this\u001b[39;00m\n\u001b[32m    930\u001b[39m \u001b[33;03m    method translates pixels through the palette.  If mode is\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    969\u001b[39m \u001b[33;03m    :returns: An :py:class:`~PIL.Image.Image` object.\u001b[39;00m\n\u001b[32m    970\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m972\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    974\u001b[39m     has_transparency = \u001b[33m\"\u001b[39m\u001b[33mtransparency\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.info\n\u001b[32m    975\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m mode \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.mode == \u001b[33m\"\u001b[39m\u001b[33mP\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    976\u001b[39m         \u001b[38;5;66;03m# determine default mode\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mk:\\Code\\Project\\Research Paper\\Emotion Detection\\Code\\torch_env\\Lib\\site-packages\\PIL\\ImageFile.py:394\u001b[39m, in \u001b[36mImageFile.load\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    391\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(msg)\n\u001b[32m    393\u001b[39m b = b + s\n\u001b[32m--> \u001b[39m\u001b[32m394\u001b[39m n, err_code = \u001b[43mdecoder\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    395\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m n < \u001b[32m0\u001b[39m:\n\u001b[32m    396\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Multimodal Fusion Training Script\n",
    "# Uses your existing text, face and speech models as experts and trains a fusion head.\n",
    "# - Loads CSV with columns: content, image_path, audio_feat_path, label\n",
    "# - Extracts embeddings from expert models (using their penultimate layers)\n",
    "# - Projects each embedding to a common dim, concatenates, and trains an MLP fusion head\n",
    "# - Saves best fusion checkpoint and prints metrics\n",
    "\n",
    "import os\n",
    "import time\n",
    "import copy\n",
    "import random\n",
    "from typing import List, Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, models\n",
    "from PIL import Image\n",
    "\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "\n",
    "# ----------------------\n",
    "# USER CONFIG (edit if needed)\n",
    "# ----------------------\n",
    "FUSION_CSV = r\"K:\\Code\\Project\\Research Paper\\Emotion Detection\\meld dataset\\MELD.Raw\\self\\fusion_dataset.csv\"\n",
    "OUTPUT_DIR = r\"K:\\Code\\Project\\Research Paper\\Emotion Detection\\meld dataset\\MELD.Raw\\self\\fusion_output\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Expert model checkpoints (from your code)\n",
    "TEXT_BEST_MODEL = r\"K:\\Code\\Project\\Research Paper\\Emotion Detection\\Code\\bert_emotion_text_final.pth\"\n",
    "TEXT_LABELS_NPY = r\"K:\\Code\\Project\\Research Paper\\Emotion Detection\\Code\\label_classes.npy\"\n",
    "TEXT_MODEL_NAME = \"bert-base-multilingual-cased\"\n",
    "\n",
    "FACE_BEST_MODEL = r\"K:\\Code\\Project\\Research Paper\\Emotion Detection\\inceptionresnetv3_face_emotion.pth\"\n",
    "FACE_LABELS_NPY = r\"K:\\Code\\Project\\Research Paper\\Emotion Detection\\emotions_face.npy\"\n",
    "\n",
    "SPEECH_BEST_MODEL = r\"./best_transformer_speech_model.pth\"\n",
    "SPEECH_CACHED_FEATURES = r\"cached_features.pkl\"  # optional fallback if audio paths missing\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"[INFO] Device:\", DEVICE)\n",
    "\n",
    "# hyperparams\n",
    "PROJECT_DIM = 256\n",
    "FUSION_HIDDEN = 512\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 12\n",
    "LR = 1e-4\n",
    "NUM_WORKERS = 0\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "random.seed(RANDOM_SEED); np.random.seed(RANDOM_SEED); torch.manual_seed(RANDOM_SEED)\n",
    "if torch.cuda.is_available(): torch.cuda.manual_seed_all(RANDOM_SEED)\n",
    "\n",
    "# ----------------------\n",
    "# Utilities\n",
    "# ----------------------\n",
    "def load_fusion_csv(path: str) -> pd.DataFrame:\n",
    "    if not os.path.exists(path):\n",
    "        raise FileNotFoundError(\"Fusion CSV not found: \" + path)\n",
    "    df = pd.read_csv(path)\n",
    "    required = ['text','image_path','audio_path','label']\n",
    "    for c in required:\n",
    "        if c not in df.columns:\n",
    "            raise RuntimeError(f\"Fusion CSV must contain column: {c}\")\n",
    "    df = df.dropna(subset=['text','label'])\n",
    "    df['text'] = df['text'].astype(str).str.strip()\n",
    "    return df.reset_index(drop=True)\n",
    "\n",
    "# ----------------------\n",
    "# Expert model wrappers (extract penultimate features)\n",
    "# ----------------------\n",
    "# TEXT encoder (BERT pooler)\n",
    "class TextEncoder(nn.Module):\n",
    "    def __init__(self, model_name=TEXT_MODEL_NAME):\n",
    "        super().__init__()\n",
    "        self.bert = BertModel.from_pretrained(model_name)\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        out = self.bert(input_ids=input_ids, attention_mask=attention_mask, return_dict=True)\n",
    "        return out.pooler_output  # [B, hidden]\n",
    "\n",
    "# FACE encoder: use inception_v3 and return 512-d feature (after first fc in your face model)\n",
    "class FaceEncoder(nn.Module):\n",
    "    def __init__(self, pretrained=True):\n",
    "        super().__init__()\n",
    "        base = models.inception_v3(\n",
    "        weights=models.Inception_V3_Weights.IMAGENET1K_V1,\n",
    "        aux_logits=True   # must be True for pretrained weights\n",
    "    )\n",
    "\n",
    "        in_features = base.fc.in_features\n",
    "        # replicate your face model's fc structure, but we'll keep final linear as identity to extract 512-d\n",
    "        fc = nn.Sequential(nn.Linear(in_features, 512), nn.ReLU(inplace=True), nn.Dropout(0.4), nn.Identity())\n",
    "        base.fc = fc\n",
    "        self.base = base\n",
    "    def forward(self, x):\n",
    "        return self.base(x)  # [B, 512]\n",
    "\n",
    "# SPEECH encoder: use TransformerLSTM but return 128-d LSTM hidden vector before final fc\n",
    "class SpeechEncoder(nn.Module):\n",
    "    def __init__(self, d_model=128, nhead=4, num_layers=2):\n",
    "        super().__init__()\n",
    "        self.feature_proj = nn.Linear(40, d_model)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, dim_feedforward=256, dropout=0.3, batch_first=True)\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.lstm = nn.LSTM(d_model, 128, batch_first=True)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        # no final fc here\n",
    "    def forward(self, x):\n",
    "        # x: [B, timesteps, feat_dim] or [B,1,feat_dim,timesteps]\n",
    "        if x.dim() == 4:\n",
    "            x = x.squeeze(1).permute(0,2,1)\n",
    "        x = self.feature_proj(x)\n",
    "        x = self.transformer(x)\n",
    "        x, _ = self.lstm(x)\n",
    "        x = self.dropout(x[:, -1, :])\n",
    "        return x  # [B,128]\n",
    "\n",
    "# ----------------------\n",
    "# Fusion head\n",
    "# ----------------------\n",
    "class FusionHead(nn.Module):\n",
    "    def __init__(self, in_dim, hidden=FUSION_HIDDEN, num_classes=7, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(in_dim, hidden)\n",
    "        self.act = nn.ReLU()\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.fc2 = nn.Linear(hidden, num_classes)\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.drop(x)\n",
    "        return self.fc2(x)\n",
    "\n",
    "# ----------------------\n",
    "# Fusion Dataset\n",
    "# ----------------------\n",
    "class FusionDataset(Dataset):\n",
    "    def __init__(self, df: pd.DataFrame, tokenizer, img_transform, label_encoder: LabelEncoder):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.img_transform = img_transform\n",
    "        self.le = label_encoder\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        text = str(row['text'])\n",
    "        enc = self.tokenizer(text, padding='max_length', truncation=True, max_length=64, return_tensors='pt')\n",
    "        input_ids = enc['input_ids'].squeeze(0)\n",
    "        attention_mask = enc['attention_mask'].squeeze(0)\n",
    "        # image\n",
    "        img_path = row['image_path']\n",
    "        try:\n",
    "            img = Image.open(img_path).convert('RGB')\n",
    "            img_t = self.img_transform(img)\n",
    "        except Exception:\n",
    "            # fallback to zeros\n",
    "            img_t = torch.zeros(3, 299, 299)\n",
    "        # audio features (.npy expected)\n",
    "        audio_feat = None\n",
    "        afp = row.get('audio_feat_path', None)\n",
    "        if isinstance(afp, str) and os.path.exists(afp):\n",
    "            try:\n",
    "                arr = np.load(afp, allow_pickle=True)\n",
    "                arr = np.array(arr, dtype=np.float32)\n",
    "                if arr.ndim == 1 and arr.size % 40 == 0:\n",
    "                    arr = arr.reshape(-1, 40)\n",
    "                audio_feat = torch.tensor(arr, dtype=torch.float32)\n",
    "            except Exception:\n",
    "                audio_feat = torch.zeros(1,40)\n",
    "        else:\n",
    "            audio_feat = torch.zeros(1,40)\n",
    "        label = self.le.transform([row['label']])[0]\n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "            'image': img_t,\n",
    "            'audio': audio_feat,\n",
    "            'label': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# ----------------------\n",
    "# Helpers for loading pretrained experts (and mapping)\n",
    "# ----------------------\n",
    "def load_text_encoder(checkpoint_path: str):\n",
    "    model = TextEncoder().to(DEVICE)\n",
    "    if os.path.exists(checkpoint_path):\n",
    "        ck = torch.load(checkpoint_path, map_location=DEVICE)\n",
    "        # checkpoint may store {'model_state':...}\n",
    "        st = ck if isinstance(ck, dict) and 'model_state' in ck else ck\n",
    "        try:\n",
    "            model_state = st['model_state'] if isinstance(st, dict) and 'model_state' in st else st\n",
    "            # We only need bert weights; if a full classifier was saved, load bert's weights via matching keys\n",
    "            model.bert.load_state_dict({k.replace('bert.',''):v for k,v in model_state.items() if k.startswith('bert.')}, strict=False)\n",
    "            print('[INFO] Loaded text encoder weights (partial).')\n",
    "        except Exception:\n",
    "            # fallback: try direct load\n",
    "            try:\n",
    "                model.load_state_dict(model_state)\n",
    "                print('[INFO] Loaded text encoder full state.')\n",
    "            except Exception:\n",
    "                print('[WARN] Could not load text checkpoint fully; using base BERT weights.')\n",
    "    return model\n",
    "\n",
    "\n",
    "def load_face_encoder(checkpoint_path: str):\n",
    "    enc = FaceEncoder(pretrained=True).to(DEVICE)\n",
    "    if os.path.exists(checkpoint_path):\n",
    "        ck = torch.load(checkpoint_path, map_location=DEVICE)\n",
    "        st = ck if isinstance(ck, dict) and 'model_state' in ck else ck\n",
    "        try:\n",
    "            state = st['model_state'] if isinstance(st, dict) and 'model_state' in st else st\n",
    "            # load state carefully\n",
    "            enc_state = {}\n",
    "            for k,v in state.items():\n",
    "                # map original face model keys to our encoder where base.fc last Linear replaced by Identity\n",
    "                enc_state[k] = v\n",
    "            enc.load_state_dict(enc_state, strict=False)\n",
    "            print('[INFO] Loaded face encoder weights (partial).')\n",
    "        except Exception:\n",
    "            print('[WARN] Could not load face checkpoint fully; using ImageNet init.')\n",
    "    return enc\n",
    "\n",
    "\n",
    "def load_speech_encoder(checkpoint_path: str):\n",
    "    enc = SpeechEncoder().to(DEVICE)\n",
    "    if os.path.exists(checkpoint_path):\n",
    "        ck = torch.load(checkpoint_path, map_location=DEVICE)\n",
    "        st = ck if isinstance(ck, dict) and ('model' in ck or 'model_state' in ck) else ck\n",
    "        try:\n",
    "            model_state = st.get('model', st.get('model_state', st))\n",
    "            enc.load_state_dict(model_state, strict=False)\n",
    "            print('[INFO] Loaded speech encoder weights (partial).')\n",
    "        except Exception:\n",
    "            print('[WARN] Could not load speech checkpoint fully; using random init.')\n",
    "    return enc\n",
    "\n",
    "# ----------------------\n",
    "# Training & evaluation for fusion head\n",
    "# ----------------------\n",
    "\n",
    "def train_fusion(fusion_model, text_enc, face_enc, speech_enc, train_loader, val_loader, num_classes, epochs=EPOCHS):\n",
    "    fusion_model.to(DEVICE)\n",
    "    # freeze encoders\n",
    "    text_enc.eval(); face_enc.eval(); speech_enc.eval()\n",
    "    for p in text_enc.parameters(): p.requires_grad = False\n",
    "    for p in face_enc.parameters(): p.requires_grad = False\n",
    "    for p in speech_enc.parameters(): p.requires_grad = False\n",
    "\n",
    "    optimizer = torch.optim.Adam(fusion_model.parameters(), lr=LR)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    best_val = -1.0\n",
    "    train_accs, val_accs = [], []\n",
    "\n",
    "    for epoch in range(1, epochs+1):\n",
    "        fusion_model.train()\n",
    "        running_correct = 0; running_total = 0\n",
    "        pbar = tqdm(train_loader, desc=f\"[FUSION] Epoch {epoch}/{epochs}\", leave=False)\n",
    "        for batch in pbar:\n",
    "            # move inputs\n",
    "            input_ids = batch['input_ids'].to(DEVICE)\n",
    "            attention_mask = batch['attention_mask'].to(DEVICE)\n",
    "            images = batch['image'].to(DEVICE)\n",
    "            audios = batch['audio']\n",
    "            # audios is list of tensors with variable timesteps; pad to max in batch\n",
    "            # we expect audios: Tensor or list; ensure tensor\n",
    "            if isinstance(audios, list) or audios.dim()==3 and audios.shape[0]!=input_ids.shape[0]:\n",
    "                # try to stack safely\n",
    "                audios = torch.stack([a if isinstance(a, torch.Tensor) else torch.tensor(a, dtype=torch.float32) for a in audios]).to(DEVICE)\n",
    "            else:\n",
    "                audios = audios.to(DEVICE)\n",
    "            labels = batch['label'].to(DEVICE)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                t_feat = text_enc(input_ids, attention_mask)  # [B,768]\n",
    "                f_feat = face_enc(images)  # [B,512]\n",
    "                s_feat = speech_enc(audios)  # [B,128]\n",
    "\n",
    "            # project\n",
    "            t_proj = proj_text(t_feat)\n",
    "            f_proj = proj_face(f_feat)\n",
    "            s_proj = proj_speech(s_feat)\n",
    "\n",
    "            fused = torch.cat([t_proj, f_proj, s_proj], dim=1)\n",
    "            logits = fusion_model(fused)\n",
    "            loss = criterion(logits, labels)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            preds = logits.argmax(dim=1)\n",
    "            running_total += labels.size(0)\n",
    "            running_correct += (preds == labels).sum().item()\n",
    "            pbar.set_postfix({'TrainAcc': f\"{100.0*running_correct/running_total:.2f}%\", 'Loss': f\"{loss.item():.4f}\"})\n",
    "\n",
    "        train_acc = 100.0 * running_correct / running_total\n",
    "        train_accs.append(train_acc)\n",
    "\n",
    "        # Validation\n",
    "        fusion_model.eval()\n",
    "        v_total = 0; v_correct = 0\n",
    "        all_probs, all_targets = [], []\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                input_ids = batch['input_ids'].to(DEVICE)\n",
    "                attention_mask = batch['attention_mask'].to(DEVICE)\n",
    "                images = batch['image'].to(DEVICE)\n",
    "                audios = batch['audio']\n",
    "                if isinstance(audios, list) or (isinstance(audios, torch.Tensor) and audios.shape[0]!=input_ids.shape[0]):\n",
    "                    audios = torch.stack([a if isinstance(a, torch.Tensor) else torch.tensor(a, dtype=torch.float32) for a in audios]).to(DEVICE)\n",
    "                else:\n",
    "                    audios = audios.to(DEVICE)\n",
    "                labels = batch['label'].to(DEVICE)\n",
    "\n",
    "                t_feat = text_enc(input_ids, attention_mask)\n",
    "                f_feat = face_enc(images)\n",
    "                s_feat = speech_enc(audios)\n",
    "\n",
    "                t_proj = proj_text(t_feat)\n",
    "                f_proj = proj_face(f_feat)\n",
    "                s_proj = proj_speech(s_feat)\n",
    "                fused = torch.cat([t_proj, f_proj, s_proj], dim=1)\n",
    "                out = fusion_model(fused)\n",
    "                probs = F.softmax(out, dim=1)\n",
    "                preds = probs.argmax(dim=1)\n",
    "                v_total += labels.size(0)\n",
    "                v_correct += (preds == labels).sum().item()\n",
    "                all_probs.append(probs.cpu().numpy()); all_targets.append(labels.cpu().numpy())\n",
    "\n",
    "        val_acc = 100.0 * v_correct / v_total if v_total>0 else 0.0\n",
    "        val_accs.append(val_acc)\n",
    "        print(f\"[FUSION] Epoch {epoch}/{epochs} → Train Acc: {train_acc:.2f}% | Val Acc: {val_acc:.2f}%\")\n",
    "\n",
    "        if val_acc > best_val:\n",
    "            best_val = val_acc\n",
    "            torch.save({'fusion_state': fusion_model.state_dict(), 'proj_text': proj_text.state_dict(), 'proj_face': proj_face.state_dict(), 'proj_speech': proj_speech.state_dict(), 'classes': le.classes_.tolist()}, os.path.join(OUTPUT_DIR, 'best_fusion.pth'))\n",
    "            print(f\"[FUSION] Best saved (Val Acc {best_val:.2f}%)\")\n",
    "\n",
    "    # final metrics\n",
    "    y_score = np.vstack(all_probs) if all_probs else np.array([])\n",
    "    y_true = np.concatenate(all_targets) if all_targets else np.array([])\n",
    "    return {'train_accs': train_accs, 'val_accs': val_accs, 'best_val': best_val, 'y_score': y_score, 'y_true': y_true}\n",
    "\n",
    "# ----------------------\n",
    "# Main\n",
    "# ----------------------\n",
    "if __name__ == '__main__':\n",
    "    df = load_fusion_csv(FUSION_CSV)\n",
    "    # label encode\n",
    "    le = LabelEncoder(); df['label_idx'] = le.fit_transform(df['label'].astype(str))\n",
    "    num_classes = len(le.classes_)\n",
    "    print('[INFO] Classes:', le.classes_)\n",
    "\n",
    "    # train/val split (stratified)\n",
    "    idxs = list(range(len(df)))\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    train_idx, val_idx = train_test_split(idxs, test_size=0.2, random_state=RANDOM_SEED, stratify=df['label_idx'])\n",
    "    train_df = df.iloc[train_idx].reset_index(drop=True)\n",
    "    val_df = df.iloc[val_idx].reset_index(drop=True)\n",
    "\n",
    "    # tokenizer and transforms\n",
    "    tokenizer = BertTokenizer.from_pretrained(TEXT_MODEL_NAME, use_fast=True)\n",
    "    img_transform = transforms.Compose([transforms.Resize((299,299)), transforms.ToTensor(), transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])])\n",
    "\n",
    "    train_ds = FusionDataset(train_df, tokenizer, img_transform, le)\n",
    "    val_ds = FusionDataset(val_df, tokenizer, img_transform, le)\n",
    "    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)\n",
    "    val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\n",
    "\n",
    "    # load encoders\n",
    "    print('[INFO] Loading encoders...')\n",
    "    text_enc = load_text_encoder(TEXT_BEST_MODEL)\n",
    "    face_enc = load_face_encoder(FACE_BEST_MODEL)\n",
    "    speech_enc = load_speech_encoder(SPEECH_BEST_MODEL)\n",
    "\n",
    "    # projection heads\n",
    "    # note: text hidden size is BERT hidden (usually 768), face encoder outputs 512, speech 128\n",
    "    # create small linear proj layers\n",
    "    proj_text = nn.Linear(768, PROJECT_DIM).to(DEVICE)\n",
    "    proj_face = nn.Linear(512, PROJECT_DIM).to(DEVICE)\n",
    "    proj_speech = nn.Linear(128, PROJECT_DIM).to(DEVICE)\n",
    "\n",
    "    # fusion model\n",
    "    fusion_in = PROJECT_DIM * 3\n",
    "    fusion_model = FusionHead(fusion_in, hidden=FUSION_HIDDEN, num_classes=num_classes).to(DEVICE)\n",
    "\n",
    "    # train fusion\n",
    "    res = train_fusion(fusion_model, text_enc, face_enc, speech_enc, train_loader, val_loader, num_classes, epochs=EPOCHS)\n",
    "\n",
    "    # plots\n",
    "    if res['train_accs']:\n",
    "        plt.figure(figsize=(7,4)); plt.plot(range(1,len(res['train_accs'])+1), res['train_accs'], marker='o', label='Train Acc'); plt.plot(range(1,len(res['val_accs'])+1), res['val_accs'], marker='o', label='Val Acc'); plt.xlabel('Epoch'); plt.ylabel('Accuracy (%)'); plt.title('Fusion Accuracy'); plt.legend(); plt.grid(True); plt.show()\n",
    "\n",
    "    # final eval: confusion\n",
    "    if res['y_score'].size != 0:\n",
    "        y_true = res['y_true']\n",
    "        y_pred = res['y_score'].argmax(axis=1)\n",
    "        cm = confusion_matrix(y_true, y_pred)\n",
    "        print('Confusion Matrix:')\n",
    "        print(cm)\n",
    "        precision = precision_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "        recall = recall_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "        f1 = f1_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "        print(f'Final Fusion Metrics → Precision: {precision:.4f}, Recall: {recall:.4f}, F1: {f1:.4f}')\n",
    "\n",
    "    print('[DONE] Fusion training complete. Best fusion saved to', os.path.join(OUTPUT_DIR, 'best_fusion.pth'))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
