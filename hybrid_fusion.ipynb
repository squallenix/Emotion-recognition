{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cdb53d4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Hybrid (logit-level) Fusion Training (fixed + preload)\n",
      "Found 13674 paired samples. Classes: 7 -> ['anger', 'disgust', 'fear', 'joy', 'neutral', 'sadness', 'surprise']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preloading dataset into RAM: 100%|███████| 10939/10939 [01:41<00:00, 108.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PRELOAD] Finished preloading 10939 samples into RAM.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preloading dataset into RAM: 100%|█████████| 2735/2735 [00:25<00:00, 109.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PRELOAD] Finished preloading 2735 samples into RAM.\n",
      "[INFO] Loaded image checkpoint (partial load allowed).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_14828\\699467080.py:1002: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler(enabled=USE_AMP)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Loaded text checkpoint (partial load).\n",
      "[INFO] Unfroze EfficientNetV2 last 2 stages + projection.\n",
      "[INFO] Unfroze audio last transformer layer + LSTM + projection.\n",
      "[INFO] Unfroze last 4 BERT layers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100 - Train Acc: 4.66% | Val Acc: 5.16% | Time: 51.7s\n",
      " ✅ Best hybrid-fusion model saved -> ./best_hybrid_fusion_model.pth (Val Acc 5.16%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100 - Train Acc: 5.06% | Val Acc: 5.12% | Time: 65.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/100 - Train Acc: 5.30% | Val Acc: 5.08% | Time: 65.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/100 - Train Acc: 7.92% | Val Acc: 17.40% | Time: 65.1s\n",
      " ✅ Best hybrid-fusion model saved -> ./best_hybrid_fusion_model.pth (Val Acc 17.40%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/100 - Train Acc: 29.72% | Val Acc: 46.98% | Time: 152.8s\n",
      " ✅ Best hybrid-fusion model saved -> ./best_hybrid_fusion_model.pth (Val Acc 46.98%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/100 - Train Acc: 46.16% | Val Acc: 46.98% | Time: 431.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/100 - Train Acc: 46.94% | Val Acc: 46.98% | Time: 433.9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/100 - Train Acc: 46.96% | Val Acc: 46.98% | Time: 433.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                          \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1169\u001b[39m\n\u001b[32m   1167\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m   1168\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mStarting Hybrid (logit-level) Fusion Training (fixed + preload)\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1169\u001b[39m     \u001b[43mtrain_hybrid_fusion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrun_pso\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreload_dataset\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1021\u001b[39m, in \u001b[36mtrain_hybrid_fusion\u001b[39m\u001b[34m(csv_path, image_model_path, text_model_path, audio_model_path, freeze_bases, device, run_pso, preload_dataset)\u001b[39m\n\u001b[32m   1018\u001b[39m optimizer.zero_grad(set_to_none=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m   1020\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.amp.autocast(\u001b[33m'\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m'\u001b[39m, enabled=USE_AMP):\n\u001b[32m-> \u001b[39m\u001b[32m1021\u001b[39m     e_img = \u001b[43mimg_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1022\u001b[39m     e_txt = txt_model(input_ids, attention_mask)\n\u001b[32m   1023\u001b[39m     e_aud = aud_model(audio)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mk:\\Code\\Project\\Research Paper\\Emotion Detection\\Code\\torch_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mk:\\Code\\Project\\Research Paper\\Emotion Detection\\Code\\torch_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 147\u001b[39m, in \u001b[36mEfficientNetV2_Embed.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    146\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m--> \u001b[39m\u001b[32m147\u001b[39m     feat = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbase\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    148\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.proj(feat)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mk:\\Code\\Project\\Research Paper\\Emotion Detection\\Code\\torch_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mk:\\Code\\Project\\Research Paper\\Emotion Detection\\Code\\torch_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mk:\\Code\\Project\\Research Paper\\Emotion Detection\\Code\\torch_env\\Lib\\site-packages\\torchvision\\models\\efficientnet.py:344\u001b[39m, in \u001b[36mEfficientNet.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    343\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m344\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_forward_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mk:\\Code\\Project\\Research Paper\\Emotion Detection\\Code\\torch_env\\Lib\\site-packages\\torchvision\\models\\efficientnet.py:334\u001b[39m, in \u001b[36mEfficientNet._forward_impl\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    333\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_forward_impl\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m334\u001b[39m     x = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    336\u001b[39m     x = \u001b[38;5;28mself\u001b[39m.avgpool(x)\n\u001b[32m    337\u001b[39m     x = torch.flatten(x, \u001b[32m1\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mk:\\Code\\Project\\Research Paper\\Emotion Detection\\Code\\torch_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mk:\\Code\\Project\\Research Paper\\Emotion Detection\\Code\\torch_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mk:\\Code\\Project\\Research Paper\\Emotion Detection\\Code\\torch_env\\Lib\\site-packages\\torch\\nn\\modules\\container.py:250\u001b[39m, in \u001b[36mSequential.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    246\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    247\u001b[39m \u001b[33;03mRuns the forward pass.\u001b[39;00m\n\u001b[32m    248\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m     \u001b[38;5;28minput\u001b[39m = \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    251\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mk:\\Code\\Project\\Research Paper\\Emotion Detection\\Code\\torch_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mk:\\Code\\Project\\Research Paper\\Emotion Detection\\Code\\torch_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mk:\\Code\\Project\\Research Paper\\Emotion Detection\\Code\\torch_env\\Lib\\site-packages\\torch\\nn\\modules\\container.py:250\u001b[39m, in \u001b[36mSequential.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    246\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    247\u001b[39m \u001b[33;03mRuns the forward pass.\u001b[39;00m\n\u001b[32m    248\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m     \u001b[38;5;28minput\u001b[39m = \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    251\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mk:\\Code\\Project\\Research Paper\\Emotion Detection\\Code\\torch_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mk:\\Code\\Project\\Research Paper\\Emotion Detection\\Code\\torch_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mk:\\Code\\Project\\Research Paper\\Emotion Detection\\Code\\torch_env\\Lib\\site-packages\\torchvision\\models\\efficientnet.py:165\u001b[39m, in \u001b[36mMBConv.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    164\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m165\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    166\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.use_res_connect:\n\u001b[32m    167\u001b[39m         result = \u001b[38;5;28mself\u001b[39m.stochastic_depth(result)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mk:\\Code\\Project\\Research Paper\\Emotion Detection\\Code\\torch_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mk:\\Code\\Project\\Research Paper\\Emotion Detection\\Code\\torch_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mk:\\Code\\Project\\Research Paper\\Emotion Detection\\Code\\torch_env\\Lib\\site-packages\\torch\\nn\\modules\\container.py:250\u001b[39m, in \u001b[36mSequential.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    246\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    247\u001b[39m \u001b[33;03mRuns the forward pass.\u001b[39;00m\n\u001b[32m    248\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m     \u001b[38;5;28minput\u001b[39m = \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    251\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mk:\\Code\\Project\\Research Paper\\Emotion Detection\\Code\\torch_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mk:\\Code\\Project\\Research Paper\\Emotion Detection\\Code\\torch_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mk:\\Code\\Project\\Research Paper\\Emotion Detection\\Code\\torch_env\\Lib\\site-packages\\torchvision\\ops\\misc.py:260\u001b[39m, in \u001b[36mSqueezeExcitation.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    259\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m260\u001b[39m     scale = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_scale\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    261\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m scale * \u001b[38;5;28minput\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mk:\\Code\\Project\\Research Paper\\Emotion Detection\\Code\\torch_env\\Lib\\site-packages\\torchvision\\ops\\misc.py:257\u001b[39m, in \u001b[36mSqueezeExcitation._scale\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    255\u001b[39m scale = \u001b[38;5;28mself\u001b[39m.activation(scale)\n\u001b[32m    256\u001b[39m scale = \u001b[38;5;28mself\u001b[39m.fc2(scale)\n\u001b[32m--> \u001b[39m\u001b[32m257\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mscale_activation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mscale\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mk:\\Code\\Project\\Research Paper\\Emotion Detection\\Code\\torch_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mk:\\Code\\Project\\Research Paper\\Emotion Detection\\Code\\torch_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mk:\\Code\\Project\\Research Paper\\Emotion Detection\\Code\\torch_env\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:359\u001b[39m, in \u001b[36mSigmoid.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    355\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m    356\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    357\u001b[39m \u001b[33;03m    Runs the forward pass.\u001b[39;00m\n\u001b[32m    358\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m359\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43msigmoid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# hybrid_fusion_from_early.py\n",
    "import os\n",
    "import copy\n",
    "import random\n",
    "import time\n",
    "from typing import List, Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from torchvision import transforms, models\n",
    "from PIL import Image\n",
    "\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from sklearn.preprocessing import LabelEncoder, label_binarize\n",
    "from sklearn.metrics import confusion_matrix, roc_curve, auc, accuracy_score\n",
    "from sklearn.metrics import classification_report, precision_recall_fscore_support\n",
    "from tqdm import tqdm\n",
    "import soundfile as sf\n",
    "import librosa\n",
    "from mpl_toolkits.mplot3d import Axes3D  # noqa: F401 - required for 3D projection\n",
    "import gc\n",
    "\n",
    "# ==========================\n",
    "# Config / Paths / Hyperparams (kept from your original file)\n",
    "# ==========================\n",
    "CSV_PATH = r\"K:\\Code\\Project\\Research Paper\\Emotion Detection\\meld dataset\\MELD.Raw\\self\\fusion_dataset.csv\"\n",
    "IMAGE_MODEL_PATH = r\"K:\\Code\\Project\\Research Paper\\Emotion Detection\\inceptionresnetv3_face_emotion.pth\"\n",
    "TEXT_MODEL_PATH  = r\"K:\\Code\\Project\\Research Paper\\Emotion Detection\\Code\\bert_emotion_text_final.pth\"\n",
    "AUDIO_MODEL_PATH = r\"K:\\Code\\Project\\Research Paper\\Emotion Detection\\best_transformer_speech_model.pth\"\n",
    "\n",
    "# Unimodal model settings (must match your trained models)\n",
    "IMG_SIZE = 224\n",
    "BERT_MODEL_NAME = \"bert-base-multilingual-cased\"\n",
    "MAX_LEN = 64\n",
    "\n",
    "# Audio MFCC params (tweak to match your audio model)\n",
    "AUDIO_MAX_PAD = 174     # same as audio model (kept)\n",
    "AUDIO_N_MFCC = 40\n",
    "AUDIO_N_FFT = 1024\n",
    "AUDIO_HOP = 512\n",
    "AUDIO_SR = 22050\n",
    "\n",
    "# Fusion training hyperparams\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 100\n",
    "LR_FUSION = 1e-3\n",
    "LR_ENCODER = 2e-5\n",
    "WEIGHT_DECAY = 1e-5\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "FREEZE_BASE_MODELS = True\n",
    "UNFREEZE_BERT_LAST_N = 4\n",
    "GRAD_CLIP_NORM = 1.0\n",
    "USE_AMP = True\n",
    "\n",
    "BEST_FUSION_PATH = \"./best_hybrid_fusion_model.pth\"\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available(): torch.cuda.manual_seed_all(SEED)\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "# Hybrid-specific configs\n",
    "AUX_UNIMODAL_LOSS = True\n",
    "AUX_WEIGHT = 0.2  # weight for auxiliary unimodal losses (sum of three)\n",
    "BRANCH_WEIGHT_LEARNABLE = True  # learnable branch weights (softmaxed)\n",
    "# ==========================\n",
    "# Utilities (audio loading / mfcc)\n",
    "# ==========================\n",
    "def load_audio(path, sr=AUDIO_SR):\n",
    "    audio, native_sr = sf.read(path)\n",
    "    if audio is None:\n",
    "        raise RuntimeError(f\"Failed reading audio: {path}\")\n",
    "    # convert stereo -> mono\n",
    "    if audio.ndim > 1:\n",
    "        audio = np.mean(audio, axis=1)\n",
    "    # resample if needed\n",
    "    if native_sr != sr:\n",
    "        audio = librosa.resample(audio, orig_sr=native_sr, target_sr=sr)\n",
    "    return audio, sr\n",
    "\n",
    "\n",
    "def extract_mfcc(file_path, sr=AUDIO_SR, n_mfcc=AUDIO_N_MFCC, max_pad_len=AUDIO_MAX_PAD,\n",
    "                 n_fft=AUDIO_N_FFT, hop_length=AUDIO_HOP):\n",
    "\n",
    "    try:\n",
    "        signal, sr = load_audio(file_path, sr)\n",
    "    except Exception:\n",
    "        # return zeros if audio load fails\n",
    "        return np.zeros((n_mfcc, max_pad_len), dtype=np.float32)\n",
    "\n",
    "    if len(signal) < 2048:\n",
    "        signal = np.pad(signal, (0, 2048 - len(signal)))\n",
    "\n",
    "    mfcc = librosa.feature.mfcc(y=signal, sr=sr, n_mfcc=n_mfcc, n_fft=n_fft, hop_length=hop_length)\n",
    "\n",
    "    # pad/crop to fixed length\n",
    "    if mfcc.shape[1] < max_pad_len:\n",
    "        mfcc = np.pad(mfcc, ((0, 0), (0, max_pad_len - mfcc.shape[1])), mode='constant')\n",
    "    else:\n",
    "        mfcc = mfcc[:, :max_pad_len]\n",
    "\n",
    "    # per-sample normalization (avoid dividing by zero)\n",
    "    mean = mfcc.mean(axis=1, keepdims=True)\n",
    "    std = mfcc.std(axis=1, keepdims=True)\n",
    "    std[std < 1e-6] = 1.0\n",
    "    mfcc = (mfcc - mean) / std\n",
    "\n",
    "    return mfcc.astype(np.float32)\n",
    "\n",
    "# ==========================\n",
    "# Encoders (unchanged)\n",
    "# ==========================\n",
    "class EfficientNetV2_Embed(nn.Module):\n",
    "    def __init__(self, embed_dim=512, pretrained=True, version=\"s\"):\n",
    "        super().__init__()\n",
    "\n",
    "        # Select EfficientNet-V2 model\n",
    "        model_fn = {\n",
    "            \"s\": models.efficientnet_v2_s,\n",
    "            \"m\": models.efficientnet_v2_m,\n",
    "            \"l\": models.efficientnet_v2_l,\n",
    "        }[version]\n",
    "\n",
    "        if pretrained:\n",
    "            base = model_fn(weights=\"IMAGENET1K_V1\")\n",
    "        else:\n",
    "            base = model_fn(weights=None)\n",
    "\n",
    "        in_features = base.classifier[1].in_features\n",
    "        base.classifier = nn.Identity()\n",
    "        self.base = base\n",
    "\n",
    "        self.proj = nn.Sequential(\n",
    "            nn.Linear(in_features, embed_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.3)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        feat = self.base(x)\n",
    "        return self.proj(feat)\n",
    "\n",
    "\n",
    "class BERTEncoder(nn.Module):\n",
    "    def __init__(self, model_name: str, embed_dim: int = 512, dropout: float = 0.2):\n",
    "        super().__init__()\n",
    "        self.bert = BertModel.from_pretrained(model_name)\n",
    "        self.proj = nn.Sequential(\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(self.bert.config.hidden_size, embed_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.2)\n",
    "        )\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        out = self.bert(input_ids=input_ids, attention_mask=attention_mask, return_dict=True)\n",
    "        pooled = out.pooler_output\n",
    "        emb = self.proj(pooled)\n",
    "        return emb\n",
    "\n",
    "class TransformerLSTM_Embed(nn.Module):\n",
    "    def __init__(self, embed_dim=512, d_model=128, nhead=4, num_layers=2):\n",
    "        super().__init__()\n",
    "        self.feature_proj = nn.Linear(AUDIO_N_MFCC, d_model)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead,\n",
    "                                                   dim_feedforward=256, dropout=0.3, batch_first=True)\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.lstm = nn.LSTM(d_model, 128, batch_first=True)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.proj = nn.Sequential(\n",
    "            nn.Linear(128, embed_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.2)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = x.squeeze(1).permute(0, 2, 1)   # (B, T, n_mfcc)\n",
    "        x = self.feature_proj(x)            # (B, T, d_model)\n",
    "        x = self.transformer(x)             # (B, T, d_model)\n",
    "        x, _ = self.lstm(x)                 # (B, T, hidden)\n",
    "        h = self.dropout(x[:, -1, :])       # (B, 128)\n",
    "        emb = self.proj(h)                  # (B, embed_dim)\n",
    "        return emb\n",
    "\n",
    "# ==========================\n",
    "# Dataset (unchanged)\n",
    "# ==========================\n",
    "class MultimodalDataset(Dataset):\n",
    "    def __init__(self, df: pd.DataFrame, tokenizer: BertTokenizer, label_encoder: LabelEncoder,\n",
    "                 img_transform, audio_pad=AUDIO_MAX_PAD, preload: bool = False, preload_verbose: bool = True):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.le = label_encoder\n",
    "        self.img_transform = img_transform\n",
    "        self.audio_pad = audio_pad\n",
    "        self.preload = preload\n",
    "        self.cache = None\n",
    "        if self.preload:\n",
    "            self._preload_to_ram(verbose=preload_verbose)\n",
    "\n",
    "    def __len__(self): return len(self.df)\n",
    "\n",
    "    def _preload_to_ram(self, verbose: bool = True):\n",
    "        \"\"\"Load images (transformed), tokenized text tensors and MFCCs into memory lists.\"\"\"\n",
    "        self.cache = [None] * len(self.df)\n",
    "        iterator = range(len(self.df))\n",
    "        if verbose:\n",
    "            iterator = tqdm(iterator, desc=\"Preloading dataset into RAM\", ncols=80)\n",
    "        for idx in iterator:\n",
    "            row = self.df.loc[idx]\n",
    "            img_path = row['image_path']\n",
    "            txt = str(row['text'])\n",
    "            audio_path = row['audio_path']\n",
    "            label = int(row['label'])\n",
    "\n",
    "            try:\n",
    "                img = Image.open(img_path).convert(\"RGB\")\n",
    "                img_t = self.img_transform(img)\n",
    "            except Exception:\n",
    "                img_t = torch.zeros(3, IMG_SIZE, IMG_SIZE, dtype=torch.float32)\n",
    "\n",
    "            try:\n",
    "                enc = self.tokenizer(txt, padding='max_length', truncation=True, max_length=MAX_LEN, return_tensors='pt')\n",
    "                input_ids = enc['input_ids'].squeeze(0)\n",
    "                attention_mask = enc['attention_mask'].squeeze(0)\n",
    "            except Exception:\n",
    "                input_ids = torch.zeros(MAX_LEN, dtype=torch.long)\n",
    "                attention_mask = torch.zeros(MAX_LEN, dtype=torch.long)\n",
    "\n",
    "            try:\n",
    "                mfcc = extract_mfcc(audio_path, sr=AUDIO_SR, n_mfcc=AUDIO_N_MFCC, max_pad_len=self.audio_pad,\n",
    "                                    n_fft=AUDIO_N_FFT, hop_length=AUDIO_HOP)\n",
    "                mfcc_t = torch.tensor(mfcc, dtype=torch.float32).unsqueeze(0)  # (1, n_mfcc, T)\n",
    "            except Exception:\n",
    "                mfcc_t = torch.zeros(1, AUDIO_N_MFCC, self.audio_pad, dtype=torch.float32)\n",
    "            self.cache[idx] = {\n",
    "                \"image\": img_t,\n",
    "                \"input_ids\": input_ids,\n",
    "                \"attention_mask\": attention_mask,\n",
    "                \"audio\": mfcc_t,\n",
    "                \"label\": torch.tensor(label, dtype=torch.long)\n",
    "            }\n",
    "        if verbose:\n",
    "            print(f\"[PRELOAD] Finished preloading {len(self.df)} samples into RAM.\")\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.preload and (self.cache is not None):\n",
    "            item = self.cache[idx]\n",
    "            return {\n",
    "                \"image\": item['image'].clone(),\n",
    "                \"input_ids\": item['input_ids'].clone(),\n",
    "                \"attention_mask\": item['attention_mask'].clone(),\n",
    "                \"audio\": item['audio'].clone(),\n",
    "                \"label\": item['label'].clone()\n",
    "            }\n",
    "\n",
    "        row = self.df.loc[idx]\n",
    "        img_path = row['image_path']\n",
    "        txt = str(row['text'])\n",
    "        audio_path = row['audio_path']\n",
    "        label = int(row['label'])\n",
    "        # Image\n",
    "        try:\n",
    "            img = Image.open(img_path).convert(\"RGB\")\n",
    "            img_t = self.img_transform(img)\n",
    "        except Exception:\n",
    "            img_t = torch.zeros(3, IMG_SIZE, IMG_SIZE)\n",
    "        # Text\n",
    "        enc = self.tokenizer(txt, padding='max_length', truncation=True, max_length=MAX_LEN, return_tensors='pt')\n",
    "        input_ids = enc['input_ids'].squeeze(0)\n",
    "        attention_mask = enc['attention_mask'].squeeze(0)\n",
    "        # Audio -> mfcc\n",
    "        try:\n",
    "            mfcc = extract_mfcc(audio_path, sr=AUDIO_SR, n_mfcc=AUDIO_N_MFCC, max_pad_len=self.audio_pad,\n",
    "                                n_fft=AUDIO_N_FFT, hop_length=AUDIO_HOP)\n",
    "            mfcc_t = torch.tensor(mfcc, dtype=torch.float32).unsqueeze(0)  # (1, n_mfcc, T)\n",
    "        except Exception:\n",
    "            mfcc_t = torch.zeros(1, AUDIO_N_MFCC, self.audio_pad, dtype=torch.float32)\n",
    "        return {\n",
    "            \"image\": img_t,\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"audio\": mfcc_t,\n",
    "            \"label\": torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# ==========================\n",
    "# Plotting utilities (unchanged)\n",
    "# ==========================\n",
    "def plot_accuracy(train_accs, val_accs):\n",
    "    plt.figure(figsize=(7,4))\n",
    "    plt.plot(range(1,len(train_accs)+1), train_accs, marker='o', label='Train Acc')\n",
    "    plt.plot(range(1,len(val_accs)+1), val_accs, marker='o', label='Val Acc')\n",
    "    plt.xlabel(\"Epoch\"); plt.ylabel(\"Accuracy (%)\"); plt.title(\"Train vs Val Acc\")\n",
    "    plt.grid(True); plt.legend(); plt.tight_layout(); plt.show()\n",
    "\n",
    "def plot_confusion(cm, labels):\n",
    "    fig, ax = plt.subplots(figsize=(7,7))\n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "    ax.figure.colorbar(im, ax=ax)\n",
    "    ax.set(xticks=np.arange(len(labels)), yticks=np.arange(len(labels)),\n",
    "           xticklabels=labels, yticklabels=labels, xlabel='Predicted', ylabel='True', title='Confusion Matrix')\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha='right')\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(j, i, format(cm[i, j], 'd'), ha='center', va='center',\n",
    "                    color='white' if cm[i, j] > thresh else 'black')\n",
    "    plt.tight_layout(); plt.show()\n",
    "\n",
    "def plot_multiclass_roc(y_true, y_score, class_names):\n",
    "    try:\n",
    "        y_true_bin = label_binarize(y_true, classes=list(range(len(class_names))))\n",
    "        plt.figure(figsize=(8,6))\n",
    "        for i in range(y_true_bin.shape[1]):\n",
    "            fpr, tpr, _ = roc_curve(y_true_bin[:, i], y_score[:, i])\n",
    "            roc_auc = auc(fpr, tpr)\n",
    "            plt.plot(fpr, tpr, label=f\"{class_names[i]} (AUC={roc_auc:.2f})\")\n",
    "        fpr, tpr, _ = roc_curve(y_true_bin.ravel(), y_score.ravel())\n",
    "        plt.plot(fpr, tpr, label=f\"micro (AUC={auc(fpr,tpr):.2f})\", linestyle='--')\n",
    "        plt.plot([0,1],[0,1],'k--', linewidth=0.6)\n",
    "        plt.xlabel(\"FPR\"); plt.ylabel(\"TPR\"); plt.title(\"Multi-class ROC\"); plt.legend(fontsize='small'); plt.grid(True); plt.tight_layout(); plt.show()\n",
    "    except Exception as e:\n",
    "        print(\"ROC plotting failed:\", e)\n",
    "\n",
    "# ==========================\n",
    "# EarlyFusion (refactored to expose embedding and logits)\n",
    "# ==========================\n",
    "class EarlyFusionModule(nn.Module):\n",
    "    \"\"\"\n",
    "    This module computes early fusion embedding/logits similarly to your EarlyFusion class.\n",
    "    We'll use it inside HybridFusion to get early_logits.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes, embed_dim=512, d_model=512,\n",
    "                 use_transformer=True, nhead=8, n_layers=1, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.use_transformer = use_transformer\n",
    "        if embed_dim != d_model:\n",
    "            self.proj_img = nn.Linear(embed_dim, d_model)\n",
    "            self.proj_txt = nn.Linear(embed_dim, d_model)\n",
    "            self.proj_aud = nn.Linear(embed_dim, d_model)\n",
    "        else:\n",
    "            self.proj_img = nn.Identity()\n",
    "            self.proj_txt = nn.Identity()\n",
    "            self.proj_aud = nn.Identity()\n",
    "\n",
    "        self.modality_type_embed = nn.Embedding(3, d_model)\n",
    "        self.pre_ln = nn.LayerNorm(d_model)\n",
    "\n",
    "        if use_transformer:\n",
    "            if d_model % nhead != 0:\n",
    "                raise ValueError(f\"d_model ({d_model}) must be divisible by nhead ({nhead})\")\n",
    "\n",
    "            encoder_layer = nn.TransformerEncoderLayer(\n",
    "                d_model=d_model,\n",
    "                nhead=nhead,\n",
    "                dim_feedforward=d_model * 2,\n",
    "                dropout=dropout,\n",
    "                batch_first=True\n",
    "            )\n",
    "            self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=n_layers)\n",
    "\n",
    "            self.classifier = nn.Sequential(\n",
    "                nn.Linear(d_model, d_model),\n",
    "                nn.LayerNorm(d_model),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout),\n",
    "                nn.Linear(d_model, num_classes),\n",
    "            )\n",
    "        else:\n",
    "            self.classifier = nn.Sequential(\n",
    "                nn.Linear(d_model * 3, d_model),\n",
    "                nn.ReLU(),\n",
    "                nn.LayerNorm(d_model),\n",
    "                nn.Dropout(dropout),\n",
    "                nn.Linear(d_model, num_classes),\n",
    "            )\n",
    "\n",
    "        self.modality_scale = nn.Parameter(torch.ones(3))\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.zeros_(m.bias)\n",
    "\n",
    "    def forward(self, e_img, e_txt, e_aud):\n",
    "        t_img = self.proj_img(e_img)\n",
    "        t_txt = self.proj_txt(e_txt)\n",
    "        t_aud = self.proj_aud(e_aud)\n",
    "\n",
    "        scales = torch.relu(self.modality_scale)\n",
    "        t_img = t_img * scales[0]\n",
    "        t_txt = t_txt * scales[1]\n",
    "        t_aud = t_aud * scales[2]\n",
    "\n",
    "        if self.use_transformer:\n",
    "            tokens = torch.stack([t_img, t_txt, t_aud], dim=1)  # (B, 3, d_model)\n",
    "            B = tokens.size(0)\n",
    "            mod_ids = torch.tensor([0, 1, 2], device=tokens.device).unsqueeze(0).repeat(B, 1)\n",
    "            tokens = tokens + self.modality_type_embed(mod_ids)\n",
    "            tokens = self.pre_ln(tokens)\n",
    "            tokens = self.transformer(tokens)\n",
    "            pooled = tokens.mean(dim=1)  # mean-pool over 3 modalities\n",
    "            logits = self.classifier(pooled)\n",
    "            return logits, pooled  # return both logits and fused embedding\n",
    "        else:\n",
    "            concat = torch.cat([t_img, t_txt, t_aud], dim=1)\n",
    "            logits = self.classifier(concat)\n",
    "            return logits, concat\n",
    "\n",
    "# ==========================\n",
    "# HybridFusion (logit-level)\n",
    "# ==========================\n",
    "# ==== PATCHED SNIPPET ====\n",
    "\n",
    "class HybridFusion(nn.Module):\n",
    "    \"\"\"\n",
    "    Logit-level hybrid fusion:\n",
    "      - unimodal heads -> produce img_logits, txt_logits, aud_logits\n",
    "      - early-fusion module -> early_logits\n",
    "      - final logits = weighted sum of [early_logits, img_logits, txt_logits, aud_logits]\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes, embed_dim=512, d_model=512, use_transformer=True,\n",
    "                 nhead=8, n_layers=1, dropout=0.2, aux_unimodal_loss=True):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.aux_unimodal_loss = aux_unimodal_loss\n",
    "\n",
    "        # Early fusion branch (reuse the logic)\n",
    "        self.early = EarlyFusionModule(num_classes=num_classes, embed_dim=embed_dim,\n",
    "                                      d_model=d_model, use_transformer=use_transformer,\n",
    "                                      nhead=nhead, n_layers=n_layers, dropout=dropout)\n",
    "\n",
    "        # Unimodal heads (simple linear classifiers from embed_dim -> num_classes)\n",
    "        self.img_head = nn.Sequential(nn.Linear(embed_dim, num_classes))\n",
    "        self.txt_head = nn.Sequential(nn.Linear(embed_dim, num_classes))\n",
    "        self.aud_head = nn.Sequential(nn.Linear(embed_dim, num_classes))\n",
    "\n",
    "        # Branch weights: [early, img, txt, aud] -> softmaxed to get contribution\n",
    "        if BRANCH_WEIGHT_LEARNABLE:\n",
    "            self.branch_logits = nn.Parameter(torch.zeros(4))  # learnable raw weights\n",
    "        else:\n",
    "            self.register_buffer('branch_logits', torch.zeros(4))\n",
    "\n",
    "        # small gating MLP example (not used by default; could be used to condition weights on inputs)\n",
    "        # keep for future extension; not used in forward unless you swap logic\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.zeros_(m.bias)\n",
    "\n",
    "    def forward(self, e_img, e_txt, e_aud):\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "          final_logits: combined logits used for CE\n",
    "          dict with components: { 'early_logits', 'img_logits', 'txt_logits', 'aud_logits', 'branch_weights' }\n",
    "        \"\"\"\n",
    "        img_logits = self.img_head(e_img)    # (B, C)\n",
    "        txt_logits = self.txt_head(e_txt)\n",
    "        aud_logits = self.aud_head(e_aud)\n",
    "        early_logits, early_emb = self.early(e_img, e_txt, e_aud)\n",
    "\n",
    "        # compute branch weights (softmax over 4)\n",
    "        bw = F.softmax(self.branch_logits, dim=0)  # (4,)\n",
    "        # broadcast to batch\n",
    "        batch_bw = bw.unsqueeze(0)  # (1,4)\n",
    "\n",
    "        # weighted sum of logits\n",
    "        # order: early, img, txt, aud\n",
    "        stacked = torch.stack([early_logits, img_logits, txt_logits, aud_logits], dim=2)  # (B, C, 4)\n",
    "        # multiply by weights -> sum across branch dim\n",
    "        final_logits = (stacked * batch_bw.unsqueeze(1)).sum(dim=2)  # (B, C)\n",
    "\n",
    "        return final_logits, {\n",
    "            \"early_logits\": early_logits,\n",
    "            \"img_logits\": img_logits,\n",
    "            \"txt_logits\": txt_logits,\n",
    "            \"aud_logits\": aud_logits,\n",
    "            \"branch_weights\": bw\n",
    "        }\n",
    "\n",
    "\n",
    "# ==========================\n",
    "# Loading unimodal models (unchanged except returns)\n",
    "# ==========================\n",
    "def load_unimodal_models(num_classes, image_model_path, text_model_path, audio_model_path, device, embed_dim=512):\n",
    "    img_model = EfficientNetV2_Embed(embed_dim=embed_dim, pretrained=True,version=\"s\")\n",
    "    if os.path.exists(image_model_path):\n",
    "        st = torch.load(image_model_path, map_location=device)\n",
    "        try:\n",
    "            if isinstance(st, dict):\n",
    "                candidate = None\n",
    "                for k in ('model','state_dict','model_state','model_state_dict'):\n",
    "                    if k in st and isinstance(st[k], dict):\n",
    "                        candidate = st[k]; break\n",
    "                if candidate is not None:\n",
    "                    st = candidate\n",
    "                st_clean = {k:v for k,v in st.items() if not (k.startswith(\"fc.\") or k.startswith(\"classifier.\") or ('fc' in k and k.endswith('weight')))}\n",
    "                img_model.load_state_dict(st_clean, strict=False)\n",
    "            else:\n",
    "                img_model.load_state_dict(st, strict=False)\n",
    "            print(\"[INFO] Loaded image checkpoint (partial load allowed).\")\n",
    "        except Exception as e:\n",
    "            print(\"[WARN] Failed to load image checkpoint cleanly:\", e)\n",
    "    img_model.to(device)\n",
    "\n",
    "    txt_model = BERTEncoder(BERT_MODEL_NAME, embed_dim=embed_dim)\n",
    "    if os.path.exists(text_model_path):\n",
    "        st = torch.load(text_model_path, map_location=device)\n",
    "        try:\n",
    "            if isinstance(st, dict):\n",
    "                candidate = None\n",
    "                for k in ('model','state_dict','model_state','model_state_dict'):\n",
    "                    if k in st and isinstance(st[k], dict):\n",
    "                        candidate = st[k]; break\n",
    "                if candidate is not None:\n",
    "                    st = candidate\n",
    "\n",
    "                st_no_classifier = {k:v for k,v in st.items() if not k.startswith(\"classifier.\")}\n",
    "                txt_model.load_state_dict(st_no_classifier, strict=False)\n",
    "            else:\n",
    "                txt_model.load_state_dict(st, strict=False)\n",
    "            print(\"[INFO] Loaded text checkpoint (partial load).\")\n",
    "        except Exception as e:\n",
    "            print(\"[WARN] Failed to load text checkpoint:\", e)\n",
    "    txt_model.to(device)\n",
    "\n",
    "    aud_model = TransformerLSTM_Embed(embed_dim=embed_dim)\n",
    "    if os.path.exists(audio_model_path):\n",
    "        st = torch.load(audio_model_path, map_location=device)\n",
    "        try:\n",
    "            if isinstance(st, dict):\n",
    "                candidate = None\n",
    "                for k in ('model','state_dict','model_state','model_state_dict'):\n",
    "                    if k in st and isinstance(st[k], dict):\n",
    "                        candidate = st[k]; break\n",
    "                if candidate is not None:\n",
    "                    st = candidate\n",
    "                aud_model.load_state_dict(st, strict=False)\n",
    "            else:\n",
    "                aud_model.load_state_dict(st, strict=False)\n",
    "            print(\"[INFO] Loaded audio checkpoint (partial load).\")\n",
    "        except Exception as e:\n",
    "            print(\"[WARN] Failed to load audio checkpoint:\", e)\n",
    "    aud_model.to(device)\n",
    "\n",
    "    return img_model, txt_model, aud_model\n",
    "\n",
    "\n",
    "# ==========================\n",
    "# Freeze utilities (unchanged)\n",
    "# ==========================\n",
    "def set_requires_grad(model, requires_grad: bool):\n",
    "    for p in model.parameters():\n",
    "        p.requires_grad = requires_grad\n",
    "\n",
    "def unfreeze_efficientnet(img_model, depth=2):\n",
    "    for p in img_model.base.parameters():\n",
    "        p.requires_grad = False\n",
    "\n",
    "    stages = [\n",
    "        img_model.base.features[2],  # Stage 3\n",
    "        img_model.base.features[3],  # Stage 4\n",
    "        img_model.base.features[4],  # Stage 5\n",
    "        img_model.base.features[5],  # Stage 6\n",
    "    ]\n",
    "    for s in stages[-depth:]:\n",
    "        for p in s.parameters():\n",
    "            p.requires_grad = True\n",
    "    for p in img_model.proj.parameters():\n",
    "        p.requires_grad = True\n",
    "    print(f\"[INFO] Unfroze last {depth} EfficientNetV2 stages + projection layer.\")\n",
    "\n",
    "def freeze_bert_layers(bert_model: BertModel, unfreeze_last_n: int = 4):\n",
    "    for name, p in bert_model.named_parameters():\n",
    "        p.requires_grad = False\n",
    "\n",
    "    for name, p in bert_model.named_parameters():\n",
    "        if name.startswith(\"embeddings.\") or name.startswith(\"pooler.\") or \"LayerNorm\" in name or \"layer_norm\" in name:\n",
    "            p.requires_grad = True\n",
    "\n",
    "    try:\n",
    "        total = bert_model.config.num_hidden_layers\n",
    "        for i in range(total - unfreeze_last_n, total):\n",
    "            prefix = f\"encoder.layer.{i}.\"\n",
    "            for name, p in bert_model.named_parameters():\n",
    "                if name.startswith(prefix):\n",
    "                    p.requires_grad = True\n",
    "    except Exception:\n",
    "        for name, p in bert_model.named_parameters():\n",
    "            if \"encoder.layer\" in name and any(f\"encoder.layer.{j}.\" in name for j in range(max(0, total-unfreeze_last_n), total)):\n",
    "                p.requires_grad = True\n",
    "\n",
    "\n",
    "# ==========================\n",
    "# Evaluation utilities for hybrid\n",
    "# ==========================\n",
    "def evaluate_full_hybrid(model_components, hybrid_head, loader, device, class_names):\n",
    "    img_model, txt_model, aud_model = model_components\n",
    "    hybrid_head.eval()\n",
    "    img_model.eval(); txt_model.eval(); aud_model.eval()\n",
    "\n",
    "    all_true = []\n",
    "    all_pred = []\n",
    "    all_probs = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "\n",
    "            img = batch['image'].to(device, non_blocking=True)\n",
    "            input_ids = batch['input_ids'].to(device, non_blocking=True)\n",
    "            attention_mask = batch['attention_mask'].to(device, non_blocking=True)\n",
    "            audio = batch['audio'].to(device, non_blocking=True)\n",
    "            labels = batch['label'].to(device, non_blocking=True)\n",
    "\n",
    "            e_img = img_model(img)\n",
    "            e_txt = txt_model(input_ids, attention_mask)\n",
    "            e_aud = aud_model(audio)\n",
    "\n",
    "            final_logits, comps = hybrid_head(e_img, e_txt, e_aud)\n",
    "\n",
    "            probs = F.softmax(final_logits, dim=1)\n",
    "\n",
    "            # ---- FIX: detach CPU copy immediately ----\n",
    "            all_true.append(labels.detach().cpu().numpy())\n",
    "            all_pred.append(probs.argmax(dim=1).detach().cpu().numpy())\n",
    "            all_probs.append(probs.detach().cpu().numpy())\n",
    "\n",
    "            # free everything used in this batch (including comps so tensors don't stick around)\n",
    "            del img, input_ids, attention_mask, audio, labels\n",
    "            del e_img, e_txt, e_aud, final_logits, probs\n",
    "            # comps may contain tensors; remove reference\n",
    "            try:\n",
    "                del comps\n",
    "            except Exception:\n",
    "                pass\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    # concatenate on CPU\n",
    "    y_true = np.concatenate(all_true)\n",
    "    y_pred = np.concatenate(all_pred)\n",
    "    y_score = np.vstack(all_probs)\n",
    "\n",
    "    acc = accuracy_score(y_true, y_pred) * 100.0\n",
    "    print(f\"[EVAL] Accuracy: {acc:.2f}%\")\n",
    "\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=list(range(len(class_names))))\n",
    "    return acc, cm, y_true, y_score\n",
    "\n",
    "\n",
    "def validate_epoch_hybrid(model_components, hybrid_head, val_loader, device, num_classes):\n",
    "    img_model, txt_model, aud_model = model_components\n",
    "\n",
    "    hybrid_head.eval()\n",
    "    img_model.eval(); txt_model.eval(); aud_model.eval()\n",
    "\n",
    "    preds_list = []\n",
    "    labels_list = []\n",
    "    probs_list = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "\n",
    "            img = batch['image'].to(device, non_blocking=True)\n",
    "            input_ids = batch['input_ids'].to(device, non_blocking=True)\n",
    "            attention_mask = batch['attention_mask'].to(device, non_blocking=True)\n",
    "            audio = batch['audio'].to(device, non_blocking=True)\n",
    "            labels = batch['label'].to(device, non_blocking=True)\n",
    "\n",
    "            with torch.amp.autocast('cuda', enabled=USE_AMP):\n",
    "                e_img = img_model(img)\n",
    "                e_txt = txt_model(input_ids, attention_mask)\n",
    "                e_aud = aud_model(audio)\n",
    "                out, comps = hybrid_head(e_img, e_txt, e_aud)\n",
    "\n",
    "            # detach → CPU\n",
    "            preds_list.append(out.argmax(dim=1).detach().cpu())\n",
    "            labels_list.append(labels.detach().cpu())\n",
    "            probs_list.append(F.softmax(out, dim=1).detach().cpu())\n",
    "\n",
    "            # only free this\n",
    "            del comps\n",
    "\n",
    "            # DO NOT: delete batch tensors\n",
    "            # DO NOT: empty cuda cache\n",
    "\n",
    "    # concat on CPU\n",
    "    y_pred = torch.cat(preds_list).numpy()\n",
    "    y_true = torch.cat(labels_list).numpy()\n",
    "    y_score = np.vstack([p.numpy() for p in probs_list])\n",
    "\n",
    "    acc = (y_pred == y_true).mean() * 100.0\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=list(range(num_classes)))\n",
    "\n",
    "    return acc, cm, y_true, y_score\n",
    "\n",
    "\n",
    "# ==========================\n",
    "# PSO helper (keeps compatibility)\n",
    "# ==========================\n",
    "def visualize_pso_3d(particles, best_particle=None, metric_names=(\"Accuracy\", \"Precision\", \"Recall\")):\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "    particles = np.array(particles)\n",
    "    fig = plt.figure(figsize=(10, 7))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    if particles.size == 0:\n",
    "        print(\"[PSO VIS] no particles to display\")\n",
    "        plt.close(fig)\n",
    "        return\n",
    "    ax.scatter(particles[:, 0], particles[:, 1], particles[:, 2], s=65, alpha=0.7)\n",
    "    if best_particle is not None:\n",
    "        best_particle = np.array(best_particle)\n",
    "        ax.scatter(best_particle[0], best_particle[1], best_particle[2],\n",
    "                   s=250, color='red', edgecolor='black', marker='o', label=\"Best Particle\")\n",
    "        ax.legend()\n",
    "    ax.set_xlabel(metric_names[0])\n",
    "    ax.set_ylabel(metric_names[1])\n",
    "    ax.set_zlabel(metric_names[2])\n",
    "    ax.set_title(\"3D PSO Per-Iteration Optimization\")\n",
    "    plt.show()\n",
    "    # Close the figure to avoid matplotlib memory leak\n",
    "    plt.close(fig)\n",
    "\n",
    "\n",
    "def run_simple_pso_evaluate(model_components, train_loader, val_loader, class_names,\n",
    "                            swarm_size=6, iters=3, device=DEVICE, quick_batches=8):\n",
    "    print(f\"[PSO] Starting simple PSO-like search: swarm={swarm_size}, iters={iters}\")\n",
    "    img_model, txt_model, aud_model = model_components\n",
    "    all_iterations_metrics = []\n",
    "\n",
    "    particles = []\n",
    "    velocities = []\n",
    "    for _ in range(swarm_size):\n",
    "        lr = 10 ** np.random.uniform(-5, -2)\n",
    "        dropout = np.random.uniform(0.0, 0.5)\n",
    "        d_ratio = np.random.uniform(0.5, 1.0)\n",
    "        particles.append([lr, dropout, d_ratio])\n",
    "        velocities.append([0.0, 0.0, 0.0])\n",
    "    pbest = particles.copy()\n",
    "    pbest_scores = [-1.0] * swarm_size\n",
    "    gbest = None\n",
    "    gbest_score = -1.0\n",
    "    for it in range(iters):\n",
    "        print(f\"[PSO] Iteration {it+1}/{iters}\")\n",
    "        iter_metrics = []\n",
    "        best_particle_metric = None\n",
    "        best_particle_acc = -1\n",
    "        for i, p in enumerate(particles):\n",
    "            lr, dropout, d_ratio = p\n",
    "            d_model = int(512 * float(d_ratio))\n",
    "            d_model = max(4, int(d_model // 4) * 4)\n",
    "\n",
    "            nhead = 4\n",
    "            if d_model % 8 == 0:\n",
    "                nhead = 8\n",
    "            elif d_model % 4 == 0:\n",
    "                nhead = 4\n",
    "            else:\n",
    "                nhead = 1\n",
    "\n",
    "            # Build a temporary Hybrid model with particle hyperparams\n",
    "            hybrid_head = HybridFusion(num_classes=len(class_names), embed_dim=512, d_model=d_model,\n",
    "                                       use_transformer=True, nhead=nhead, n_layers=1, dropout=dropout,\n",
    "                                       aux_unimodal_loss=AUX_UNIMODAL_LOSS).to(device)\n",
    "\n",
    "            # freeze encoders during PSO evaluation (as in original)\n",
    "            hybrid_head.train()\n",
    "            img_model.eval(); txt_model.eval(); aud_model.eval()\n",
    "\n",
    "            opt = torch.optim.AdamW(hybrid_head.parameters(), lr=lr, weight_decay=WEIGHT_DECAY)\n",
    "            criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "            batch_iter = iter(train_loader)\n",
    "            for b_idx in range(quick_batches):\n",
    "                try:\n",
    "                    batch = next(batch_iter)\n",
    "                except StopIteration:\n",
    "                    batch_iter = iter(train_loader)\n",
    "                    batch = next(batch_iter)\n",
    "                img = batch['image'].to(device, non_blocking=True)\n",
    "                input_ids = batch['input_ids'].to(device, non_blocking=True)\n",
    "                attention_mask = batch['attention_mask'].to(device, non_blocking=True)\n",
    "                audio = batch['audio'].to(device, non_blocking=True)\n",
    "                labels = batch['label'].to(device, non_blocking=True)\n",
    "                opt.zero_grad()\n",
    "                with torch.amp.autocast('cuda', enabled=USE_AMP):\n",
    "                    e_img = img_model(img)\n",
    "                    e_txt = txt_model(input_ids, attention_mask)\n",
    "                    e_aud = aud_model(audio)\n",
    "                    final_logits, comps = hybrid_head(e_img, e_txt, e_aud)\n",
    "                    loss = criterion(final_logits, labels)\n",
    "                    if AUX_UNIMODAL_LOSS:\n",
    "                        loss = loss + AUX_WEIGHT * (criterion(comps['img_logits'], labels) +\n",
    "                                                    criterion(comps['txt_logits'], labels) +\n",
    "                                                    criterion(comps['aud_logits'], labels))\n",
    "                loss.backward()\n",
    "                opt.step()\n",
    "\n",
    "                # delete comps and per-batch tensors used by the temporary hybrid to free memory promptly\n",
    "                try:\n",
    "                    del comps\n",
    "                except Exception:\n",
    "                    pass\n",
    "                del img, input_ids, attention_mask, audio, labels, e_img, e_txt, e_aud, final_logits, loss\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "            # Evaluate the temporary hybrid_head on validation (this returns numpy arrays)\n",
    "            val_acc, _, y_true, y_score = evaluate_full_hybrid(\n",
    "                (img_model, txt_model, aud_model),\n",
    "                hybrid_head, val_loader, device, class_names\n",
    "            )\n",
    "\n",
    "            # Immediately free the temporary hybrid model and optimizer\n",
    "            try:\n",
    "                del hybrid_head\n",
    "            except Exception:\n",
    "                pass\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "            y_pred = np.argmax(y_score, axis=1)\n",
    "            precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "                y_true, y_pred, average='macro', zero_division=0\n",
    "            )\n",
    "            print(f\"[PSO] Particle {i} -> Acc: {val_acc:.2f} Prec: {precision:.4f} Rec: {recall:.4f}\")\n",
    "            metric_vec = [val_acc, precision*100, recall*100]\n",
    "            iter_metrics.append(metric_vec)\n",
    "            if val_acc > best_particle_acc:\n",
    "                best_particle_acc = val_acc\n",
    "                best_particle_metric = metric_vec\n",
    "            if val_acc > pbest_scores[i]:\n",
    "                pbest_scores[i] = val_acc\n",
    "                pbest[i] = p\n",
    "            if val_acc > gbest_score:\n",
    "                gbest_score = val_acc\n",
    "                gbest = p\n",
    "        all_iterations_metrics.append(iter_metrics)\n",
    "\n",
    "        print(f\"[PSO] Visualizing iteration {it+1}\")\n",
    "        visualize_pso_3d(particles=iter_metrics, best_particle=best_particle_metric,\n",
    "                         metric_names=(\"Val Acc (%)\", \"Precision (%)\", \"Recall (%)\"))\n",
    "\n",
    "        for i in range(swarm_size):\n",
    "            inertia = 0.5\n",
    "            cognitive = 0.8\n",
    "            social = 0.9\n",
    "            r1 = np.random.rand(3)\n",
    "            r2 = np.random.rand(3)\n",
    "            v = np.array(velocities[i])\n",
    "            pb = np.array(pbest[i])\n",
    "            gb = np.array(gbest) if gbest is not None else np.array(particles[i])\n",
    "            pos = np.array(particles[i])\n",
    "            v = inertia * v + cognitive * r1 * (pb - pos) + social * r2 * (gb - pos)\n",
    "            new_pos = pos + v\n",
    "            particles[i] = [\n",
    "                float(np.clip(new_pos[0], 1e-6, 1e-1)),\n",
    "                float(np.clip(new_pos[1], 0.0, 0.8)),\n",
    "                float(np.clip(new_pos[2], 0.3, 1.2))\n",
    "            ]\n",
    "            velocities[i] = v.tolist()\n",
    "    print(f\"[PSO] Done. Best acc found: {gbest_score:.2f} (particle hyperparams {gbest})\")\n",
    "    return all_iterations_metrics\n",
    "\n",
    "\n",
    "# ==========================\n",
    "# Training loop (converted to hybrid)\n",
    "# ==========================\n",
    "def train_hybrid_fusion(csv_path=CSV_PATH, image_model_path=IMAGE_MODEL_PATH,\n",
    "                        text_model_path=TEXT_MODEL_PATH, audio_model_path=AUDIO_MODEL_PATH,\n",
    "                        freeze_bases=FREEZE_BASE_MODELS, device=DEVICE,\n",
    "                        run_pso=False, preload_dataset: bool = True):\n",
    "    if not os.path.exists(csv_path):\n",
    "        raise FileNotFoundError(f\"CSV mapping not found: {csv_path}\")\n",
    "    df = pd.read_csv(csv_path)\n",
    "    required_cols = {'image_path', 'text', 'audio_path', 'label'}\n",
    "    if not required_cols.issubset(set(df.columns)):\n",
    "        raise RuntimeError(f\"CSV must have columns: {required_cols}\")\n",
    "    # label encoding\n",
    "    if df['label'].dtype == object:\n",
    "        le = LabelEncoder()\n",
    "        df['label'] = le.fit_transform(df['label'].astype(str))\n",
    "        class_names = list(le.classes_)\n",
    "    else:\n",
    "        le = None\n",
    "        class_names = sorted(df['label'].unique().tolist())\n",
    "        class_names = [str(int(x)) for x in class_names]\n",
    "    num_classes = int(df['label'].nunique())\n",
    "    print(f\"Found {len(df)} paired samples. Classes: {num_classes} -> {class_names}\")\n",
    "    idxs = list(range(len(df)))\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    train_idx, val_idx = train_test_split(idxs, test_size=0.2, random_state=SEED, stratify=df['label'])\n",
    "    train_df = df.loc[train_idx].reset_index(drop=True)\n",
    "    val_df = df.loc[val_idx].reset_index(drop=True)\n",
    "    tokenizer = BertTokenizer.from_pretrained(BERT_MODEL_NAME, use_fast=True)\n",
    "    img_transform = transforms.Compose([\n",
    "        transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])\n",
    "    ])\n",
    "\n",
    "    train_ds = MultimodalDataset(train_df, tokenizer, le, img_transform, preload=preload_dataset)\n",
    "    val_ds   = MultimodalDataset(val_df, tokenizer, le, img_transform, preload=preload_dataset)\n",
    "\n",
    "    # Use a safe number of workers and enable persistent_workers to avoid worker restart fragmentation on Windows\n",
    "    num_workers = 0\n",
    "    persistent = True if num_workers > 0 else False\n",
    "    pin_mem = True if device.type == 'cuda' else False\n",
    "    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, pin_memory=pin_mem,\n",
    "                              num_workers=num_workers, persistent_workers=persistent)\n",
    "    val_loader   = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, pin_memory=pin_mem,\n",
    "                              num_workers=num_workers, persistent_workers=persistent)\n",
    "\n",
    "    EMBED_DIM = 512\n",
    "    img_model, txt_model, aud_model = load_unimodal_models(num_classes, image_model_path, text_model_path, audio_model_path, device, embed_dim=EMBED_DIM)\n",
    "\n",
    "    # freezing logic (kept from your code)\n",
    "    if FREEZE_BASE_MODELS:\n",
    "        set_requires_grad(img_model, False)\n",
    "        set_requires_grad(txt_model, False)\n",
    "        set_requires_grad(aud_model, False)\n",
    "\n",
    "        for p in img_model.base.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "        # EfficientNetV2 stages (features[2]–[5])\n",
    "        stages = [\n",
    "            img_model.base.features[2],  # Stage 3\n",
    "            img_model.base.features[3],  # Stage 4\n",
    "            img_model.base.features[4],  # Stage 5\n",
    "            img_model.base.features[5],  # Stage 6\n",
    "        ]\n",
    "        # unfreeze last 2 stages: Stage 5 & 6\n",
    "        for s in stages[-2:]:\n",
    "            for p in s.parameters():\n",
    "                p.requires_grad = True\n",
    "        # projection head always trainable\n",
    "        set_requires_grad(img_model.proj, True)\n",
    "        print(\"[INFO] Unfroze EfficientNetV2 last 2 stages + projection.\")\n",
    "\n",
    "        for name, p in aud_model.named_parameters():\n",
    "            if \"transformer.layers.1\" in name or \"lstm\" in name:\n",
    "                p.requires_grad = True\n",
    "        # audio projection always trainable\n",
    "        set_requires_grad(aud_model.proj, True)\n",
    "        print(\"[INFO] Unfroze audio last transformer layer + LSTM + projection.\")\n",
    "\n",
    "        freeze_bert_layers(txt_model.bert, UNFREEZE_BERT_LAST_N)\n",
    "        print(f\"[INFO] Unfroze last {UNFREEZE_BERT_LAST_N} BERT layers.\")\n",
    "    else:\n",
    "        # No freezing at all\n",
    "        set_requires_grad(img_model, True)\n",
    "        set_requires_grad(txt_model, True)\n",
    "        set_requires_grad(aud_model, True)\n",
    "        print(\"[INFO] All encoders fully trainable.\")\n",
    "\n",
    "    # Build hybrid head\n",
    "    hybrid_head = HybridFusion(num_classes=num_classes, embed_dim=EMBED_DIM, d_model=256,\n",
    "                               use_transformer=True, nhead=4, n_layers=1, dropout=0.2,\n",
    "                               aux_unimodal_loss=AUX_UNIMODAL_LOSS).to(device)\n",
    "\n",
    "    # Build optimizer: include unfrozen encoder params + hybrid params\n",
    "    encoder_params = []\n",
    "    for m in (img_model, txt_model, aud_model):\n",
    "        for p in m.parameters():\n",
    "            if p.requires_grad:\n",
    "                encoder_params.append(p)\n",
    "\n",
    "    # We'll use an optimizer similar to what you used (custom lrs)\n",
    "    optimizer = torch.optim.AdamW([\n",
    "        {\"params\": img_model.parameters(),   \"lr\": 1e-4, \"weight_decay\": 0.01},\n",
    "        {\"params\": txt_model.parameters(),  \"lr\": 6e-5, \"weight_decay\": 0.01},\n",
    "        {\"params\": aud_model.parameters(), \"lr\": 1e-4, \"weight_decay\": 0.0},\n",
    "        {\"params\": hybrid_head.parameters(), \"lr\": 5e-4, \"weight_decay\": 0.0},\n",
    "    ])\n",
    "\n",
    "    total_steps = int(EPOCHS * len(train_loader))\n",
    "    warmup_steps = max(1, int(0.1 * total_steps))\n",
    "    def lr_lambda(current_step):\n",
    "        if current_step < warmup_steps:\n",
    "            return float(current_step) / float(max(1, warmup_steps))\n",
    "        return max(0.0, float(total_steps - current_step) / float(max(1, total_steps - warmup_steps)))\n",
    "    scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    best_val_acc = -1.0\n",
    "    best_state = None\n",
    "    # Keep histories but they are small; don't store huge per-epoch objects\n",
    "    train_accs, val_accs = [], []\n",
    "\n",
    "    scaler = torch.cuda.amp.GradScaler(enabled=USE_AMP)\n",
    "\n",
    "    for epoch in range(1, EPOCHS+1):\n",
    "        t0 = time.time()\n",
    "        hybrid_head.train()\n",
    "        img_model.train(); txt_model.train(); aud_model.train()\n",
    "        running_correct = 0\n",
    "        running_total = 0\n",
    "        pbar = tqdm(train_loader, desc=f\"Epoch {epoch}/{EPOCHS}\", leave=False)\n",
    "        for batch in pbar:\n",
    "            img = batch['image'].to(device, non_blocking=True)\n",
    "            input_ids = batch['input_ids'].to(device, non_blocking=True)\n",
    "            attention_mask = batch['attention_mask'].to(device, non_blocking=True)\n",
    "            audio = batch['audio'].to(device, non_blocking=True)\n",
    "            labels = batch['label'].to(device, non_blocking=True)\n",
    "\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "            with torch.amp.autocast('cuda', enabled=USE_AMP):\n",
    "                e_img = img_model(img)\n",
    "                e_txt = txt_model(input_ids, attention_mask)\n",
    "                e_aud = aud_model(audio)\n",
    "\n",
    "                final_logits, comps = hybrid_head(e_img, e_txt, e_aud)\n",
    "\n",
    "                loss = criterion(final_logits, labels)\n",
    "\n",
    "                if AUX_UNIMODAL_LOSS:\n",
    "                    loss = loss + AUX_WEIGHT * (\n",
    "                        criterion(comps['img_logits'], labels) +\n",
    "                        criterion(comps['txt_logits'], labels) +\n",
    "                        criterion(comps['aud_logits'], labels)\n",
    "                    )\n",
    "\n",
    "            if USE_AMP:\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.unscale_(optimizer)\n",
    "                torch.nn.utils.clip_grad_norm_(\n",
    "                    [p for g in optimizer.param_groups for p in g['params'] if p.requires_grad],\n",
    "                    GRAD_CLIP_NORM\n",
    "                )\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "            else:\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(\n",
    "                    [p for g in optimizer.param_groups for p in g['params'] if p.requires_grad],\n",
    "                    GRAD_CLIP_NORM\n",
    "                )\n",
    "                optimizer.step()\n",
    "\n",
    "            # --- compute train acc (CPU only) ---\n",
    "            preds = final_logits.argmax(dim=1)\n",
    "            running_total += labels.size(0)\n",
    "            running_correct += (preds == labels).sum().item()\n",
    "            train_acc = 100.0 * running_correct / running_total\n",
    "\n",
    "            # detach BEFORE sending to tqdm\n",
    "            loss_value = float(loss.detach().cpu())\n",
    "\n",
    "            pbar.set_postfix({\"TrainAcc\": f\"{train_acc:.2f}%\", \"Loss\": f\"{loss_value:.4f}\"})\n",
    "\n",
    "            # --- FREE ALL GPU TENSORS EACH BATCH ---\n",
    "            try:\n",
    "                del comps\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "            \n",
    "\n",
    "        scheduler.step()\n",
    "        train_accs.append(train_acc)\n",
    "\n",
    "        # Run validation\n",
    "        val_acc, cm, y_true_val, y_score_val = validate_epoch_hybrid((img_model, txt_model, aud_model), hybrid_head, val_loader, device, num_classes)\n",
    "        val_accs.append(val_acc)\n",
    "        print(f\"Epoch {epoch}/{EPOCHS} - Train Acc: {train_acc:.2f}% | Val Acc: {val_acc:.2f}% | Time: {(time.time()-t0):.1f}s\")\n",
    "\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            best_state = {\n",
    "                \"epoch\": epoch,\n",
    "                \"hybrid_state\": copy.deepcopy(hybrid_head.state_dict()),\n",
    "                \"img_state\": copy.deepcopy(img_model.state_dict()),\n",
    "                \"txt_state\": copy.deepcopy(txt_model.state_dict()),\n",
    "                \"aud_state\": copy.deepcopy(aud_model.state_dict()),\n",
    "                \"val_acc\": val_acc,\n",
    "                \"class_names\": class_names\n",
    "            }\n",
    "            torch.save(best_state, BEST_FUSION_PATH)\n",
    "            print(f\" ✅ Best hybrid-fusion model saved -> {BEST_FUSION_PATH} (Val Acc {val_acc:.2f}%)\")\n",
    "\n",
    "        # Optionally run PSO less frequently to avoid explosion (you can change frequency)\n",
    "        if run_pso and (epoch % 10 == 0):\n",
    "            PSO_SWARM = 6\n",
    "            PSO_ITERS = 3\n",
    "            QUICK_BATCHES = 6\n",
    "            particle_metrics = run_simple_pso_evaluate((img_model, txt_model, aud_model),\n",
    "                                                       train_loader, val_loader, class_names,\n",
    "                                                       swarm_size=PSO_SWARM, iters=PSO_ITERS, device=device,\n",
    "                                                       quick_batches=QUICK_BATCHES)\n",
    "\n",
    "            flattened = [m for iter_batch in particle_metrics for m in iter_batch]\n",
    "            visualize_pso_3d(flattened, metric_names=(\"Val Acc (%)\", \"Precision (pct)\", \"Recall (pct)\"))\n",
    "        \n",
    "    # final plotting & cleanup\n",
    "    plot_accuracy(train_accs, val_accs)\n",
    "    # close any matplotlib figures that helper functions might have created\n",
    "    try:\n",
    "        import matplotlib.pyplot as plt\n",
    "        plt.close('all')\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    if best_state is not None:\n",
    "        print(f\"Loaded best hybrid fusion (epoch {best_state['epoch']}, val acc {best_state['val_acc']:.2f}%)\")\n",
    "        hybrid_head.load_state_dict(best_state['hybrid_state'])\n",
    "        img_model.load_state_dict(best_state['img_state'], strict=False)\n",
    "        txt_model.load_state_dict(best_state['txt_state'], strict=False)\n",
    "        aud_model.load_state_dict(best_state['aud_state'], strict=False)\n",
    "\n",
    "    final_acc, final_cm, y_true, y_score = evaluate_full_hybrid((img_model, txt_model, aud_model), hybrid_head, val_loader, device, class_names)\n",
    "    print_classification_metrics(y_true, np.argmax(y_score, axis=1), class_names)\n",
    "\n",
    "    # plotting helpers: close figures afterwards to avoid leaking\n",
    "    try:\n",
    "        plot_confusion(final_cm, class_names)\n",
    "    except Exception:\n",
    "        pass\n",
    "    try:\n",
    "        import matplotlib.pyplot as plt\n",
    "        plt.close('all')\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        plot_multiclass_roc(y_true, y_score, class_names)\n",
    "    except Exception as e:\n",
    "        print(\"ROC plot error:\", e)\n",
    "    try:\n",
    "        import matplotlib.pyplot as plt\n",
    "        plt.close('all')\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    print(\"[DONE]\")\n",
    "\n",
    "# ==== END PATCHED SNIPPET ====\n",
    "\n",
    "\n",
    "# ==========================\n",
    "# Metrics printing (unchanged)\n",
    "# ==========================\n",
    "def print_classification_metrics(y_true, y_pred, class_names):\n",
    "    print(\"\\n========== Classification Report ==========\" )\n",
    "    print(classification_report(y_true, y_pred, target_names=class_names, digits=4))\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='weighted')\n",
    "    print(\"Weighted Precision: {:.4f}\".format(precision))\n",
    "    print(\"Weighted Recall:    {:.4f}\".format(recall))\n",
    "    print(\"Weighted F1-score:  {:.4f}\".format(f1))\n",
    "    print(\"===========================================\\n\")\n",
    "\n",
    "# ==========================\n",
    "# Main guard\n",
    "# ==========================\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Starting Hybrid (logit-level) Fusion Training (fixed + preload)\")\n",
    "    train_hybrid_fusion(run_pso=True, preload_dataset=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99841b82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Hybrid (logit-level) Fusion Training (fixed + preload)\n",
      "Found 13674 paired samples. Classes: 7 -> ['anger', 'disgust', 'fear', 'joy', 'neutral', 'sadness', 'surprise']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preloading dataset into RAM: 100%|███████| 10939/10939 [01:41<00:00, 108.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PRELOAD] Finished preloading 10939 samples into RAM.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preloading dataset into RAM: 100%|█████████| 2735/2735 [00:24<00:00, 109.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PRELOAD] Finished preloading 2735 samples into RAM.\n",
      "[INFO] Loaded image checkpoint (partial load allowed).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_15940\\4012635895.py:934: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler(enabled=USE_AMP)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Loaded text checkpoint (partial load).\n",
      "[INFO] Unfroze EfficientNetV2 last 2 stages + projection.\n",
      "[INFO] Unfroze audio last transformer layer + LSTM + projection.\n",
      "[INFO] Unfroze last 4 BERT layers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100 - Train Acc: 6.81% | Val Acc: 4.13% | Time: 55.0s\n",
      " ✅ Best hybrid-fusion model saved -> ./best_hybrid_fusion_model.pth (Val Acc 4.13%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100 - Train Acc: 8.82% | Val Acc: 8.63% | Time: 286.0s\n",
      " ✅ Best hybrid-fusion model saved -> ./best_hybrid_fusion_model.pth (Val Acc 8.63%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/100 - Train Acc: 29.98% | Val Acc: 46.98% | Time: 473.2s\n",
      " ✅ Best hybrid-fusion model saved -> ./best_hybrid_fusion_model.pth (Val Acc 46.98%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/100 - Train Acc: 46.57% | Val Acc: 46.98% | Time: 389.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/100 - Train Acc: 46.95% | Val Acc: 46.98% | Time: 366.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/100 - Train Acc: 46.96% | Val Acc: 46.98% | Time: 217.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/100 - Train Acc: 46.96% | Val Acc: 46.98% | Time: 121.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/100 - Train Acc: 46.94% | Val Acc: 46.98% | Time: 111.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/100 - Train Acc: 46.93% | Val Acc: 46.98% | Time: 128.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/100 - Train Acc: 46.96% | Val Acc: 46.98% | Time: 146.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/100 - Train Acc: 46.99% | Val Acc: 46.98% | Time: 157.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/100 - Train Acc: 47.02% | Val Acc: 46.98% | Time: 145.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/100 - Train Acc: 47.27% | Val Acc: 46.95% | Time: 137.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/100 - Train Acc: 47.39% | Val Acc: 48.34% | Time: 101.4s\n",
      " ✅ Best hybrid-fusion model saved -> ./best_hybrid_fusion_model.pth (Val Acc 48.34%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/100 - Train Acc: 48.25% | Val Acc: 48.85% | Time: 220.7s\n",
      " ✅ Best hybrid-fusion model saved -> ./best_hybrid_fusion_model.pth (Val Acc 48.85%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/100 - Train Acc: 48.74% | Val Acc: 51.22% | Time: 192.0s\n",
      " ✅ Best hybrid-fusion model saved -> ./best_hybrid_fusion_model.pth (Val Acc 51.22%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/100 - Train Acc: 49.28% | Val Acc: 51.59% | Time: 267.9s\n",
      " ✅ Best hybrid-fusion model saved -> ./best_hybrid_fusion_model.pth (Val Acc 51.59%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/100 - Train Acc: 50.11% | Val Acc: 52.50% | Time: 271.3s\n",
      " ✅ Best hybrid-fusion model saved -> ./best_hybrid_fusion_model.pth (Val Acc 52.50%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/100 - Train Acc: 50.53% | Val Acc: 52.43% | Time: 178.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/100 - Train Acc: 51.07% | Val Acc: 52.14% | Time: 80.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/100 - Train Acc: 51.62% | Val Acc: 52.29% | Time: 127.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/100 - Train Acc: 51.87% | Val Acc: 52.65% | Time: 114.6s\n",
      " ✅ Best hybrid-fusion model saved -> ./best_hybrid_fusion_model.pth (Val Acc 52.65%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/100 - Train Acc: 51.95% | Val Acc: 53.24% | Time: 142.6s\n",
      " ✅ Best hybrid-fusion model saved -> ./best_hybrid_fusion_model.pth (Val Acc 53.24%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/100 - Train Acc: 52.73% | Val Acc: 53.86% | Time: 266.8s\n",
      " ✅ Best hybrid-fusion model saved -> ./best_hybrid_fusion_model.pth (Val Acc 53.86%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/100 - Train Acc: 53.08% | Val Acc: 54.22% | Time: 129.7s\n",
      " ✅ Best hybrid-fusion model saved -> ./best_hybrid_fusion_model.pth (Val Acc 54.22%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/100 - Train Acc: 53.57% | Val Acc: 54.55% | Time: 292.1s\n",
      " ✅ Best hybrid-fusion model saved -> ./best_hybrid_fusion_model.pth (Val Acc 54.55%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/100 - Train Acc: 53.35% | Val Acc: 54.33% | Time: 382.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/100 - Train Acc: 54.12% | Val Acc: 54.81% | Time: 372.9s\n",
      " ✅ Best hybrid-fusion model saved -> ./best_hybrid_fusion_model.pth (Val Acc 54.81%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/100 - Train Acc: 54.65% | Val Acc: 55.03% | Time: 211.8s\n",
      " ✅ Best hybrid-fusion model saved -> ./best_hybrid_fusion_model.pth (Val Acc 55.03%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/100 - Train Acc: 55.55% | Val Acc: 55.17% | Time: 52.5s\n",
      " ✅ Best hybrid-fusion model saved -> ./best_hybrid_fusion_model.pth (Val Acc 55.17%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31/100 - Train Acc: 55.27% | Val Acc: 55.76% | Time: 277.1s\n",
      " ✅ Best hybrid-fusion model saved -> ./best_hybrid_fusion_model.pth (Val Acc 55.76%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32/100 - Train Acc: 56.12% | Val Acc: 56.34% | Time: 226.3s\n",
      " ✅ Best hybrid-fusion model saved -> ./best_hybrid_fusion_model.pth (Val Acc 56.34%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33/100 - Train Acc: 55.95% | Val Acc: 56.60% | Time: 100.5s\n",
      " ✅ Best hybrid-fusion model saved -> ./best_hybrid_fusion_model.pth (Val Acc 56.60%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34/100 - Train Acc: 56.74% | Val Acc: 56.97% | Time: 41.8s\n",
      " ✅ Best hybrid-fusion model saved -> ./best_hybrid_fusion_model.pth (Val Acc 56.97%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35/100 - Train Acc: 57.03% | Val Acc: 57.15% | Time: 157.8s\n",
      " ✅ Best hybrid-fusion model saved -> ./best_hybrid_fusion_model.pth (Val Acc 57.15%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36/100 - Train Acc: 57.75% | Val Acc: 57.44% | Time: 51.1s\n",
      " ✅ Best hybrid-fusion model saved -> ./best_hybrid_fusion_model.pth (Val Acc 57.44%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37/100 - Train Acc: 57.14% | Val Acc: 57.33% | Time: 289.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38/100 - Train Acc: 57.17% | Val Acc: 58.10% | Time: 322.8s\n",
      " ✅ Best hybrid-fusion model saved -> ./best_hybrid_fusion_model.pth (Val Acc 58.10%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39/100 - Train Acc: 58.03% | Val Acc: 57.62% | Time: 408.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40/100 - Train Acc: 58.63% | Val Acc: 58.03% | Time: 257.9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41/100 - Train Acc: 58.37% | Val Acc: 58.14% | Time: 224.8s\n",
      " ✅ Best hybrid-fusion model saved -> ./best_hybrid_fusion_model.pth (Val Acc 58.14%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42/100 - Train Acc: 58.92% | Val Acc: 57.99% | Time: 172.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43/100 - Train Acc: 58.85% | Val Acc: 58.87% | Time: 255.9s\n",
      " ✅ Best hybrid-fusion model saved -> ./best_hybrid_fusion_model.pth (Val Acc 58.87%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44/100 - Train Acc: 58.83% | Val Acc: 58.50% | Time: 297.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45/100 - Train Acc: 59.33% | Val Acc: 58.90% | Time: 159.2s\n",
      " ✅ Best hybrid-fusion model saved -> ./best_hybrid_fusion_model.pth (Val Acc 58.90%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46/100 - Train Acc: 59.58% | Val Acc: 58.87% | Time: 342.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47/100 - Train Acc: 59.48% | Val Acc: 59.09% | Time: 173.6s\n",
      " ✅ Best hybrid-fusion model saved -> ./best_hybrid_fusion_model.pth (Val Acc 59.09%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48/100 - Train Acc: 59.59% | Val Acc: 59.71% | Time: 167.3s\n",
      " ✅ Best hybrid-fusion model saved -> ./best_hybrid_fusion_model.pth (Val Acc 59.71%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49/100 - Train Acc: 59.35% | Val Acc: 59.34% | Time: 101.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/100 - Train Acc: 59.76% | Val Acc: 59.49% | Time: 293.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 51/100 - Train Acc: 59.65% | Val Acc: 58.94% | Time: 132.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 52/100 - Train Acc: 59.87% | Val Acc: 59.34% | Time: 116.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 53/100 - Train Acc: 60.18% | Val Acc: 59.85% | Time: 126.9s\n",
      " ✅ Best hybrid-fusion model saved -> ./best_hybrid_fusion_model.pth (Val Acc 59.85%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 54/100 - Train Acc: 60.27% | Val Acc: 59.45% | Time: 92.9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 55/100 - Train Acc: 60.49% | Val Acc: 60.07% | Time: 119.3s\n",
      " ✅ Best hybrid-fusion model saved -> ./best_hybrid_fusion_model.pth (Val Acc 60.07%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56/100 - Train Acc: 60.14% | Val Acc: 59.16% | Time: 143.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57/100 - Train Acc: 60.82% | Val Acc: 59.82% | Time: 77.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58/100 - Train Acc: 60.92% | Val Acc: 59.67% | Time: 102.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59/100 - Train Acc: 60.76% | Val Acc: 60.00% | Time: 262.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 60/100:   0%|          | 0/171 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "# hybrid_fusion_from_early.py\n",
    "import os\n",
    "import copy\n",
    "import random\n",
    "import time\n",
    "from typing import List, Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from torchvision import transforms, models\n",
    "from PIL import Image\n",
    "\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from sklearn.preprocessing import LabelEncoder, label_binarize\n",
    "from sklearn.metrics import confusion_matrix, roc_curve, auc, accuracy_score\n",
    "from sklearn.metrics import classification_report, precision_recall_fscore_support\n",
    "from tqdm import tqdm\n",
    "import soundfile as sf\n",
    "import librosa\n",
    "from mpl_toolkits.mplot3d import Axes3D  # noqa: F401 - required for 3D projection\n",
    "import gc\n",
    "# ==========================\n",
    "# Config / Paths / Hyperparams (kept from your original file)\n",
    "# ==========================\n",
    "CSV_PATH = r\"K:\\Code\\Project\\Research Paper\\Emotion Detection\\meld dataset\\MELD.Raw\\self\\fusion_dataset.csv\"\n",
    "IMAGE_MODEL_PATH = r\"K:\\Code\\Project\\Research Paper\\Emotion Detection\\inceptionresnetv3_face_emotion.pth\"\n",
    "TEXT_MODEL_PATH  = r\"K:\\Code\\Project\\Research Paper\\Emotion Detection\\Code\\bert_emotion_text_final.pth\"\n",
    "AUDIO_MODEL_PATH = r\"K:\\Code\\Project\\Research Paper\\Emotion Detection\\best_transformer_speech_model.pth\"\n",
    "# Unimodal model settings (must match your trained models)\n",
    "IMG_SIZE = 224\n",
    "BERT_MODEL_NAME = \"bert-base-multilingual-cased\"\n",
    "MAX_LEN = 64\n",
    "\n",
    "# Audio MFCC params (tweak to match your audio model)\n",
    "AUDIO_MAX_PAD = 174     # same as audio model (kept)\n",
    "AUDIO_N_MFCC = 40\n",
    "AUDIO_N_FFT = 1024\n",
    "AUDIO_HOP = 512\n",
    "AUDIO_SR = 22050\n",
    "\n",
    "# Fusion training hyperparams\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 100\n",
    "LR_FUSION = 1e-3\n",
    "LR_ENCODER = 2e-5\n",
    "WEIGHT_DECAY = 1e-5\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "FREEZE_BASE_MODELS = True\n",
    "UNFREEZE_BERT_LAST_N = 4\n",
    "GRAD_CLIP_NORM = 1.0\n",
    "USE_AMP = True\n",
    "\n",
    "BEST_FUSION_PATH = \"./best_hybrid_fusion_model.pth\"\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available(): torch.cuda.manual_seed_all(SEED)\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "# Hybrid-specific configs\n",
    "AUX_UNIMODAL_LOSS = True\n",
    "AUX_WEIGHT = 0.2  # weight for auxiliary unimodal losses (sum of three)\n",
    "BRANCH_WEIGHT_LEARNABLE = True  # learnable branch weights (softmaxed)\n",
    "# ==========================\n",
    "# Utilities (audio loading / mfcc)\n",
    "# ==========================\n",
    "def load_audio(path, sr=AUDIO_SR):\n",
    "    audio, native_sr = sf.read(path)\n",
    "    if audio is None:\n",
    "        raise RuntimeError(f\"Failed reading audio: {path}\")\n",
    "    # convert stereo -> mono\n",
    "    if audio.ndim > 1:\n",
    "        audio = np.mean(audio, axis=1)\n",
    "    # resample if needed\n",
    "    if native_sr != sr:\n",
    "        audio = librosa.resample(audio, orig_sr=native_sr, target_sr=sr)\n",
    "    return audio, sr\n",
    "\n",
    "\n",
    "def extract_mfcc(file_path, sr=AUDIO_SR, n_mfcc=AUDIO_N_MFCC, max_pad_len=AUDIO_MAX_PAD,\n",
    "                 n_fft=AUDIO_N_FFT, hop_length=AUDIO_HOP):\n",
    "\n",
    "    try:\n",
    "        signal, sr = load_audio(file_path, sr)\n",
    "    except Exception:\n",
    "        # return zeros if audio load fails\n",
    "        return np.zeros((n_mfcc, max_pad_len), dtype=np.float32)\n",
    "\n",
    "    if len(signal) < 2048:\n",
    "        signal = np.pad(signal, (0, 2048 - len(signal)))\n",
    "\n",
    "    mfcc = librosa.feature.mfcc(y=signal, sr=sr, n_mfcc=n_mfcc, n_fft=n_fft, hop_length=hop_length)\n",
    "\n",
    "    # pad/crop to fixed length\n",
    "    if mfcc.shape[1] < max_pad_len:\n",
    "        mfcc = np.pad(mfcc, ((0, 0), (0, max_pad_len - mfcc.shape[1])), mode='constant')\n",
    "    else:\n",
    "        mfcc = mfcc[:, :max_pad_len]\n",
    "\n",
    "    # per-sample normalization (avoid dividing by zero)\n",
    "    mean = mfcc.mean(axis=1, keepdims=True)\n",
    "    std = mfcc.std(axis=1, keepdims=True)\n",
    "    std[std < 1e-6] = 1.0\n",
    "    mfcc = (mfcc - mean) / std\n",
    "\n",
    "    return mfcc.astype(np.float32)\n",
    "\n",
    "# ==========================\n",
    "# Encoders (unchanged)\n",
    "# ==========================\n",
    "class EfficientNetV2_Embed(nn.Module):\n",
    "    def __init__(self, embed_dim=512, pretrained=True, version=\"s\"):\n",
    "        super().__init__()\n",
    "\n",
    "        # Select EfficientNet-V2 model\n",
    "        model_fn = {\n",
    "            \"s\": models.efficientnet_v2_s,\n",
    "            \"m\": models.efficientnet_v2_m,\n",
    "            \"l\": models.efficientnet_v2_l,\n",
    "        }[version]\n",
    "\n",
    "        if pretrained:\n",
    "            base = model_fn(weights=\"IMAGENET1K_V1\")\n",
    "        else:\n",
    "            base = model_fn(weights=None)\n",
    "\n",
    "        in_features = base.classifier[1].in_features\n",
    "        base.classifier = nn.Identity()\n",
    "        self.base = base\n",
    "\n",
    "        self.proj = nn.Sequential(\n",
    "            nn.Linear(in_features, embed_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.3)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        feat = self.base(x)\n",
    "        return self.proj(feat)\n",
    "\n",
    "\n",
    "class BERTEncoder(nn.Module):\n",
    "    def __init__(self, model_name: str, embed_dim: int = 512, dropout: float = 0.2):\n",
    "        super().__init__()\n",
    "        self.bert = BertModel.from_pretrained(model_name)\n",
    "        self.proj = nn.Sequential(\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(self.bert.config.hidden_size, embed_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.2)\n",
    "        )\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        out = self.bert(input_ids=input_ids, attention_mask=attention_mask, return_dict=True)\n",
    "        pooled = out.pooler_output\n",
    "        emb = self.proj(pooled)\n",
    "        return emb\n",
    "\n",
    "class TransformerLSTM_Embed(nn.Module):\n",
    "    def __init__(self, embed_dim=512, d_model=128, nhead=4, num_layers=2):\n",
    "        super().__init__()\n",
    "        self.feature_proj = nn.Linear(AUDIO_N_MFCC, d_model)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead,\n",
    "                                                   dim_feedforward=256, dropout=0.3, batch_first=True)\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.lstm = nn.LSTM(d_model, 128, batch_first=True)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.proj = nn.Sequential(\n",
    "            nn.Linear(128, embed_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.2)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = x.squeeze(1).permute(0, 2, 1)   # (B, T, n_mfcc)\n",
    "        x = self.feature_proj(x)            # (B, T, d_model)\n",
    "        x = self.transformer(x)             # (B, T, d_model)\n",
    "        x, _ = self.lstm(x)                 # (B, T, hidden)\n",
    "        h = self.dropout(x[:, -1, :])       # (B, 128)\n",
    "        emb = self.proj(h)                  # (B, embed_dim)\n",
    "        return emb\n",
    "\n",
    "# ==========================\n",
    "# Dataset (unchanged)\n",
    "# ==========================\n",
    "class MultimodalDataset(Dataset):\n",
    "    def __init__(self, df: pd.DataFrame, tokenizer: BertTokenizer, label_encoder: LabelEncoder,\n",
    "                 img_transform, audio_pad=AUDIO_MAX_PAD, preload: bool = False, preload_verbose: bool = True):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.le = label_encoder\n",
    "        self.img_transform = img_transform\n",
    "        self.audio_pad = audio_pad\n",
    "        self.preload = preload\n",
    "        self.cache = None\n",
    "        if self.preload:\n",
    "            self._preload_to_ram(verbose=preload_verbose)\n",
    "\n",
    "    def __len__(self): return len(self.df)\n",
    "\n",
    "    def _preload_to_ram(self, verbose: bool = True):\n",
    "        \"\"\"Load images (transformed), tokenized text tensors and MFCCs into memory lists.\"\"\"\n",
    "        self.cache = [None] * len(self.df)\n",
    "        iterator = range(len(self.df))\n",
    "        if verbose:\n",
    "            iterator = tqdm(iterator, desc=\"Preloading dataset into RAM\", ncols=80)\n",
    "        for idx in iterator:\n",
    "            row = self.df.loc[idx]\n",
    "            img_path = row['image_path']\n",
    "            txt = str(row['text'])\n",
    "            audio_path = row['audio_path']\n",
    "            label = int(row['label'])\n",
    "\n",
    "            try:\n",
    "                img = Image.open(img_path).convert(\"RGB\")\n",
    "                img_t = self.img_transform(img)\n",
    "            except Exception:\n",
    "                img_t = torch.zeros(3, IMG_SIZE, IMG_SIZE, dtype=torch.float32)\n",
    "\n",
    "            try:\n",
    "                enc = self.tokenizer(txt, padding='max_length', truncation=True, max_length=MAX_LEN, return_tensors='pt')\n",
    "                input_ids = enc['input_ids'].squeeze(0)\n",
    "                attention_mask = enc['attention_mask'].squeeze(0)\n",
    "            except Exception:\n",
    "                input_ids = torch.zeros(MAX_LEN, dtype=torch.long)\n",
    "                attention_mask = torch.zeros(MAX_LEN, dtype=torch.long)\n",
    "\n",
    "            try:\n",
    "                mfcc = extract_mfcc(audio_path, sr=AUDIO_SR, n_mfcc=AUDIO_N_MFCC, max_pad_len=self.audio_pad,\n",
    "                                    n_fft=AUDIO_N_FFT, hop_length=AUDIO_HOP)\n",
    "                mfcc_t = torch.tensor(mfcc, dtype=torch.float32).unsqueeze(0)  # (1, n_mfcc, T)\n",
    "            except Exception:\n",
    "                mfcc_t = torch.zeros(1, AUDIO_N_MFCC, self.audio_pad, dtype=torch.float32)\n",
    "            self.cache[idx] = {\n",
    "                \"image\": img_t,\n",
    "                \"input_ids\": input_ids,\n",
    "                \"attention_mask\": attention_mask,\n",
    "                \"audio\": mfcc_t,\n",
    "                \"label\": torch.tensor(label, dtype=torch.long)\n",
    "            }\n",
    "        if verbose:\n",
    "            print(f\"[PRELOAD] Finished preloading {len(self.df)} samples into RAM.\")\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.preload and (self.cache is not None):\n",
    "            item = self.cache[idx]\n",
    "            return {\n",
    "                \"image\": item['image'].clone(),\n",
    "                \"input_ids\": item['input_ids'].clone(),\n",
    "                \"attention_mask\": item['attention_mask'].clone(),\n",
    "                \"audio\": item['audio'].clone(),\n",
    "                \"label\": item['label'].clone()\n",
    "            }\n",
    "\n",
    "        row = self.df.loc[idx]\n",
    "        img_path = row['image_path']\n",
    "        txt = str(row['text'])\n",
    "        audio_path = row['audio_path']\n",
    "        label = int(row['label'])\n",
    "        # Image\n",
    "        try:\n",
    "            img = Image.open(img_path).convert(\"RGB\")\n",
    "            img_t = self.img_transform(img)\n",
    "        except Exception:\n",
    "            img_t = torch.zeros(3, IMG_SIZE, IMG_SIZE)\n",
    "        # Text\n",
    "        enc = self.tokenizer(txt, padding='max_length', truncation=True, max_length=MAX_LEN, return_tensors='pt')\n",
    "        input_ids = enc['input_ids'].squeeze(0)\n",
    "        attention_mask = enc['attention_mask'].squeeze(0)\n",
    "        # Audio -> mfcc\n",
    "        try:\n",
    "            mfcc = extract_mfcc(audio_path, sr=AUDIO_SR, n_mfcc=AUDIO_N_MFCC, max_pad_len=self.audio_pad,\n",
    "                                n_fft=AUDIO_N_FFT, hop_length=AUDIO_HOP)\n",
    "            mfcc_t = torch.tensor(mfcc, dtype=torch.float32).unsqueeze(0)  # (1, n_mfcc, T)\n",
    "        except Exception:\n",
    "            mfcc_t = torch.zeros(1, AUDIO_N_MFCC, self.audio_pad, dtype=torch.float32)\n",
    "        return {\n",
    "            \"image\": img_t,\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"audio\": mfcc_t,\n",
    "            \"label\": torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# ==========================\n",
    "# Plotting utilities (unchanged)\n",
    "# ==========================\n",
    "def plot_accuracy(train_accs, val_accs):\n",
    "    plt.figure(figsize=(7,4))\n",
    "    plt.plot(range(1,len(train_accs)+1), train_accs, marker='o', label='Train Acc')\n",
    "    plt.plot(range(1,len(val_accs)+1), val_accs, marker='o', label='Val Acc')\n",
    "    plt.xlabel(\"Epoch\"); plt.ylabel(\"Accuracy (%)\"); plt.title(\"Train vs Val Acc\")\n",
    "    plt.grid(True); plt.legend(); plt.tight_layout(); plt.show()\n",
    "\n",
    "def plot_confusion(cm, labels):\n",
    "    fig, ax = plt.subplots(figsize=(7,7))\n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "    ax.figure.colorbar(im, ax=ax)\n",
    "    ax.set(xticks=np.arange(len(labels)), yticks=np.arange(len(labels)),\n",
    "           xticklabels=labels, yticklabels=labels, xlabel='Predicted', ylabel='True', title='Confusion Matrix')\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha='right')\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(j, i, format(cm[i, j], 'd'), ha='center', va='center',\n",
    "                    color='white' if cm[i, j] > thresh else 'black')\n",
    "    plt.tight_layout(); plt.show()\n",
    "\n",
    "def plot_multiclass_roc(y_true, y_score, class_names):\n",
    "    try:\n",
    "        y_true_bin = label_binarize(y_true, classes=list(range(len(class_names))))\n",
    "        plt.figure(figsize=(8,6))\n",
    "        for i in range(y_true_bin.shape[1]):\n",
    "            fpr, tpr, _ = roc_curve(y_true_bin[:, i], y_score[:, i])\n",
    "            roc_auc = auc(fpr, tpr)\n",
    "            plt.plot(fpr, tpr, label=f\"{class_names[i]} (AUC={roc_auc:.2f})\")\n",
    "        fpr, tpr, _ = roc_curve(y_true_bin.ravel(), y_score.ravel())\n",
    "        plt.plot(fpr, tpr, label=f\"micro (AUC={auc(fpr,tpr):.2f})\", linestyle='--')\n",
    "        plt.plot([0,1],[0,1],'k--', linewidth=0.6)\n",
    "        plt.xlabel(\"FPR\"); plt.ylabel(\"TPR\"); plt.title(\"Multi-class ROC\"); plt.legend(fontsize='small'); plt.grid(True); plt.tight_layout(); plt.show()\n",
    "    except Exception as e:\n",
    "        print(\"ROC plotting failed:\", e)\n",
    "\n",
    "# ==========================\n",
    "# EarlyFusion (refactored to expose embedding and logits)\n",
    "# ==========================\n",
    "class EarlyFusionModule(nn.Module):\n",
    "    \"\"\"\n",
    "    This module computes early fusion embedding/logits similarly to your EarlyFusion class.\n",
    "    We'll use it inside HybridFusion to get early_logits.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes, embed_dim=512, d_model=512,\n",
    "                 use_transformer=True, nhead=8, n_layers=1, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.use_transformer = use_transformer\n",
    "        if embed_dim != d_model:\n",
    "            self.proj_img = nn.Linear(embed_dim, d_model)\n",
    "            self.proj_txt = nn.Linear(embed_dim, d_model)\n",
    "            self.proj_aud = nn.Linear(embed_dim, d_model)\n",
    "        else:\n",
    "            self.proj_img = nn.Identity()\n",
    "            self.proj_txt = nn.Identity()\n",
    "            self.proj_aud = nn.Identity()\n",
    "\n",
    "        self.modality_type_embed = nn.Embedding(3, d_model)\n",
    "        self.pre_ln = nn.LayerNorm(d_model)\n",
    "\n",
    "        if use_transformer:\n",
    "            if d_model % nhead != 0:\n",
    "                raise ValueError(f\"d_model ({d_model}) must be divisible by nhead ({nhead})\")\n",
    "\n",
    "            encoder_layer = nn.TransformerEncoderLayer(\n",
    "                d_model=d_model,\n",
    "                nhead=nhead,\n",
    "                dim_feedforward=d_model * 2,\n",
    "                dropout=dropout,\n",
    "                batch_first=True\n",
    "            )\n",
    "            self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=n_layers)\n",
    "\n",
    "            self.classifier = nn.Sequential(\n",
    "                nn.Linear(d_model, d_model),\n",
    "                nn.LayerNorm(d_model),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout),\n",
    "                nn.Linear(d_model, num_classes),\n",
    "            )\n",
    "        else:\n",
    "            self.classifier = nn.Sequential(\n",
    "                nn.Linear(d_model * 3, d_model),\n",
    "                nn.ReLU(),\n",
    "                nn.LayerNorm(d_model),\n",
    "                nn.Dropout(dropout),\n",
    "                nn.Linear(d_model, num_classes),\n",
    "            )\n",
    "\n",
    "        self.modality_scale = nn.Parameter(torch.ones(3))\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.zeros_(m.bias)\n",
    "\n",
    "    def forward(self, e_img, e_txt, e_aud):\n",
    "        t_img = self.proj_img(e_img)\n",
    "        t_txt = self.proj_txt(e_txt)\n",
    "        t_aud = self.proj_aud(e_aud)\n",
    "\n",
    "        scales = torch.relu(self.modality_scale)\n",
    "        t_img = t_img * scales[0]\n",
    "        t_txt = t_txt * scales[1]\n",
    "        t_aud = t_aud * scales[2]\n",
    "\n",
    "        if self.use_transformer:\n",
    "            tokens = torch.stack([t_img, t_txt, t_aud], dim=1)  # (B, 3, d_model)\n",
    "            B = tokens.size(0)\n",
    "            mod_ids = torch.tensor([0, 1, 2], device=tokens.device).unsqueeze(0).repeat(B, 1)\n",
    "            tokens = tokens + self.modality_type_embed(mod_ids)\n",
    "            tokens = self.pre_ln(tokens)\n",
    "            tokens = self.transformer(tokens)\n",
    "            pooled = tokens.mean(dim=1)  # mean-pool over 3 modalities\n",
    "            logits = self.classifier(pooled)\n",
    "            return logits, pooled  # return both logits and fused embedding\n",
    "        else:\n",
    "            concat = torch.cat([t_img, t_txt, t_aud], dim=1)\n",
    "            logits = self.classifier(concat)\n",
    "            return logits, concat\n",
    "\n",
    "# ==========================\n",
    "# HybridFusion (logit-level)\n",
    "# ==========================\n",
    "class HybridFusion(nn.Module):\n",
    "    \"\"\"\n",
    "    Logit-level hybrid fusion:\n",
    "      - unimodal heads -> produce img_logits, txt_logits, aud_logits\n",
    "      - early-fusion module -> early_logits\n",
    "      - final logits = weighted sum of [early_logits, img_logits, txt_logits, aud_logits]\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes, embed_dim=512, d_model=512, use_transformer=True,\n",
    "                 nhead=8, n_layers=1, dropout=0.2, aux_unimodal_loss=True):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.aux_unimodal_loss = aux_unimodal_loss\n",
    "\n",
    "        # Early fusion branch (reuse the logic)\n",
    "        self.early = EarlyFusionModule(num_classes=num_classes, embed_dim=embed_dim,\n",
    "                                      d_model=d_model, use_transformer=use_transformer,\n",
    "                                      nhead=nhead, n_layers=n_layers, dropout=dropout)\n",
    "\n",
    "        # Unimodal heads (simple linear classifiers from embed_dim -> num_classes)\n",
    "        self.img_head = nn.Sequential(nn.Linear(embed_dim, num_classes))\n",
    "        self.txt_head = nn.Sequential(nn.Linear(embed_dim, num_classes))\n",
    "        self.aud_head = nn.Sequential(nn.Linear(embed_dim, num_classes))\n",
    "\n",
    "        # Branch weights: [early, img, txt, aud] -> softmaxed to get contribution\n",
    "        if BRANCH_WEIGHT_LEARNABLE:\n",
    "            self.branch_logits = nn.Parameter(torch.zeros(4))  # learnable raw weights\n",
    "        else:\n",
    "            self.register_buffer('branch_logits', torch.zeros(4))\n",
    "\n",
    "        # small gating MLP example (not used by default; could be used to condition weights on inputs)\n",
    "        # keep for future extension; not used in forward unless you swap logic\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.zeros_(m.bias)\n",
    "\n",
    "    def forward(self, e_img, e_txt, e_aud):\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "          final_logits: combined logits used for CE\n",
    "          dict with components: { 'early_logits', 'img_logits', 'txt_logits', 'aud_logits', 'branch_weights' }\n",
    "        \"\"\"\n",
    "        img_logits = self.img_head(e_img)    # (B, C)\n",
    "        txt_logits = self.txt_head(e_txt)\n",
    "        aud_logits = self.aud_head(e_aud)\n",
    "        early_logits, early_emb = self.early(e_img, e_txt, e_aud)\n",
    "\n",
    "        # compute branch weights (softmax over 4)\n",
    "        bw = F.softmax(self.branch_logits, dim=0)  # (4,)\n",
    "        # broadcast to batch\n",
    "        batch_bw = bw.unsqueeze(0)  # (1,4)\n",
    "\n",
    "        # weighted sum of logits\n",
    "        # order: early, img, txt, aud\n",
    "        stacked = torch.stack([early_logits, img_logits, txt_logits, aud_logits], dim=2)  # (B, C, 4)\n",
    "        # multiply by weights -> sum across branch dim\n",
    "        final_logits = (stacked * batch_bw.unsqueeze(1)).sum(dim=2)  # (B, C)\n",
    "\n",
    "        return final_logits, {\n",
    "            \"early_logits\": early_logits,\n",
    "            \"img_logits\": img_logits,\n",
    "            \"txt_logits\": txt_logits,\n",
    "            \"aud_logits\": aud_logits,\n",
    "            \"branch_weights\": bw\n",
    "        }\n",
    "\n",
    "# ==========================\n",
    "# Loading unimodal models (unchanged except returns)\n",
    "# ==========================\n",
    "def load_unimodal_models(num_classes, image_model_path, text_model_path, audio_model_path, device, embed_dim=512):\n",
    "    img_model = EfficientNetV2_Embed(embed_dim=embed_dim, pretrained=True,version=\"s\")\n",
    "    if os.path.exists(image_model_path):\n",
    "        st = torch.load(image_model_path, map_location=device)\n",
    "        try:\n",
    "            if isinstance(st, dict):\n",
    "                candidate = None\n",
    "                for k in ('model','state_dict','model_state','model_state_dict'):\n",
    "                    if k in st and isinstance(st[k], dict):\n",
    "                        candidate = st[k]; break\n",
    "                if candidate is not None:\n",
    "                    st = candidate\n",
    "                st_clean = {k:v for k,v in st.items() if not (k.startswith(\"fc.\") or k.startswith(\"classifier.\") or ('fc' in k and k.endswith('weight')))}\n",
    "                img_model.load_state_dict(st_clean, strict=False)\n",
    "            else:\n",
    "                img_model.load_state_dict(st, strict=False)\n",
    "            print(\"[INFO] Loaded image checkpoint (partial load allowed).\")\n",
    "        except Exception as e:\n",
    "            print(\"[WARN] Failed to load image checkpoint cleanly:\", e)\n",
    "    img_model.to(device)\n",
    "\n",
    "    txt_model = BERTEncoder(BERT_MODEL_NAME, embed_dim=embed_dim)\n",
    "    if os.path.exists(text_model_path):\n",
    "        st = torch.load(text_model_path, map_location=device)\n",
    "        try:\n",
    "            if isinstance(st, dict):\n",
    "                candidate = None\n",
    "                for k in ('model','state_dict','model_state','model_state_dict'):\n",
    "                    if k in st and isinstance(st[k], dict):\n",
    "                        candidate = st[k]; break\n",
    "                if candidate is not None:\n",
    "                    st = candidate\n",
    "\n",
    "                st_no_classifier = {k:v for k,v in st.items() if not k.startswith(\"classifier.\")}\n",
    "                txt_model.load_state_dict(st_no_classifier, strict=False)\n",
    "            else:\n",
    "                txt_model.load_state_dict(st, strict=False)\n",
    "            print(\"[INFO] Loaded text checkpoint (partial load).\")\n",
    "        except Exception as e:\n",
    "            print(\"[WARN] Failed to load text checkpoint:\", e)\n",
    "    txt_model.to(device)\n",
    "\n",
    "    aud_model = TransformerLSTM_Embed(embed_dim=embed_dim)\n",
    "    if os.path.exists(audio_model_path):\n",
    "        st = torch.load(audio_model_path, map_location=device)\n",
    "        try:\n",
    "            if isinstance(st, dict):\n",
    "                candidate = None\n",
    "                for k in ('model','state_dict','model_state','model_state_dict'):\n",
    "                    if k in st and isinstance(st[k], dict):\n",
    "                        candidate = st[k]; break\n",
    "                if candidate is not None:\n",
    "                    st = candidate\n",
    "                aud_model.load_state_dict(st, strict=False)\n",
    "            else:\n",
    "                aud_model.load_state_dict(st, strict=False)\n",
    "            print(\"[INFO] Loaded audio checkpoint (partial load).\")\n",
    "        except Exception as e:\n",
    "            print(\"[WARN] Failed to load audio checkpoint:\", e)\n",
    "    aud_model.to(device)\n",
    "\n",
    "    return img_model, txt_model, aud_model\n",
    "\n",
    "# ==========================\n",
    "# Freeze utilities (unchanged)\n",
    "# ==========================\n",
    "def set_requires_grad(model, requires_grad: bool):\n",
    "    for p in model.parameters():\n",
    "        p.requires_grad = requires_grad\n",
    "\n",
    "def unfreeze_efficientnet(img_model, depth=2):\n",
    "    for p in img_model.base.parameters():\n",
    "        p.requires_grad = False\n",
    "\n",
    "    stages = [\n",
    "        img_model.base.features[2],  # Stage 3\n",
    "        img_model.base.features[3],  # Stage 4\n",
    "        img_model.base.features[4],  # Stage 5\n",
    "        img_model.base.features[5],  # Stage 6\n",
    "    ]\n",
    "    for s in stages[-depth:]:\n",
    "        for p in s.parameters():\n",
    "            p.requires_grad = True\n",
    "    for p in img_model.proj.parameters():\n",
    "        p.requires_grad = True\n",
    "    print(f\"[INFO] Unfroze last {depth} EfficientNetV2 stages + projection layer.\")\n",
    "\n",
    "def freeze_bert_layers(bert_model: BertModel, unfreeze_last_n: int = 4):\n",
    "    for name, p in bert_model.named_parameters():\n",
    "        p.requires_grad = False\n",
    "\n",
    "    for name, p in bert_model.named_parameters():\n",
    "        if name.startswith(\"embeddings.\") or name.startswith(\"pooler.\") or \"LayerNorm\" in name or \"layer_norm\" in name:\n",
    "            p.requires_grad = True\n",
    "\n",
    "    try:\n",
    "        total = bert_model.config.num_hidden_layers\n",
    "        for i in range(total - unfreeze_last_n, total):\n",
    "            prefix = f\"encoder.layer.{i}.\"\n",
    "            for name, p in bert_model.named_parameters():\n",
    "                if name.startswith(prefix):\n",
    "                    p.requires_grad = True\n",
    "    except Exception:\n",
    "        for name, p in bert_model.named_parameters():\n",
    "            if \"encoder.layer\" in name and any(f\"encoder.layer.{j}.\" in name for j in range(max(0, total-unfreeze_last_n), total)):\n",
    "                p.requires_grad = True\n",
    "\n",
    "# ==========================\n",
    "# Evaluation utilities for hybrid\n",
    "# ==========================\n",
    "def evaluate_full_hybrid(model_components, hybrid_head, loader, device, class_names):\n",
    "    img_model, txt_model, aud_model = model_components\n",
    "    hybrid_head.eval()\n",
    "    img_model.eval(); txt_model.eval(); aud_model.eval()\n",
    "    all_true = []\n",
    "    all_pred = []\n",
    "    all_probs = []\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            img = batch['image'].to(device)\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            audio = batch['audio'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "            e_img = img_model(img)\n",
    "            e_txt = txt_model(input_ids, attention_mask)\n",
    "            e_aud = aud_model(audio)\n",
    "            final_logits, comps = hybrid_head(e_img, e_txt, e_aud)\n",
    "            probs = F.softmax(final_logits, dim=1).detach().cpu().numpy()\n",
    "            preds = probs.argmax(dim=1)\n",
    "            all_true.append(labels.cpu().numpy())\n",
    "            all_pred.append(preds.cpu().numpy())\n",
    "            all_probs.append(probs.cpu().numpy())\n",
    "    y_true = np.concatenate(all_true)\n",
    "    y_pred = np.concatenate(all_pred)\n",
    "    y_score = np.vstack(all_probs)\n",
    "    acc = accuracy_score(y_true, y_pred) * 100.0\n",
    "    print(f\"[EVAL] Accuracy: {acc:.2f}%\")\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=list(range(len(class_names))))\n",
    "    return acc, cm, y_true, y_score\n",
    "\n",
    "def validate_epoch_hybrid(model_components, hybrid_head, val_loader, device, num_classes):\n",
    "    img_model, txt_model, aud_model = model_components\n",
    "    hybrid_head.eval()\n",
    "    img_model.eval(); txt_model.eval(); aud_model.eval()\n",
    "\n",
    "    preds_list = []\n",
    "    labels_list = []\n",
    "    all_probs = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            img = batch['image'].to(device)\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            audio = batch['audio'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "\n",
    "            with torch.amp.autocast('cuda', enabled=USE_AMP):\n",
    "                e_img = img_model(img)\n",
    "                e_txt = txt_model(input_ids, attention_mask)\n",
    "                e_aud = aud_model(audio)\n",
    "                out, comps = hybrid_head(e_img, e_txt, e_aud)\n",
    "\n",
    "            probs = F.softmax(out, dim=1).detach().cpu()\n",
    "            preds = probs.argmax(dim=1).cpu()\n",
    "            labels_cpu = labels.cpu()\n",
    "\n",
    "            preds_list.append(preds.numpy())\n",
    "            labels_list.append(labels_cpu.numpy())\n",
    "            all_probs.append(probs.numpy())\n",
    "\n",
    "    y_pred = np.concatenate(preds_list, axis=0)\n",
    "    y_true = np.concatenate(labels_list, axis=0)\n",
    "    y_score = np.vstack(all_probs)\n",
    "    acc = (y_pred == y_true).mean() * 100.0\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=list(range(num_classes)))\n",
    "    return acc, cm, y_true, y_score\n",
    "\n",
    "# ==========================\n",
    "# PSO helper (keeps compatibility)\n",
    "# ==========================\n",
    "def visualize_pso_3d(particles, best_particle=None, metric_names=(\"Accuracy\", \"Precision\", \"Recall\")):\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "    particles = np.array(particles)\n",
    "    fig = plt.figure(figsize=(10, 7))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    if particles.size == 0:\n",
    "        print(\"[PSO VIS] no particles to display\")\n",
    "        return\n",
    "    ax.scatter(particles[:, 0], particles[:, 1], particles[:, 2], s=65, alpha=0.7)\n",
    "    if best_particle is not None:\n",
    "        best_particle = np.array(best_particle)\n",
    "        ax.scatter(best_particle[0], best_particle[1], best_particle[2],\n",
    "                   s=250, color='red', edgecolor='black', marker='o', label=\"Best Particle\")\n",
    "        ax.legend()\n",
    "    ax.set_xlabel(metric_names[0])\n",
    "    ax.set_ylabel(metric_names[1])\n",
    "    ax.set_zlabel(metric_names[2])\n",
    "    ax.set_title(\"3D PSO Per-Iteration Optimization\")\n",
    "    plt.show()\n",
    "\n",
    "def run_simple_pso_evaluate(model_components, train_loader, val_loader, class_names,\n",
    "                            swarm_size=6, iters=3, device=DEVICE, quick_batches=8):\n",
    "    print(f\"[PSO] Starting simple PSO-like search: swarm={swarm_size}, iters={iters}\")\n",
    "    img_model, txt_model, aud_model = model_components\n",
    "    all_iterations_metrics = []\n",
    "\n",
    "    particles = []\n",
    "    velocities = []\n",
    "    for _ in range(swarm_size):\n",
    "        lr = 10 ** np.random.uniform(-5, -2)\n",
    "        dropout = np.random.uniform(0.0, 0.5)\n",
    "        d_ratio = np.random.uniform(0.5, 1.0)\n",
    "        particles.append([lr, dropout, d_ratio])\n",
    "        velocities.append([0.0, 0.0, 0.0])\n",
    "    pbest = particles.copy()\n",
    "    pbest_scores = [-1.0] * swarm_size\n",
    "    gbest = None\n",
    "    gbest_score = -1.0\n",
    "    for it in range(iters):\n",
    "        print(f\"[PSO] Iteration {it+1}/{iters}\")\n",
    "        iter_metrics = []\n",
    "        best_particle_metric = None\n",
    "        best_particle_acc = -1\n",
    "        for i, p in enumerate(particles):\n",
    "            lr, dropout, d_ratio = p\n",
    "            d_model = int(512 * float(d_ratio))\n",
    "            d_model = max(4, int(d_model // 4) * 4)\n",
    "\n",
    "            nhead = 4\n",
    "            if d_model % 8 == 0:\n",
    "                nhead = 8\n",
    "            elif d_model % 4 == 0:\n",
    "                nhead = 4\n",
    "            else:\n",
    "                nhead = 1\n",
    "\n",
    "            # Build a temporary Hybrid model with particle hyperparams\n",
    "            hybrid_head = HybridFusion(num_classes=len(class_names), embed_dim=512, d_model=d_model,\n",
    "                                       use_transformer=True, nhead=nhead, n_layers=1, dropout=dropout,\n",
    "                                       aux_unimodal_loss=AUX_UNIMODAL_LOSS).to(device)\n",
    "\n",
    "            # freeze encoders during PSO evaluation (as in original)\n",
    "            hybrid_head.train()\n",
    "            img_model.eval(); txt_model.eval(); aud_model.eval()\n",
    "\n",
    "            opt = torch.optim.AdamW(hybrid_head.parameters(), lr=lr, weight_decay=WEIGHT_DECAY)\n",
    "            criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "            batch_iter = iter(train_loader)\n",
    "            for b_idx in range(quick_batches):\n",
    "                try:\n",
    "                    batch = next(batch_iter)\n",
    "                except StopIteration:\n",
    "                    batch_iter = iter(train_loader)\n",
    "                    batch = next(batch_iter)\n",
    "                img = batch['image'].to(device)\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                audio = batch['audio'].to(device)\n",
    "                labels = batch['label'].to(device)\n",
    "                opt.zero_grad()\n",
    "                with torch.amp.autocast('cuda', enabled=USE_AMP):\n",
    "                    e_img = img_model(img)\n",
    "                    e_txt = txt_model(input_ids, attention_mask)\n",
    "                    e_aud = aud_model(audio)\n",
    "                    final_logits, comps = hybrid_head(e_img, e_txt, e_aud)\n",
    "                    loss = criterion(final_logits, labels)\n",
    "                    if AUX_UNIMODAL_LOSS:\n",
    "                        loss += AUX_WEIGHT * (criterion(comps['img_logits'], labels) +\n",
    "                                              criterion(comps['txt_logits'], labels) +\n",
    "                                              criterion(comps['aud_logits'], labels))\n",
    "                loss.backward()\n",
    "                opt.step()\n",
    "            val_acc, _, y_true, y_score = evaluate_full_hybrid(\n",
    "                (img_model, txt_model, aud_model),\n",
    "                hybrid_head, val_loader, device, class_names\n",
    "            )\n",
    "            y_pred = np.argmax(y_score, axis=1)\n",
    "            precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "                y_true, y_pred, average='macro', zero_division=0\n",
    "            )\n",
    "            print(f\"[PSO] Particle {i} -> Acc: {val_acc:.2f} Prec: {precision:.4f} Rec: {recall:.4f}\")\n",
    "            metric_vec = [val_acc, precision*100, recall*100]\n",
    "            iter_metrics.append(metric_vec)\n",
    "            if val_acc > best_particle_acc:\n",
    "                best_particle_acc = val_acc\n",
    "                best_particle_metric = metric_vec\n",
    "            if val_acc > pbest_scores[i]:\n",
    "                pbest_scores[i] = val_acc\n",
    "                pbest[i] = p\n",
    "            if val_acc > gbest_score:\n",
    "                gbest_score = val_acc\n",
    "                gbest = p\n",
    "        all_iterations_metrics.append(iter_metrics)\n",
    "        print(f\"[PSO] Visualizing iteration {it+1}\")\n",
    "        visualize_pso_3d(particles=iter_metrics, best_particle=best_particle_metric,\n",
    "                         metric_names=(\"Val Acc (%)\", \"Precision (%)\", \"Recall (%)\"))\n",
    "\n",
    "        for i in range(swarm_size):\n",
    "            inertia = 0.5\n",
    "            cognitive = 0.8\n",
    "            social = 0.9\n",
    "            r1 = np.random.rand(3)\n",
    "            r2 = np.random.rand(3)\n",
    "            v = np.array(velocities[i])\n",
    "            pb = np.array(pbest[i])\n",
    "            gb = np.array(gbest) if gbest is not None else np.array(particles[i])\n",
    "            pos = np.array(particles[i])\n",
    "            v = inertia * v + cognitive * r1 * (pb - pos) + social * r2 * (gb - pos)\n",
    "            new_pos = pos + v\n",
    "            particles[i] = [\n",
    "                float(np.clip(new_pos[0], 1e-6, 1e-1)),\n",
    "                float(np.clip(new_pos[1], 0.0, 0.8)),\n",
    "                float(np.clip(new_pos[2], 0.3, 1.2))\n",
    "            ]\n",
    "            velocities[i] = v.tolist()\n",
    "    print(f\"[PSO] Done. Best acc found: {gbest_score:.2f} (particle hyperparams {gbest})\")\n",
    "    return all_iterations_metrics\n",
    "\n",
    "# ==========================\n",
    "# Training loop (converted to hybrid)\n",
    "# ==========================\n",
    "def train_hybrid_fusion(csv_path=CSV_PATH, image_model_path=IMAGE_MODEL_PATH,\n",
    "                        text_model_path=TEXT_MODEL_PATH, audio_model_path=AUDIO_MODEL_PATH,\n",
    "                        freeze_bases=FREEZE_BASE_MODELS, device=DEVICE,\n",
    "                        run_pso=False, preload_dataset: bool = True):\n",
    "    if not os.path.exists(csv_path):\n",
    "        raise FileNotFoundError(f\"CSV mapping not found: {csv_path}\")\n",
    "    df = pd.read_csv(csv_path)\n",
    "    required_cols = {'image_path', 'text', 'audio_path', 'label'}\n",
    "    if not required_cols.issubset(set(df.columns)):\n",
    "        raise RuntimeError(f\"CSV must have columns: {required_cols}\")\n",
    "    # label encoding\n",
    "    if df['label'].dtype == object:\n",
    "        le = LabelEncoder()\n",
    "        df['label'] = le.fit_transform(df['label'].astype(str))\n",
    "        class_names = list(le.classes_)\n",
    "    else:\n",
    "        le = None\n",
    "        class_names = sorted(df['label'].unique().tolist())\n",
    "        class_names = [str(int(x)) for x in class_names]\n",
    "    num_classes = int(df['label'].nunique())\n",
    "    print(f\"Found {len(df)} paired samples. Classes: {num_classes} -> {class_names}\")\n",
    "    idxs = list(range(len(df)))\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    train_idx, val_idx = train_test_split(idxs, test_size=0.2, random_state=SEED, stratify=df['label'])\n",
    "    train_df = df.loc[train_idx].reset_index(drop=True)\n",
    "    val_df = df.loc[val_idx].reset_index(drop=True)\n",
    "    tokenizer = BertTokenizer.from_pretrained(BERT_MODEL_NAME, use_fast=True)\n",
    "    img_transform = transforms.Compose([\n",
    "        transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])\n",
    "    ])\n",
    "\n",
    "    train_ds = MultimodalDataset(train_df, tokenizer, le, img_transform, preload=preload_dataset)\n",
    "    val_ds   = MultimodalDataset(val_df, tokenizer, le, img_transform, preload=preload_dataset)\n",
    "\n",
    "    pin_mem = True if device.type == 'cuda' else False\n",
    "    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, pin_memory=pin_mem)\n",
    "    val_loader   = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, pin_memory=pin_mem)\n",
    "\n",
    "    EMBED_DIM = 512\n",
    "    img_model, txt_model, aud_model = load_unimodal_models(num_classes, image_model_path, text_model_path, audio_model_path, device, embed_dim=EMBED_DIM)\n",
    "\n",
    "    # freezing logic (kept from your code)\n",
    "    if FREEZE_BASE_MODELS:\n",
    "        set_requires_grad(img_model, False)\n",
    "        set_requires_grad(txt_model, False)\n",
    "        set_requires_grad(aud_model, False)\n",
    "\n",
    "        for p in img_model.base.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "        # EfficientNetV2 stages (features[2]–[5])\n",
    "        stages = [\n",
    "            img_model.base.features[2],  # Stage 3\n",
    "            img_model.base.features[3],  # Stage 4\n",
    "            img_model.base.features[4],  # Stage 5\n",
    "            img_model.base.features[5],  # Stage 6\n",
    "        ]\n",
    "        # unfreeze last 2 stages: Stage 5 & 6\n",
    "        for s in stages[-2:]:\n",
    "            for p in s.parameters():\n",
    "                p.requires_grad = True\n",
    "        # projection head always trainable\n",
    "        set_requires_grad(img_model.proj, True)\n",
    "        print(\"[INFO] Unfroze EfficientNetV2 last 2 stages + projection.\")\n",
    "\n",
    "        for name, p in aud_model.named_parameters():\n",
    "            if \"transformer.layers.1\" in name or \"lstm\" in name:\n",
    "                p.requires_grad = True\n",
    "        # audio projection always trainable\n",
    "        set_requires_grad(aud_model.proj, True)\n",
    "        print(\"[INFO] Unfroze audio last transformer layer + LSTM + projection.\")\n",
    "\n",
    "        freeze_bert_layers(txt_model.bert, UNFREEZE_BERT_LAST_N)\n",
    "        print(f\"[INFO] Unfroze last {UNFREEZE_BERT_LAST_N} BERT layers.\")\n",
    "    else:\n",
    "        # No freezing at all\n",
    "        set_requires_grad(img_model, True)\n",
    "        set_requires_grad(txt_model, True)\n",
    "        set_requires_grad(aud_model, True)\n",
    "        print(\"[INFO] All encoders fully trainable.\")\n",
    "\n",
    "    # Build hybrid head\n",
    "    hybrid_head = HybridFusion(num_classes=num_classes, embed_dim=EMBED_DIM, d_model=512,\n",
    "                               use_transformer=True, nhead=8, n_layers=1, dropout=0.2,\n",
    "                               aux_unimodal_loss=AUX_UNIMODAL_LOSS).to(device)\n",
    "\n",
    "    # Build optimizer: include unfrozen encoder params + hybrid params\n",
    "    encoder_params = []\n",
    "    for m in (img_model, txt_model, aud_model):\n",
    "        for p in m.parameters():\n",
    "            if p.requires_grad:\n",
    "                encoder_params.append(p)\n",
    "\n",
    "    # We'll use an optimizer similar to what you used (custom lrs)\n",
    "    optimizer = torch.optim.AdamW([\n",
    "        {\"params\": img_model.parameters(),   \"lr\": 1e-4, \"weight_decay\": 0.01},\n",
    "        {\"params\": txt_model.parameters(),  \"lr\": 6e-5, \"weight_decay\": 0.01},\n",
    "        {\"params\": aud_model.parameters(), \"lr\": 1e-4, \"weight_decay\": 0.0},\n",
    "        {\"params\": hybrid_head.parameters(), \"lr\": 5e-4, \"weight_decay\": 0.0},\n",
    "    ])\n",
    "\n",
    "    total_steps = int(EPOCHS * len(train_loader))\n",
    "    warmup_steps = max(1, int(0.1 * total_steps))\n",
    "    def lr_lambda(current_step):\n",
    "        if current_step < warmup_steps:\n",
    "            return float(current_step) / float(max(1, warmup_steps))\n",
    "        return max(0.0, float(total_steps - current_step) / float(max(1, total_steps - warmup_steps)))\n",
    "    scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    best_val_acc = -1.0\n",
    "    best_state = None\n",
    "    train_accs, val_accs = [], []\n",
    "\n",
    "    scaler = torch.cuda.amp.GradScaler(enabled=USE_AMP)\n",
    "\n",
    "    for epoch in range(1, EPOCHS+1):\n",
    "        t0 = time.time()\n",
    "        hybrid_head.train()\n",
    "        img_model.train(); txt_model.train(); aud_model.train()\n",
    "        running_correct = 0\n",
    "        running_total = 0\n",
    "        pbar = tqdm(train_loader, desc=f\"Epoch {epoch}/{EPOCHS}\", leave=False)\n",
    "        for batch in pbar:\n",
    "            img = batch['image'].to(device)\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            audio = batch['audio'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            with torch.amp.autocast('cuda',enabled=USE_AMP):\n",
    "                e_img = img_model(img)\n",
    "                e_txt = txt_model(input_ids, attention_mask)\n",
    "                e_aud = aud_model(audio)\n",
    "                final_logits, comps = hybrid_head(e_img, e_txt, e_aud)\n",
    "                loss = criterion(final_logits, labels)\n",
    "                if AUX_UNIMODAL_LOSS:\n",
    "                    loss = loss + AUX_WEIGHT * (criterion(comps['img_logits'], labels) +\n",
    "                                                criterion(comps['txt_logits'], labels) +\n",
    "                                                criterion(comps['aud_logits'], labels))\n",
    "            # backward / step\n",
    "            if USE_AMP:\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.unscale_(optimizer)\n",
    "                torch.nn.utils.clip_grad_norm_( [p for g in optimizer.param_groups for p in g['params'] if p.requires_grad], GRAD_CLIP_NORM)\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "            else:\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_( [p for g in optimizer.param_groups for p in g['params'] if p.requires_grad], GRAD_CLIP_NORM)\n",
    "                optimizer.step()\n",
    "\n",
    "            preds = final_logits.detach().argmax(dim=1).cpu()\n",
    "            running_total += labels.size(0)\n",
    "            running_correct += (preds == labels.cpu()).sum().item()\n",
    "            train_acc = 100.0 * running_correct / running_total\n",
    "            pbar.set_postfix({\"TrainAcc\": f\"{train_acc:.2f}%\", \"Loss\": f\"{loss.item():.4f}\"})\n",
    "        scheduler.step()\n",
    "        train_accs.append(train_acc)\n",
    "        val_acc, cm, y_true_val, y_score_val = validate_epoch_hybrid((img_model, txt_model, aud_model), hybrid_head, val_loader, device, num_classes)   \n",
    "        val_accs.append(val_acc)\n",
    "        print(f\"Epoch {epoch}/{EPOCHS} - Train Acc: {train_acc:.2f}% | Val Acc: {val_acc:.2f}% | Time: {(time.time()-t0):.1f}s\")\n",
    "\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            best_state = {\n",
    "                \"epoch\": epoch,\n",
    "                \"hybrid_state\": copy.deepcopy(hybrid_head.state_dict()),\n",
    "                \"img_state\": copy.deepcopy(img_model.state_dict()),\n",
    "                \"txt_state\": copy.deepcopy(txt_model.state_dict()),\n",
    "                \"aud_state\": copy.deepcopy(aud_model.state_dict()),\n",
    "                \"val_acc\": val_acc,\n",
    "                \"class_names\": class_names\n",
    "            }\n",
    "            torch.save(best_state, BEST_FUSION_PATH)\n",
    "            print(f\" ✅ Best hybrid-fusion model saved -> {BEST_FUSION_PATH} (Val Acc {val_acc:.2f}%)\")\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "    plot_accuracy(train_accs, val_accs)\n",
    "    if best_state is not None:\n",
    "        print(f\"Loaded best hybrid fusion (epoch {best_state['epoch']}, val acc {best_state['val_acc']:.2f}%)\")\n",
    "        hybrid_head.load_state_dict(best_state['hybrid_state'])\n",
    "        img_model.load_state_dict(best_state['img_state'], strict=False)\n",
    "        txt_model.load_state_dict(best_state['txt_state'], strict=False)\n",
    "        aud_model.load_state_dict(best_state['aud_state'], strict=False)\n",
    "\n",
    "    final_acc, final_cm, y_true, y_score = evaluate_full_hybrid((img_model, txt_model, aud_model), hybrid_head, val_loader, device, class_names)\n",
    "    print_classification_metrics(y_true, np.argmax(y_score, axis=1), class_names)\n",
    "    plot_confusion(final_cm, class_names)\n",
    "    try:\n",
    "        plot_multiclass_roc(y_true, y_score, class_names)\n",
    "    except Exception as e:\n",
    "        print(\"ROC plot error:\", e)\n",
    "\n",
    "    if run_pso:\n",
    "        PSO_SWARM = 6\n",
    "        PSO_ITERS = 3\n",
    "        QUICK_BATCHES = 6\n",
    "        particle_metrics = run_simple_pso_evaluate((img_model, txt_model, aud_model),\n",
    "                                                   train_loader, val_loader, class_names,\n",
    "                                                   swarm_size=PSO_SWARM, iters=PSO_ITERS, device=device,\n",
    "                                                   quick_batches=QUICK_BATCHES)\n",
    "\n",
    "        flattened = [m for iter_batch in particle_metrics for m in iter_batch]\n",
    "        visualize_pso_3d(flattened, metric_names=(\"Val Acc (%)\", \"Precision (pct)\", \"Recall (pct)\"))\n",
    "\n",
    "    print(\"[DONE]\")\n",
    "\n",
    "# ==========================\n",
    "# Metrics printing (unchanged)\n",
    "# ==========================\n",
    "def print_classification_metrics(y_true, y_pred, class_names):\n",
    "    print(\"\\n========== Classification Report ==========\" )\n",
    "    print(classification_report(y_true, y_pred, target_names=class_names, digits=4))\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='weighted')\n",
    "    print(\"Weighted Precision: {:.4f}\".format(precision))\n",
    "    print(\"Weighted Recall:    {:.4f}\".format(recall))\n",
    "    print(\"Weighted F1-score:  {:.4f}\".format(f1))\n",
    "    print(\"===========================================\\n\")\n",
    "\n",
    "# ==========================\n",
    "# Main guard\n",
    "# ==========================\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Starting Hybrid (logit-level) Fusion Training (fixed + preload)\")\n",
    "    train_hybrid_fusion(run_pso=True, preload_dataset=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
