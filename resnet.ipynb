{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77af2365",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import librosa\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "# virtual env: conda activate \"K:\\Code\\Project\\Research Paper\\Emotion Detection\\Code\\resnet_env\"\n",
    "\n",
    "# -----------------------------\n",
    "# Feature extraction\n",
    "# -----------------------------\n",
    "def extract_features(file_path, max_pad_len=174):\n",
    "    signal, sr = librosa.load(file_path, sr=22050)\n",
    "    mfcc = librosa.feature.mfcc(y=signal, sr=sr, n_mfcc=40)\n",
    "    \n",
    "    # Pad/truncate MFCC to fixed length for CNN\n",
    "    if mfcc.shape[1] < max_pad_len:\n",
    "        pad_width = max_pad_len - mfcc.shape[1]\n",
    "        mfcc = np.pad(mfcc, pad_width=((0, 0), (0, pad_width)), mode='constant')\n",
    "    else:\n",
    "        mfcc = mfcc[:, :max_pad_len]\n",
    "    return mfcc\n",
    "\n",
    "# -----------------------------\n",
    "# Dataset preparation\n",
    "# -----------------------------\n",
    "data_dirs = [\n",
    "    \"K:\\\\Code\\\\Project\\\\Research Paper\\\\Emotion Detection\\\\voice_data\\\\train_data\\\\SER\",\n",
    "    \"K:\\\\Code\\\\Project\\\\Research Paper\\\\Emotion Detection\\\\archive\\\\audio_speech_actors_01-24\\\\Actor_01\",\n",
    "    \"K:\\\\Code\\\\Project\\\\Research Paper\\\\Emotion Detection\\\\archive\\\\audio_speech_actors_01-24\\\\Actor_02\",\n",
    "    \"K:\\\\Code\\\\Project\\\\Research Paper\\\\Emotion Detection\\\\archive\\\\audio_speech_actors_01-24\\\\Actor_03\",\n",
    "    \"K:\\\\Code\\\\Project\\\\Research Paper\\\\Emotion Detection\\\\archive\\\\audio_speech_actors_01-24\\\\Actor_04\",\n",
    "    \"K:\\\\Code\\\\Project\\\\Research Paper\\\\Emotion Detection\\\\archive\\\\audio_speech_actors_01-24\\\\Actor_05\",\n",
    "    \"K:\\\\Code\\\\Project\\\\Research Paper\\\\Emotion Detection\\\\archive\\\\audio_speech_actors_01-24\\\\Actor_06\",\n",
    "    \"K:\\\\Code\\\\Project\\\\Research Paper\\\\Emotion Detection\\\\archive\\\\audio_speech_actors_01-24\\\\Actor_07\",\n",
    "    \"K:\\\\Code\\\\Project\\\\Research Paper\\\\Emotion Detection\\\\archive\\\\audio_speech_actors_01-24\\\\Actor_08\",\n",
    "    \"K:\\\\Code\\\\Project\\\\Research Paper\\\\Emotion Detection\\\\archive\\\\audio_speech_actors_01-24\\\\Actor_09\",\n",
    "    \"K:\\\\Code\\\\Project\\\\Research Paper\\\\Emotion Detection\\\\archive\\\\audio_speech_actors_01-24\\\\Actor_10\",\n",
    "    \"K:\\\\Code\\\\Project\\\\Research Paper\\\\Emotion Detection\\\\archive\\\\audio_speech_actors_01-24\\\\Actor_11\",\n",
    "    \"K:\\\\Code\\\\Project\\\\Research Paper\\\\Emotion Detection\\\\archive\\\\audio_speech_actors_01-24\\\\Actor_12\",\n",
    "    \"K:\\\\Code\\\\Project\\\\Research Paper\\\\Emotion Detection\\\\archive\\\\audio_speech_actors_01-24\\\\Actor_13\",\n",
    "    \"K:\\\\Code\\\\Project\\\\Research Paper\\\\Emotion Detection\\\\archive\\\\audio_speech_actors_01-24\\\\Actor_14\",\n",
    "    \"K:\\\\Code\\\\Project\\\\Research Paper\\\\Emotion Detection\\\\archive\\\\audio_speech_actors_01-24\\\\Actor_15\",\n",
    "    \"K:\\\\Code\\\\Project\\\\Research Paper\\\\Emotion Detection\\\\archive\\\\audio_speech_actors_01-24\\\\Actor_16\",\n",
    "    \"K:\\\\Code\\\\Project\\\\Research Paper\\\\Emotion Detection\\\\archive\\\\audio_speech_actors_01-24\\\\Actor_17\",\n",
    "    \"K:\\\\Code\\\\Project\\\\Research Paper\\\\Emotion Detection\\\\archive\\\\audio_speech_actors_01-24\\\\Actor_18\",\n",
    "    \"K:\\\\Code\\\\Project\\\\Research Paper\\\\Emotion Detection\\\\archive\\\\audio_speech_actors_01-24\\\\Actor_19\",\n",
    "    \"K:\\\\Code\\\\Project\\\\Research Paper\\\\Emotion Detection\\\\archive\\\\audio_speech_actors_01-24\\\\Actor_20\",\n",
    "    \"K:\\\\Code\\\\Project\\\\Research Paper\\\\Emotion Detection\\\\archive\\\\audio_speech_actors_01-24\\\\Actor_21\",\n",
    "    \"K:\\\\Code\\\\Project\\\\Research Paper\\\\Emotion Detection\\\\archive\\\\audio_speech_actors_01-24\\\\Actor_22\",\n",
    "    \"K:\\\\Code\\\\Project\\\\Research Paper\\\\Emotion Detection\\\\archive\\\\audio_speech_actors_01-24\\\\Actor_23\",\n",
    "    \"K:\\\\Code\\\\Project\\\\Research Paper\\\\Emotion Detection\\\\archive\\\\audio_speech_actors_01-24\\\\Actor_24\",\n",
    "    \"K:\\\\Code\\\\Project\\\\Research Paper\\\\Emotion Detection\\\\archive2\\\\Angry\",\n",
    "    \"K:\\\\Code\\\\Project\\\\Research Paper\\\\Emotion Detection\\\\archive2\\\\Happy\",\n",
    "    \"K:\\\\Code\\\\Project\\\\Research Paper\\\\Emotion Detection\\\\archive2\\\\Natural\",\n",
    "    \"K:\\\\Code\\\\Project\\\\Research Paper\\\\Emotion Detection\\\\archive2\\\\Sad\",\n",
    "    \"K:\\\\Code\\\\Project\\\\Research Paper\\\\Emotion Detection\\\\archive2\\\\Surprised\"\n",
    "\n",
    "]\n",
    "X, y, emotions = [], [], []\n",
    "\n",
    "for data_dir in data_dirs:\n",
    "    if not os.path.exists(data_dir):\n",
    "        print(f\"âš ï¸ Warning: {data_dir} does not exist, skipping...\")\n",
    "        continue\n",
    "\n",
    "    for file in os.listdir(data_dir):\n",
    "        if file.endswith(\".wav\"):\n",
    "            label = file.split(\"_\")[-1].replace(\".wav\", \"\")\n",
    "            if label not in emotions:\n",
    "                emotions.append(label)\n",
    "\n",
    "            audio_path = os.path.join(data_dir, file)\n",
    "            mfcc = extract_features(audio_path)\n",
    "            X.append(mfcc)\n",
    "            y.append(emotions.index(label))\n",
    "\n",
    "X = np.array(X)\n",
    "X = X[..., np.newaxis]  # CNN needs channel dimension\n",
    "y = to_categorical(np.array(y), num_classes=len(emotions))\n",
    "\n",
    "# -----------------------------\n",
    "# Train/test split\n",
    "# -----------------------------\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.15, random_state=None\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# Build CNN model\n",
    "# -----------------------------\n",
    "def build_resnet_lstm_model(num_emotions):\n",
    "    # Input shape = (40, 174, 1) â†’ 40 MFCCs over 174 time frames\n",
    "    input_layer = layers.Input(shape=(40, 174, 1))\n",
    "\n",
    "    # Convert grayscale (1 channel) â†’ 3 channels for ResNet\n",
    "    x = layers.Conv2D(3, (1,1), padding='same')(input_layer)\n",
    "    x = layers.Lambda(lambda img: tf.image.resize(img, (224, 224)))(x)\n",
    "\n",
    "\n",
    "    # Pre-trained ResNet feature extractor (no top layers)\n",
    "    base_model = ResNet50(include_top=False, weights='imagenet', input_shape=(224, 224, 3))\n",
    "    \n",
    "\n",
    "\n",
    "    # Optionally freeze first few layers (helps transfer learning)\n",
    "    for layer in base_model.layers[:80]:\n",
    "        layer.trainable = False\n",
    "\n",
    "    # Extract features from ResNet\n",
    "    x = base_model(x)              # shape: (None, H, W, C)\n",
    "    x = layers.GlobalAveragePooling2D()(x)  # Flatten spatial dimensions\n",
    "    x = layers.RepeatVector(40)(x)          # Repeat features for LSTM time steps\n",
    "\n",
    "    # LSTM temporal modeling\n",
    "    x = layers.LSTM(128, return_sequences=False)(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "\n",
    "    # Classification layers\n",
    "    x = layers.Dense(128, activation='relu')(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "    output = layers.Dense(num_emotions, activation='softmax')(x)\n",
    "\n",
    "    model = models.Model(inputs=input_layer, outputs=output)\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "# -----------------------------\n",
    "# Train or load choice\n",
    "# -----------------------------\n",
    "choice = input(\"Do you want to load the saved model? (yes/no): \").strip().lower()\n",
    "model_path = \"K:\\\\Code\\\\Project\\\\Research Paper\\\\Emotion Detection\\\\Code\\\\resnet_tensor_emotion_model.keras\"\n",
    "\n",
    "if choice == \"yes\" and os.path.exists(model_path):\n",
    "    print(\" Loading saved model...\")\n",
    "    model = load_model(model_path)\n",
    "    # also load emotions list\n",
    "    import numpy as np\n",
    "    emotions = np.load(\"emotions11.npy\", allow_pickle=True).tolist()\n",
    "else:\n",
    "    print(\" Training model from scratch...\")\n",
    "    model = build_resnet_lstm_model(len(emotions))\n",
    "    model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=100, batch_size=32)\n",
    "    model.save(model_path)\n",
    "    np.save(\"emotions11.npy\", emotions)  # save label mapping\n",
    "\n",
    "# -----------------------------\n",
    "# Prediction function\n",
    "# -----------------------------\n",
    "def predict_emotion(file_path):\n",
    "    mfcc = extract_features(file_path)\n",
    "    mfcc = mfcc[np.newaxis, ..., np.newaxis]  # reshape for CNN\n",
    "    prediction = model.predict(mfcc)\n",
    "    return emotions[np.argmax(prediction)]\n",
    "\n",
    "# -----------------------------\n",
    "# Test prediction on new audio\n",
    "# -----------------------------\n",
    "test_file = \"K:\\\\Code\\\\Project\\\\Research Paper\\\\Emotion Detection\\\\voice_data\\\\test_data\\\\self.wav\"\n",
    "print(\"Predicted Emotion:\", predict_emotion(test_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c98a412",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Loaded 35887 images | 7 emotion classes: ['angry', 'disgust', 'fear', 'happy', 'neutral', 'sad', 'surprise']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_20928\\3635724056.py:123: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler()  # Mixed precision training\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ§  Training model from scratch...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/30 [Training]:   0%|          | 0/477 [00:00<?, ?batch/s]C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_20928\\3635724056.py:144: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "Epoch 1/30 [Training]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 477/477 [00:42<00:00, 11.29batch/s, acc=52.91%, loss=1.2504]\n",
      "Epoch 1/30 [Validation]:   0%|          | 0/85 [00:00<?, ?batch/s]C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_20928\\3635724056.py:171: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "Epoch 1/30 [Validation]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:16<00:00,  5.23batch/s, acc=61.33%, loss=1.0225]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š Epoch 1/30 Summary:\n",
      "   ðŸ‹ï¸ Train -> Loss: 1.2504, Acc: 52.91%\n",
      "   ðŸ” Val   -> Loss: 1.0225, Acc: 61.33%\n",
      "\n",
      "ðŸ’¾ Best model saved (Val Acc: 61.33%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/30 [Training]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 477/477 [00:41<00:00, 11.41batch/s, acc=70.33%, loss=0.8181]\n",
      "Epoch 2/30 [Validation]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:16<00:00,  5.25batch/s, acc=62.91%, loss=1.0284]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š Epoch 2/30 Summary:\n",
      "   ðŸ‹ï¸ Train -> Loss: 0.8181, Acc: 70.33%\n",
      "   ðŸ” Val   -> Loss: 1.0284, Acc: 62.91%\n",
      "\n",
      "ðŸ’¾ Best model saved (Val Acc: 62.91%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/30 [Training]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 477/477 [00:41<00:00, 11.43batch/s, acc=85.69%, loss=0.4157]\n",
      "Epoch 3/30 [Validation]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:16<00:00,  5.26batch/s, acc=61.27%, loss=1.2336]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š Epoch 3/30 Summary:\n",
      "   ðŸ‹ï¸ Train -> Loss: 0.4157, Acc: 85.69%\n",
      "   ðŸ” Val   -> Loss: 1.2336, Acc: 61.27%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/30 [Training]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 477/477 [00:41<00:00, 11.45batch/s, acc=95.27%, loss=0.1569]\n",
      "Epoch 4/30 [Validation]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:17<00:00,  4.93batch/s, acc=60.16%, loss=1.4439]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š Epoch 4/30 Summary:\n",
      "   ðŸ‹ï¸ Train -> Loss: 0.1569, Acc: 95.27%\n",
      "   ðŸ” Val   -> Loss: 1.4439, Acc: 60.16%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/30 [Training]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 477/477 [00:41<00:00, 11.43batch/s, acc=97.03%, loss=0.1033] \n",
      "Epoch 5/30 [Validation]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:16<00:00,  5.25batch/s, acc=61.13%, loss=1.5284]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š Epoch 5/30 Summary:\n",
      "   ðŸ‹ï¸ Train -> Loss: 0.1033, Acc: 97.03%\n",
      "   ðŸ” Val   -> Loss: 1.5284, Acc: 61.13%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/30 [Training]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 477/477 [00:41<00:00, 11.46batch/s, acc=97.55%, loss=0.0831]\n",
      "Epoch 6/30 [Validation]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:16<00:00,  5.19batch/s, acc=62.30%, loss=1.6227]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š Epoch 6/30 Summary:\n",
      "   ðŸ‹ï¸ Train -> Loss: 0.0831, Acc: 97.55%\n",
      "   ðŸ” Val   -> Loss: 1.6227, Acc: 62.30%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/30 [Training]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 477/477 [00:42<00:00, 11.23batch/s, acc=97.38%, loss=0.0840]\n",
      "Epoch 7/30 [Validation]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:16<00:00,  5.23batch/s, acc=61.70%, loss=1.6278]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š Epoch 7/30 Summary:\n",
      "   ðŸ‹ï¸ Train -> Loss: 0.0840, Acc: 97.38%\n",
      "   ðŸ” Val   -> Loss: 1.6278, Acc: 61.70%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/30 [Training]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 477/477 [00:41<00:00, 11.44batch/s, acc=97.55%, loss=0.0761]\n",
      "Epoch 8/30 [Validation]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:16<00:00,  5.05batch/s, acc=62.20%, loss=1.6569]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š Epoch 8/30 Summary:\n",
      "   ðŸ‹ï¸ Train -> Loss: 0.0761, Acc: 97.55%\n",
      "   ðŸ” Val   -> Loss: 1.6569, Acc: 62.20%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/30 [Training]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 477/477 [00:42<00:00, 11.26batch/s, acc=98.05%, loss=0.0607]\n",
      "Epoch 9/30 [Validation]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:16<00:00,  5.22batch/s, acc=60.83%, loss=1.7756]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š Epoch 9/30 Summary:\n",
      "   ðŸ‹ï¸ Train -> Loss: 0.0607, Acc: 98.05%\n",
      "   ðŸ” Val   -> Loss: 1.7756, Acc: 60.83%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/30 [Training]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 477/477 [00:42<00:00, 11.36batch/s, acc=97.85%, loss=0.0669]\n",
      "Epoch 10/30 [Validation]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:16<00:00,  5.24batch/s, acc=61.29%, loss=1.8066]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š Epoch 10/30 Summary:\n",
      "   ðŸ‹ï¸ Train -> Loss: 0.0669, Acc: 97.85%\n",
      "   ðŸ” Val   -> Loss: 1.8066, Acc: 61.29%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/30 [Training]:   0%|          | 0/477 [00:08<?, ?batch/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 140\u001b[0m\n\u001b[0;32m    137\u001b[0m running_loss, correct, total \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    139\u001b[0m train_bar \u001b[38;5;241m=\u001b[39m tqdm(train_loader, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m [Training]\u001b[39m\u001b[38;5;124m\"\u001b[39m, unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 140\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m images, labels \u001b[38;5;129;01min\u001b[39;00m train_bar:\n\u001b[0;32m    141\u001b[0m     images, labels \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mto(device, non_blocking\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m), labels\u001b[38;5;241m.\u001b[39mto(device, non_blocking\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    142\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad(set_to_none\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\Administrator\\.conda\\envs\\torch310\\lib\\site-packages\\tqdm\\std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[0;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1181\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[0;32m   1182\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[0;32m   1183\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[0;32m   1184\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Administrator\\.conda\\envs\\torch310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:499\u001b[0m, in \u001b[0;36mDataLoader.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    497\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator\n\u001b[0;32m    498\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 499\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Administrator\\.conda\\envs\\torch310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:432\u001b[0m, in \u001b[0;36mDataLoader._get_iterator\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    430\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    431\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_worker_number_rationality()\n\u001b[1;32m--> 432\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_MultiProcessingDataLoaderIter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Administrator\\.conda\\envs\\torch310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1188\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter.__init__\u001b[1;34m(self, loader)\u001b[0m\n\u001b[0;32m   1181\u001b[0m w\u001b[38;5;241m.\u001b[39mdaemon \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   1182\u001b[0m \u001b[38;5;66;03m# NB: Process.start() actually take some time as it needs to\u001b[39;00m\n\u001b[0;32m   1183\u001b[0m \u001b[38;5;66;03m#     start a process and pass the arguments over via a pipe.\u001b[39;00m\n\u001b[0;32m   1184\u001b[0m \u001b[38;5;66;03m#     Therefore, we only add a worker to self._workers list after\u001b[39;00m\n\u001b[0;32m   1185\u001b[0m \u001b[38;5;66;03m#     it started, so that we do not call .join() if program dies\u001b[39;00m\n\u001b[0;32m   1186\u001b[0m \u001b[38;5;66;03m#     before it starts, and __del__ tries to join but will get:\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[38;5;66;03m#     AssertionError: can only join a started process.\u001b[39;00m\n\u001b[1;32m-> 1188\u001b[0m \u001b[43mw\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1189\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_index_queues\u001b[38;5;241m.\u001b[39mappend(index_queue)\n\u001b[0;32m   1190\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_workers\u001b[38;5;241m.\u001b[39mappend(w)\n",
      "File \u001b[1;32mc:\\Users\\Administrator\\.conda\\envs\\torch310\\lib\\multiprocessing\\process.py:121\u001b[0m, in \u001b[0;36mBaseProcess.start\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _current_process\u001b[38;5;241m.\u001b[39m_config\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdaemon\u001b[39m\u001b[38;5;124m'\u001b[39m), \\\n\u001b[0;32m    119\u001b[0m        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdaemonic processes are not allowed to have children\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    120\u001b[0m _cleanup()\n\u001b[1;32m--> 121\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_popen \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_Popen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    122\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sentinel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_popen\u001b[38;5;241m.\u001b[39msentinel\n\u001b[0;32m    123\u001b[0m \u001b[38;5;66;03m# Avoid a refcycle if the target function holds an indirect\u001b[39;00m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;66;03m# reference to the process object (see bpo-30775)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Administrator\\.conda\\envs\\torch310\\lib\\multiprocessing\\context.py:224\u001b[0m, in \u001b[0;36mProcess._Popen\u001b[1;34m(process_obj)\u001b[0m\n\u001b[0;32m    222\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[0;32m    223\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_Popen\u001b[39m(process_obj):\n\u001b[1;32m--> 224\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mProcess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_Popen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Administrator\\.conda\\envs\\torch310\\lib\\multiprocessing\\context.py:336\u001b[0m, in \u001b[0;36mSpawnProcess._Popen\u001b[1;34m(process_obj)\u001b[0m\n\u001b[0;32m    333\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[0;32m    334\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_Popen\u001b[39m(process_obj):\n\u001b[0;32m    335\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpopen_spawn_win32\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Popen\n\u001b[1;32m--> 336\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Administrator\\.conda\\envs\\torch310\\lib\\multiprocessing\\popen_spawn_win32.py:93\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[1;34m(self, process_obj)\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     92\u001b[0m     reduction\u001b[38;5;241m.\u001b[39mdump(prep_data, to_child)\n\u001b[1;32m---> 93\u001b[0m     \u001b[43mreduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mto_child\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     94\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     95\u001b[0m     set_spawning_popen(\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\Administrator\\.conda\\envs\\torch310\\lib\\multiprocessing\\reduction.py:60\u001b[0m, in \u001b[0;36mdump\u001b[1;34m(obj, file, protocol)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdump\u001b[39m(obj, file, protocol\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m     59\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''Replacement for pickle.dump() using ForkingPickler.'''\u001b[39;00m\n\u001b[1;32m---> 60\u001b[0m     \u001b[43mForkingPickler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprotocol\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, models, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import shutil\n",
    "\n",
    "# ======================================================\n",
    "# CONFIGURATION\n",
    "# ======================================================\n",
    "data_dirs = [\n",
    "    r\"K:\\Code\\Project\\Research Paper\\Emotion Detection\\facial_data\\archive\\train\",\n",
    "    r\"K:\\Code\\Project\\Research Paper\\Emotion Detection\\facial_data\\archive\\test\"\n",
    "]\n",
    "model_path = r\"K:\\Code\\Project\\Research Paper\\Emotion Detection\\Code\\resnet_image_emotion_model.pth\"\n",
    "emotions_path = r\"K:\\Code\\Project\\Research Paper\\Emotion Detection\\Code\\emotions_image.npy\"\n",
    "batch_size = 64\n",
    "num_epochs = 30\n",
    "learning_rate = 1e-4\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "torch.backends.cudnn.benchmark = True  # Optimize for GPU speed\n",
    "\n",
    "# ======================================================\n",
    "# MERGE DATASET\n",
    "# ======================================================\n",
    "all_images_dir = r\"K:\\Code\\Project\\Research Paper\\Emotion Detection\\facial_data\\merged_dataset\"\n",
    "if not os.path.exists(all_images_dir):\n",
    "    os.makedirs(all_images_dir, exist_ok=True)\n",
    "    print(\"ðŸ”„ Combining all datasets into:\", all_images_dir)\n",
    "\n",
    "    for data_dir in data_dirs:\n",
    "        if not os.path.exists(data_dir):\n",
    "            print(f\"âš ï¸ Warning: {data_dir} not found, skipping...\")\n",
    "            continue\n",
    "        for emotion in os.listdir(data_dir):\n",
    "            src_path = os.path.join(data_dir, emotion)\n",
    "            if not os.path.isdir(src_path):\n",
    "                continue\n",
    "            dest_path = os.path.join(all_images_dir, emotion)\n",
    "            os.makedirs(dest_path, exist_ok=True)\n",
    "            for img_file in os.listdir(src_path):\n",
    "                if img_file.lower().endswith(\".jpg\"):\n",
    "                    shutil.copy2(\n",
    "                        os.path.join(src_path, img_file),\n",
    "                        os.path.join(dest_path, f\"{emotion}_{img_file}\")\n",
    "                    )\n",
    "\n",
    "# ======================================================\n",
    "# TRANSFORMS (Data Augmentation)\n",
    "# ======================================================\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "    transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                         [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                         [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# ======================================================\n",
    "# DATASETS & LOADERS\n",
    "# ======================================================\n",
    "full_dataset = datasets.ImageFolder(root=all_images_dir, transform=train_transform)\n",
    "num_classes = len(full_dataset.classes)\n",
    "print(f\"âœ… Loaded {len(full_dataset)} images | {num_classes} emotion classes: {full_dataset.classes}\")\n",
    "\n",
    "np.save(emotions_path, full_dataset.classes)\n",
    "\n",
    "train_size = int(0.85 * len(full_dataset))\n",
    "test_size = len(full_dataset) - train_size\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(full_dataset, [train_size, test_size])\n",
    "\n",
    "# Update test dataset transform to non-augmented\n",
    "test_dataset.dataset.transform = val_transform\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, pin_memory=True, num_workers=4)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, pin_memory=True, num_workers=4)\n",
    "\n",
    "# ======================================================\n",
    "# MODEL SETUP\n",
    "# ======================================================\n",
    "def build_resnet(num_classes):\n",
    "    model = models.resnet50(pretrained=True)\n",
    "\n",
    "    # Unfreeze the last residual block for fine-tuning\n",
    "    for name, param in model.named_parameters():\n",
    "        param.requires_grad = (\"layer4\" in name) or (\"fc\" in name)\n",
    "\n",
    "    in_features = model.fc.in_features\n",
    "    model.fc = nn.Sequential(\n",
    "        nn.Linear(in_features, 512),\n",
    "        nn.BatchNorm1d(512),  # âœ… Batch normalization\n",
    "        nn.ReLU(inplace=True),\n",
    "        nn.Dropout(0.4),\n",
    "        nn.Linear(512, num_classes)\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "# ======================================================\n",
    "# TRAINING OR LOADING\n",
    "# ======================================================\n",
    "choice = input(\"Do you want to load the saved model? (yes/no): \").strip().lower()\n",
    "\n",
    "model = build_resnet(num_classes).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
    "scaler = torch.cuda.amp.GradScaler()  # Mixed precision training\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)\n",
    "\n",
    "best_val_acc = 0.0\n",
    "train_losses, val_losses, train_accs, val_accs = [], [], [], []\n",
    "\n",
    "if choice == \"yes\" and os.path.exists(model_path):\n",
    "    print(\"ðŸ“‚ Loading saved model...\")\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "else:\n",
    "    print(\"ðŸ§  Training model from scratch...\")\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss, correct, total = 0.0, 0, 0\n",
    "\n",
    "        train_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Training]\", unit=\"batch\")\n",
    "        for images, labels in train_bar:\n",
    "            images, labels = images.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "            with torch.cuda.amp.autocast():\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            running_loss += loss.item()\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "            train_bar.set_postfix(loss=f\"{running_loss / (total // batch_size + 1):.4f}\",\n",
    "                                  acc=f\"{100 * correct / total:.2f}%\")\n",
    "\n",
    "        train_loss = running_loss / len(train_loader)\n",
    "        train_acc = 100 * correct / total\n",
    "\n",
    "        # -------------------------\n",
    "        # VALIDATION LOOP\n",
    "        # -------------------------\n",
    "        model.eval()\n",
    "        val_loss, val_correct, val_total = 0.0, 0, 0\n",
    "        with torch.no_grad():\n",
    "            val_bar = tqdm(test_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Validation]\", unit=\"batch\")\n",
    "            for images, labels in val_bar:\n",
    "                images, labels = images.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n",
    "                with torch.cuda.amp.autocast():\n",
    "                    outputs = model(images)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                val_correct += (preds == labels).sum().item()\n",
    "                val_total += labels.size(0)\n",
    "                val_bar.set_postfix(loss=f\"{val_loss / (val_total // batch_size + 1):.4f}\",\n",
    "                                    acc=f\"{100 * val_correct / val_total:.2f}%\")\n",
    "\n",
    "        val_loss /= len(test_loader)\n",
    "        val_acc = 100 * val_correct / val_total\n",
    "\n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        train_accs.append(train_acc)\n",
    "        val_accs.append(val_acc)\n",
    "\n",
    "        print(f\"\\nðŸ“Š Epoch {epoch+1}/{num_epochs} Summary:\")\n",
    "        print(f\"   ðŸ‹ï¸ Train -> Loss: {train_loss:.4f}, Acc: {train_acc:.2f}%\")\n",
    "        print(f\"   ðŸ” Val   -> Loss: {val_loss:.4f}, Acc: {val_acc:.2f}%\\n\")\n",
    "\n",
    "        # Save best model\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "            print(f\"ðŸ’¾ Best model saved (Val Acc: {val_acc:.2f}%)\")\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "# ======================================================\n",
    "# EVALUATION ON TEST DATA\n",
    "# ======================================================\n",
    "model.eval()\n",
    "correct, total = 0, 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in tqdm(test_loader, desc=\"Final Evaluation\", unit=\"batch\"):\n",
    "        images, labels = images.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n",
    "        outputs = model(images)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "print(f\"âœ… Final Test Accuracy: {100 * correct / total:.2f}%\")\n",
    "\n",
    "# ======================================================\n",
    "# PLOTTING\n",
    "# ======================================================\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(train_losses, label=\"Train Loss\")\n",
    "plt.plot(val_losses, label=\"Val Loss\")\n",
    "plt.title(\"Loss per Epoch\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(train_accs, label=\"Train Acc\")\n",
    "plt.plot(val_accs, label=\"Val Acc\")\n",
    "plt.title(\"Accuracy per Epoch\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy (%)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ======================================================\n",
    "# PREDICTION FUNCTION\n",
    "# ======================================================\n",
    "def predict_emotion(img_path):\n",
    "    model.eval()\n",
    "    emotions = np.load(emotions_path, allow_pickle=True).tolist()\n",
    "    img = Image.open(img_path).convert(\"RGB\")\n",
    "    img_tensor = val_transform(img).unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad(), torch.cuda.amp.autocast():\n",
    "        outputs = model(img_tensor)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "    return emotions[predicted.item()]\n",
    "\n",
    "# Example test\n",
    "test_image = r\"K:\\Code\\Project\\Research Paper\\Emotion Detection\\test5.jpg\"\n",
    "print(\"Predicted Emotion:\", predict_emotion(test_image))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
