{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f188ec8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# For the reason for training and getting better accuracy, this file has been modified from the original version.\n",
    "\n",
    "import os\n",
    "import copy\n",
    "import random\n",
    "import time\n",
    "from typing import List, Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from torchvision import transforms, models\n",
    "from PIL import Image\n",
    "\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from sklearn.preprocessing import LabelEncoder, label_binarize\n",
    "from sklearn.metrics import confusion_matrix, roc_curve, auc, accuracy_score\n",
    "from sklearn.metrics import classification_report, precision_recall_fscore_support\n",
    "from tqdm import tqdm\n",
    "import soundfile as sf\n",
    "import librosa\n",
    "from mpl_toolkits.mplot3d import Axes3D  # noqa: F401 - required for 3D projection\n",
    "\n",
    "# -----------------------------\n",
    "# USER CONFIG - adjust paths\n",
    "# -----------------------------\n",
    "CSV_PATH = r\"K:\\Code\\Project\\Research Paper\\Emotion Detection\\meld dataset\\MELD.Raw\\self\\fusion_dataset.csv\"\n",
    "IMAGE_MODEL_PATH = r\"K:\\Code\\Project\\Research Paper\\Emotion Detection\\inceptionresnetv3_face_emotion.pth\"\n",
    "TEXT_MODEL_PATH  = r\"K:\\Code\\Project\\Research Paper\\Emotion Detection\\Code\\bert_emotion_text_final.pth\"\n",
    "AUDIO_MODEL_PATH = r\"K:\\Code\\Project\\Research Paper\\Emotion Detection\\best_transformer_speech_model.pth\"\n",
    "\n",
    "# Unimodal model settings (must match your trained models)\n",
    "IMG_SIZE = 299\n",
    "BERT_MODEL_NAME = \"bert-base-multilingual-cased\"\n",
    "MAX_LEN = 64\n",
    "AUDIO_MAX_PAD = 174  # same as audio model\n",
    "\n",
    "# Fusion training hyperparams\n",
    "BATCH_SIZE = 128\n",
    "EPOCHS = 50\n",
    "LR = 1e-4\n",
    "WEIGHT_DECAY = 1e-5\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "FREEZE_BASE_MODELS = True  # True: only fusion head trained. Set False to fine-tune bases.\n",
    "\n",
    "BEST_FUSION_PATH = \"./best_early_fusion_model.pth\"\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available(): torch.cuda.manual_seed_all(SEED)\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "# -----------------------------\n",
    "# UTIL: audio feature extraction (soundfile + librosa)\n",
    "# -----------------------------\n",
    "def load_audio(path, sr=22050):\n",
    "    audio, native_sr = sf.read(path)\n",
    "    \n",
    "    if audio.ndim > 1:\n",
    "        audio = np.mean(audio, axis=1)\n",
    "    \n",
    "    if native_sr != sr:\n",
    "        audio = librosa.resample(audio, orig_sr=native_sr, target_sr=sr)\n",
    "    return audio, sr\n",
    "\n",
    "def extract_mfcc(file_path, sr=22050, n_mfcc=40, max_pad_len=AUDIO_MAX_PAD):\n",
    "    signal, sr = load_audio(file_path, sr)\n",
    "    if len(signal) < 2048:\n",
    "        signal = np.pad(signal, (0, 2048 - len(signal)))\n",
    "    mfcc = librosa.feature.mfcc(y=signal, sr=sr, n_mfcc=n_mfcc, n_fft=1024, hop_length=512)\n",
    "    if mfcc.shape[1] < max_pad_len:\n",
    "        mfcc = np.pad(mfcc, ((0, 0), (0, max_pad_len - mfcc.shape[1])), mode='constant')\n",
    "    else:\n",
    "        mfcc = mfcc[:, :max_pad_len]\n",
    "    return mfcc\n",
    "\n",
    "# -----------------------------\n",
    "# UNIMODAL MODEL WRAPPERS (EMBEDDING OUTPUTS)\n",
    "# -----------------------------\n",
    "class InceptionResNetV3_Embed(nn.Module):\n",
    "    def __init__(self, embed_dim: int = 512, pretrained=True):\n",
    "        super().__init__()\n",
    "        if pretrained:\n",
    "            base = models.inception_v3(weights=models.Inception_V3_Weights.IMAGENET1K_V1, aux_logits=False)\n",
    "        else:\n",
    "            base = models.inception_v3(aux_logits=False)\n",
    "        in_features = base.fc.in_features\n",
    "        base.fc = nn.Identity()\n",
    "        self.base = base\n",
    "        self.proj = nn.Sequential(\n",
    "            nn.Linear(in_features, embed_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.3)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        feat = self.base(x)\n",
    "        emb = self.proj(feat)\n",
    "        return emb\n",
    "\n",
    "class BERTEncoder(nn.Module):\n",
    "    def __init__(self, model_name: str, embed_dim: int = 512, dropout: float = 0.2):\n",
    "        super().__init__()\n",
    "        self.bert = BertModel.from_pretrained(model_name)\n",
    "        self.proj = nn.Sequential(\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(self.bert.config.hidden_size, embed_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.2)\n",
    "        )\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        out = self.bert(input_ids=input_ids, attention_mask=attention_mask, return_dict=True)\n",
    "        pooled = out.pooler_output\n",
    "        emb = self.proj(pooled)\n",
    "        return emb\n",
    "\n",
    "class TransformerLSTM_Embed(nn.Module):\n",
    "    def __init__(self, embed_dim=512, d_model=128, nhead=4, num_layers=2):\n",
    "        super().__init__()\n",
    "        self.feature_proj = nn.Linear(40, d_model)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead,\n",
    "                                                   dim_feedforward=256, dropout=0.3, batch_first=True)\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.lstm = nn.LSTM(d_model, 128, batch_first=True)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.proj = nn.Sequential(\n",
    "            nn.Linear(128, embed_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.2)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = x.squeeze(1).permute(0, 2, 1)\n",
    "        x = self.feature_proj(x)\n",
    "        x = self.transformer(x)\n",
    "        x, _ = self.lstm(x)\n",
    "        h = self.dropout(x[:, -1, :])\n",
    "        emb = self.proj(h)\n",
    "        return emb\n",
    "\n",
    "# -----------------------------\n",
    "# MULTIMODAL DATASET\n",
    "# -----------------------------\n",
    "class MultimodalDataset(Dataset):\n",
    "    def __init__(self, df: pd.DataFrame, tokenizer: BertTokenizer, label_encoder: LabelEncoder,\n",
    "                 img_transform, audio_pad=AUDIO_MAX_PAD):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.le = label_encoder\n",
    "        self.img_transform = img_transform\n",
    "        self.audio_pad = audio_pad\n",
    "    def __len__(self): return len(self.df)\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.loc[idx]\n",
    "        img_path = row['image_path']\n",
    "        txt = str(row['text'])\n",
    "        audio_path = row['audio_path']\n",
    "        label = int(row['label'])\n",
    "        try:\n",
    "            img = Image.open(img_path).convert(\"RGB\")\n",
    "            img_t = self.img_transform(img)\n",
    "        except Exception:\n",
    "            img_t = torch.zeros(3, IMG_SIZE, IMG_SIZE)\n",
    "        enc = self.tokenizer(txt, padding='max_length', truncation=True, max_length=MAX_LEN, return_tensors='pt')\n",
    "        input_ids = enc['input_ids'].squeeze(0)\n",
    "        attention_mask = enc['attention_mask'].squeeze(0)\n",
    "        try:\n",
    "            mfcc = extract_mfcc(audio_path, max_pad_len=self.audio_pad)\n",
    "            mfcc_t = torch.tensor(mfcc, dtype=torch.float32).unsqueeze(0)\n",
    "        except Exception:\n",
    "            mfcc_t = torch.zeros(1, 40, self.audio_pad)\n",
    "        return {\n",
    "            \"image\": img_t,\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"audio\": mfcc_t,\n",
    "            \"label\": torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# -----------------------------\n",
    "# PLOTTING HELPERS\n",
    "# -----------------------------\n",
    "def plot_accuracy(train_accs, val_accs):\n",
    "    plt.figure(figsize=(7,4))\n",
    "    plt.plot(range(1,len(train_accs)+1), train_accs, marker='o', label='Train Acc')\n",
    "    plt.plot(range(1,len(val_accs)+1), val_accs, marker='o', label='Val Acc')\n",
    "    plt.xlabel(\"Epoch\"); plt.ylabel(\"Accuracy (%)\"); plt.title(\"Train vs Val Acc\")\n",
    "    plt.grid(True); plt.legend(); plt.tight_layout(); plt.show()\n",
    "\n",
    "def plot_confusion(cm, labels):\n",
    "    fig, ax = plt.subplots(figsize=(7,7))\n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "    ax.figure.colorbar(im, ax=ax)\n",
    "    ax.set(xticks=np.arange(len(labels)), yticks=np.arange(len(labels)),\n",
    "           xticklabels=labels, yticklabels=labels, xlabel='Predicted', ylabel='True', title='Confusion Matrix')\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha='right')\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(j, i, format(cm[i, j], 'd'), ha='center', va='center',\n",
    "                    color='white' if cm[i, j] > thresh else 'black')\n",
    "    plt.tight_layout(); plt.show()\n",
    "\n",
    "def plot_multiclass_roc(y_true, y_score, class_names):\n",
    "    try:\n",
    "        y_true_bin = label_binarize(y_true, classes=list(range(len(class_names))))\n",
    "        plt.figure(figsize=(8,6))\n",
    "        for i in range(y_true_bin.shape[1]):\n",
    "            fpr, tpr, _ = roc_curve(y_true_bin[:, i], y_score[:, i])\n",
    "            roc_auc = auc(fpr, tpr)\n",
    "            plt.plot(fpr, tpr, label=f\"{class_names[i]} (AUC={roc_auc:.2f})\")\n",
    "        fpr, tpr, _ = roc_curve(y_true_bin.ravel(), y_score.ravel())\n",
    "        plt.plot(fpr, tpr, label=f\"micro (AUC={auc(fpr,tpr):.2f})\", linestyle='--')\n",
    "        plt.plot([0,1],[0,1],'k--', linewidth=0.6)\n",
    "        plt.xlabel(\"FPR\"); plt.ylabel(\"TPR\"); plt.title(\"Multi-class ROC\"); plt.legend(fontsize='small'); plt.grid(True); plt.tight_layout(); plt.show()\n",
    "    except Exception as e:\n",
    "        print(\"ROC plotting failed:\", e)\n",
    "\n",
    "# -----------------------------\n",
    "# EARLY FUSION HEAD\n",
    "# -----------------------------\n",
    "class EarlyFusion(nn.Module):\n",
    "    def __init__(self, num_classes, embed_dim=512, d_model=512, use_transformer=True, nhead=8, n_layers=1, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.use_transformer = use_transformer\n",
    "        if embed_dim != d_model:\n",
    "            self.proj_img = nn.Linear(embed_dim, d_model)\n",
    "            self.proj_txt = nn.Linear(embed_dim, d_model)\n",
    "            self.proj_aud = nn.Linear(embed_dim, d_model)\n",
    "        else:\n",
    "            self.proj_img = nn.Identity()\n",
    "            self.proj_txt = nn.Identity()\n",
    "            self.proj_aud = nn.Identity()\n",
    "        if use_transformer:\n",
    "            encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead,\n",
    "                                                       dim_feedforward=d_model*2, dropout=dropout, batch_first=True)\n",
    "            self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=n_layers)\n",
    "            self.classifier = nn.Sequential(\n",
    "                nn.Linear(d_model, d_model//1),\n",
    "                nn.BatchNorm1d(d_model//1),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout),\n",
    "                nn.Linear(d_model//1, num_classes)\n",
    "            )\n",
    "        else:\n",
    "            self.classifier = nn.Sequential(\n",
    "                nn.Linear(d_model*3, d_model),\n",
    "                nn.ReLU(),\n",
    "                nn.BatchNorm1d(d_model),\n",
    "                nn.Dropout(dropout),\n",
    "                nn.Linear(d_model, num_classes)\n",
    "            )\n",
    "        self.modality_scale = nn.Parameter(torch.ones(3))\n",
    "        self._init_weights()\n",
    "    def _init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.zeros_(m.bias)\n",
    "    def forward(self, e_img, e_txt, e_aud):\n",
    "        t_img = self.proj_img(e_img)\n",
    "        t_txt = self.proj_txt(e_txt)\n",
    "        t_aud = self.proj_aud(e_aud)\n",
    "        scales = torch.softmax(self.modality_scale, dim=0)\n",
    "        t_img = t_img * scales[0]\n",
    "        t_txt = t_txt * scales[1]\n",
    "        t_aud = t_aud * scales[2]\n",
    "        if self.use_transformer:\n",
    "            tokens = torch.stack([t_img, t_txt, t_aud], dim=1)\n",
    "            tokens = self.transformer(tokens)\n",
    "            pooled = tokens.mean(dim=1)\n",
    "            logits = self.classifier(pooled)\n",
    "        else:\n",
    "            concat = torch.cat([t_img, t_txt, t_aud], dim=1)\n",
    "            logits = self.classifier(concat)\n",
    "        return logits\n",
    "\n",
    "# -----------------------------\n",
    "# MODEL LOADING UTIL\n",
    "# -----------------------------\n",
    "def load_unimodal_models(num_classes, image_model_path, text_model_path, audio_model_path, device, embed_dim=512):\n",
    "    img_model = InceptionResNetV3_Embed(embed_dim=embed_dim, pretrained=False)\n",
    "    if os.path.exists(image_model_path):\n",
    "        st = torch.load(image_model_path, map_location=device)\n",
    "        try:\n",
    "            if isinstance(st, dict):\n",
    "                candidate = None\n",
    "                for k in ('model','state_dict','model_state','model_state_dict'):\n",
    "                    if k in st and isinstance(st[k], dict):\n",
    "                        candidate = st[k]; break\n",
    "                if candidate is not None:\n",
    "                    st = candidate\n",
    "                st_clean = {k:v for k,v in st.items() if not (k.startswith(\"fc.\") or k.startswith(\"classifier.\") or 'fc' in k and k.endswith('weight'))}\n",
    "                img_model.load_state_dict(st_clean, strict=False)\n",
    "            else:\n",
    "                img_model.load_state_dict(st, strict=False)\n",
    "            print(\"[INFO] Loaded image checkpoint (partial load allowed).\")\n",
    "        except Exception as e:\n",
    "            print(\"[WARN] Failed to load image checkpoint cleanly:\", e)\n",
    "    img_model.to(device).eval()\n",
    "    txt_model = BERTEncoder(BERT_MODEL_NAME, embed_dim=embed_dim)\n",
    "    if os.path.exists(text_model_path):\n",
    "        st = torch.load(text_model_path, map_location=device)\n",
    "        try:\n",
    "            if isinstance(st, dict):\n",
    "                candidate = None\n",
    "                for k in ('model','state_dict','model_state','model_state_dict'):\n",
    "                    if k in st and isinstance(st[k], dict):\n",
    "                        candidate = st[k]; break\n",
    "                if candidate is not None:\n",
    "                    st = candidate\n",
    "                st_no_classifier = {k:v for k,v in st.items() if not k.startswith(\"classifier.\")}\n",
    "                txt_model.load_state_dict(st_no_classifier, strict=False)\n",
    "            else:\n",
    "                txt_model.load_state_dict(st, strict=False)\n",
    "            print(\"[INFO] Loaded text checkpoint (partial load).\")\n",
    "        except Exception as e:\n",
    "            print(\"[WARN] Failed to load text checkpoint:\", e)\n",
    "    txt_model.to(device).eval()\n",
    "    aud_model = TransformerLSTM_Embed(embed_dim=embed_dim)\n",
    "    if os.path.exists(audio_model_path):\n",
    "        st = torch.load(audio_model_path, map_location=device)\n",
    "        try:\n",
    "            if isinstance(st, dict):\n",
    "                candidate = None\n",
    "                for k in ('model','state_dict','model_state','model_state_dict'):\n",
    "                    if k in st and isinstance(st[k], dict):\n",
    "                        candidate = st[k]; break\n",
    "                if candidate is not None:\n",
    "                    st = candidate\n",
    "                aud_model.load_state_dict(st, strict=False)\n",
    "            else:\n",
    "                aud_model.load_state_dict(st, strict=False)\n",
    "            print(\"[INFO] Loaded audio checkpoint (partial load).\")\n",
    "        except Exception as e:\n",
    "            print(\"[WARN] Failed to load audio checkpoint:\", e)\n",
    "    aud_model.to(device).eval()\n",
    "    return img_model, txt_model, aud_model\n",
    "\n",
    "def set_requires_grad(model, requires_grad: bool):\n",
    "    for p in model.parameters():\n",
    "        p.requires_grad = requires_grad\n",
    "\n",
    "# -----------------------------\n",
    "# EVAL / TRAIN LOOPS (early fusion)\n",
    "# -----------------------------\n",
    "def evaluate_full_early(model_components, fusion_head, loader, device, class_names):\n",
    "    img_model, txt_model, aud_model = model_components\n",
    "    fusion_head.eval()\n",
    "    img_model.eval(); txt_model.eval(); aud_model.eval()\n",
    "    all_true = []\n",
    "    all_pred = []\n",
    "    all_probs = []\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            img = batch['image'].to(device)\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            audio = batch['audio'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "            e_img = img_model(img)\n",
    "            e_txt = txt_model(input_ids, attention_mask)\n",
    "            e_aud = aud_model(audio)\n",
    "            out = fusion_head(e_img, e_txt, e_aud)\n",
    "            probs = F.softmax(out, dim=1)\n",
    "            preds = probs.argmax(dim=1)\n",
    "            all_true.append(labels.cpu().numpy())\n",
    "            all_pred.append(preds.cpu().numpy())\n",
    "            all_probs.append(probs.cpu().numpy())\n",
    "    y_true = np.concatenate(all_true)\n",
    "    y_pred = np.concatenate(all_pred)\n",
    "    y_score = np.vstack(all_probs)\n",
    "    acc = accuracy_score(y_true, y_pred) * 100.0\n",
    "    print(f\"[EVAL] Accuracy: {acc:.2f}%\")\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=list(range(len(class_names))))\n",
    "    return acc, cm, y_true, y_score\n",
    "\n",
    "def validate_epoch_early(model_components, fusion_head, val_loader, device, num_classes):\n",
    "    img_model, txt_model, aud_model = model_components\n",
    "    fusion_head.eval()\n",
    "    img_model.eval(); txt_model.eval(); aud_model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            img = batch['image'].to(device)\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            audio = batch['audio'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "            e_img = img_model(img)\n",
    "            e_txt = txt_model(input_ids, attention_mask)\n",
    "            e_aud = aud_model(audio)\n",
    "            out = fusion_head(e_img, e_txt, e_aud)\n",
    "            preds = out.argmax(dim=1)\n",
    "            all_preds.append(preds.cpu().numpy())\n",
    "            all_labels.append(labels.cpu().numpy())\n",
    "    y_pred = np.concatenate(all_preds)\n",
    "    y_true = np.concatenate(all_labels)\n",
    "    acc = (y_pred == y_true).mean() * 100.0\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=list(range(num_classes)))\n",
    "    return acc, cm, y_true, None\n",
    "\n",
    "def print_classification_metrics(y_true, y_pred, class_names):\n",
    "    print(\"\\n========== Classification Report ==========\\n\")\n",
    "    print(classification_report(y_true, y_pred, target_names=class_names, digits=4))\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='weighted')\n",
    "    print(\"Weighted Precision: {:.4f}\".format(precision))\n",
    "    print(\"Weighted Recall:    {:.4f}\".format(recall))\n",
    "    print(\"Weighted F1-score:  {:.4f}\".format(f1))\n",
    "    print(\"===========================================\\n\")\n",
    "\n",
    "# -----------------------------\n",
    "# 3D PSO VISUALIZATION\n",
    "# -----------------------------\n",
    "def visualize_pso_3d(particles, best_particle=None, metric_names=(\"Accuracy\", \"Precision\", \"Recall\")):\n",
    "    \n",
    "    import matplotlib.pyplot as plt\n",
    "    from mpl_toolkits.mplot3d import Axes3D\n",
    "    import numpy as np\n",
    "\n",
    "    particles = np.array(particles)\n",
    "\n",
    "    fig = plt.figure(figsize=(10, 7))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "    # Normal particle points\n",
    "    ax.scatter(\n",
    "        particles[:, 0], particles[:, 1], particles[:, 2],\n",
    "        s=65, alpha=0.7\n",
    "    )\n",
    "\n",
    "    # Highlight best performer\n",
    "    if best_particle is not None:\n",
    "        best_particle = np.array(best_particle)\n",
    "        ax.scatter(\n",
    "            best_particle[0], best_particle[1], best_particle[2],\n",
    "            s=250, color='red', edgecolor='black', marker='o', label=\"Best Particle\"\n",
    "        )\n",
    "        ax.legend()\n",
    "\n",
    "    ax.set_xlabel(metric_names[0])\n",
    "    ax.set_ylabel(metric_names[1])\n",
    "    ax.set_zlabel(metric_names[2])\n",
    "    ax.set_title(\"3D PSO Per-Iteration Optimization\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# -----------------------------\n",
    "# SIMPLE PSO-LIKE SEARCH (illustrative)\n",
    "# -----------------------------\n",
    "def run_simple_pso_evaluate(model_components, train_loader, val_loader, class_names,\n",
    "                            swarm_size=6, iters=3, device=DEVICE, quick_batches=8):\n",
    "\n",
    "    print(f\"[PSO] Starting simple PSO-like search: swarm={swarm_size}, iters={iters}\")\n",
    "\n",
    "    img_model, txt_model, aud_model = model_components\n",
    "    all_iterations_metrics = []  \n",
    "\n",
    "    particles = []\n",
    "    velocities = []\n",
    "    for _ in range(swarm_size):\n",
    "        lr = 10 ** np.random.uniform(-5, -2)\n",
    "        dropout = np.random.uniform(0.0, 0.5)\n",
    "        d_ratio = np.random.uniform(0.5, 1.0)\n",
    "        particles.append([lr, dropout, d_ratio])\n",
    "        velocities.append([0.0, 0.0, 0.0])\n",
    "\n",
    "    pbest = particles.copy()\n",
    "    pbest_scores = [-1.0] * swarm_size\n",
    "    gbest = None\n",
    "    gbest_score = -1.0\n",
    "\n",
    "    for it in range(iters):\n",
    "        print(f\"[PSO] Iteration {it+1}/{iters}\")\n",
    "\n",
    "        iter_metrics = []\n",
    "\n",
    "        # NEW — track best in this iteration\n",
    "        best_particle_metric = None\n",
    "        best_particle_acc = -1  \n",
    "\n",
    "        for i, p in enumerate(particles):\n",
    "            lr, dropout, d_ratio = p\n",
    "            d_model = int(512 * float(d_ratio))\n",
    "            d_model = max(4, int(d_model // 4) * 4)\n",
    "\n",
    "            fusion_head = EarlyFusion(num_classes=len(class_names), embed_dim=512, d_model=d_model,\n",
    "                                      use_transformer=True, nhead=4, n_layers=1, dropout=dropout).to(device)\n",
    "\n",
    "            opt = torch.optim.AdamW(fusion_head.parameters(), lr=lr, weight_decay=WEIGHT_DECAY)\n",
    "            criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "            fusion_head.train()\n",
    "            img_model.eval(); txt_model.eval(); aud_model.eval()\n",
    "\n",
    "            batch_iter = iter(train_loader)\n",
    "            for b_idx in range(quick_batches):\n",
    "                try:\n",
    "                    batch = next(batch_iter)\n",
    "                except StopIteration:\n",
    "                    batch_iter = iter(train_loader)\n",
    "                    batch = next(batch_iter)\n",
    "\n",
    "                img = batch['image'].to(device)\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                audio = batch['audio'].to(device)\n",
    "                labels = batch['label'].to(device)\n",
    "\n",
    "                opt.zero_grad()\n",
    "                with torch.no_grad():\n",
    "                    e_img = img_model(img)\n",
    "                    e_txt = txt_model(input_ids, attention_mask)\n",
    "                    e_aud = aud_model(audio)\n",
    "\n",
    "                out = fusion_head(e_img, e_txt, e_aud)\n",
    "                loss = criterion(out, labels)\n",
    "                loss.backward()\n",
    "                opt.step()\n",
    "\n",
    "            val_acc, _, y_true, y_score = evaluate_full_early(\n",
    "                (img_model, txt_model, aud_model), \n",
    "                fusion_head, val_loader, device, class_names\n",
    "            )\n",
    "\n",
    "            y_pred = np.argmax(y_score, axis=1)\n",
    "            precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "                y_true, y_pred, average='macro', zero_division=0\n",
    "            )\n",
    "\n",
    "            print(f\"[PSO] Particle {i} -> Acc: {val_acc:.2f} Prec: {precision:.4f} Rec: {recall:.4f}\")\n",
    "\n",
    "            metric_vec = [val_acc, precision*100, recall*100]\n",
    "            iter_metrics.append(metric_vec)\n",
    "\n",
    "            # update best for THIS iteration\n",
    "            if val_acc > best_particle_acc:\n",
    "                best_particle_acc = val_acc\n",
    "                best_particle_metric = metric_vec\n",
    "\n",
    "            # update personal/global bests\n",
    "            if val_acc > pbest_scores[i]:\n",
    "                pbest_scores[i] = val_acc\n",
    "                pbest[i] = p\n",
    "\n",
    "            if val_acc > gbest_score:\n",
    "                gbest_score = val_acc\n",
    "                gbest = p\n",
    "\n",
    "        # save iteration metrics\n",
    "        all_iterations_metrics.append(iter_metrics)\n",
    "\n",
    "        # visualize this iteration (with red best dot)\n",
    "        print(f\"[PSO] Visualizing iteration {it+1}\")\n",
    "        visualize_pso_3d(\n",
    "            particles=iter_metrics,\n",
    "            best_particle=best_particle_metric,\n",
    "            metric_names=(\"Val Acc (%)\", \"Precision (%)\", \"Recall (%)\")\n",
    "        )\n",
    "\n",
    "        # update particles\n",
    "        for i in range(swarm_size):\n",
    "            inertia = 0.5\n",
    "            cognitive = 0.8\n",
    "            social = 0.9\n",
    "            r1 = np.random.rand(3)\n",
    "            r2 = np.random.rand(3)\n",
    "            v = np.array(velocities[i])\n",
    "            pb = np.array(pbest[i])\n",
    "            gb = np.array(gbest)\n",
    "            pos = np.array(particles[i])\n",
    "            v = inertia * v + cognitive * r1 * (pb - pos) + social * r2 * (gb - pos)\n",
    "\n",
    "            new_pos = pos + v\n",
    "            particles[i] = [\n",
    "                float(np.clip(new_pos[0], 1e-6, 1e-1)),\n",
    "                float(np.clip(new_pos[1], 0.0, 0.8)),\n",
    "                float(np.clip(new_pos[2], 0.3, 1.2))\n",
    "            ]\n",
    "            velocities[i] = v.tolist()\n",
    "\n",
    "    print(f\"[PSO] Done. Best acc found: {gbest_score:.2f} (particle hyperparams {gbest})\")\n",
    "\n",
    "    return all_iterations_metrics\n",
    "\n",
    "# -----------------------------\n",
    "# MAIN TRAINING WITH OPTIONAL PSO\n",
    "# -----------------------------\n",
    "def train_early_fusion(csv_path=CSV_PATH, image_model_path=IMAGE_MODEL_PATH,\n",
    "                 text_model_path=TEXT_MODEL_PATH, audio_model_path=AUDIO_MODEL_PATH,\n",
    "                 freeze_bases=FREEZE_BASE_MODELS, device=DEVICE,\n",
    "                 run_pso=False):\n",
    "    if not os.path.exists(csv_path):\n",
    "        raise FileNotFoundError(f\"CSV mapping not found: {csv_path}\")\n",
    "    df = pd.read_csv(csv_path)\n",
    "    required_cols = {'image_path', 'text', 'audio_path', 'label'}\n",
    "    if not required_cols.issubset(set(df.columns)):\n",
    "        raise RuntimeError(f\"CSV must have columns: {required_cols}\")\n",
    "    if df['label'].dtype == object:\n",
    "        le = LabelEncoder()\n",
    "        df['label'] = le.fit_transform(df['label'].astype(str))\n",
    "        class_names = list(le.classes_)\n",
    "    else:\n",
    "        le = None\n",
    "        class_names = sorted(df['label'].unique().tolist())\n",
    "        class_names = [str(int(x)) for x in class_names]\n",
    "    num_classes = int(df['label'].nunique())\n",
    "    print(f\"Found {len(df)} paired samples. Classes: {num_classes} -> {class_names}\")\n",
    "    idxs = list(range(len(df)))\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    train_idx, val_idx = train_test_split(idxs, test_size=0.2, random_state=SEED, stratify=df['label'])\n",
    "    train_df = df.loc[train_idx].reset_index(drop=True)\n",
    "    val_df = df.loc[val_idx].reset_index(drop=True)\n",
    "    tokenizer = BertTokenizer.from_pretrained(BERT_MODEL_NAME, use_fast=True)\n",
    "    img_transform = transforms.Compose([\n",
    "        transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])\n",
    "    ])\n",
    "    train_ds = MultimodalDataset(train_df, tokenizer, LabelEncoder() if le is None else le, img_transform)\n",
    "    val_ds   = MultimodalDataset(val_df, tokenizer, LabelEncoder() if le is None else le, img_transform)\n",
    "    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=0, pin_memory=True)\n",
    "    val_loader   = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=0, pin_memory=True)\n",
    "    EMBED_DIM = 512\n",
    "    img_model, txt_model, aud_model = load_unimodal_models(num_classes, image_model_path, text_model_path, audio_model_path, device, embed_dim=EMBED_DIM)\n",
    "    set_requires_grad(img_model, not (freeze_bases))\n",
    "    set_requires_grad(txt_model, not (freeze_bases))\n",
    "    set_requires_grad(aud_model, not (freeze_bases))\n",
    "    fusion_head = EarlyFusion(num_classes, embed_dim=EMBED_DIM, d_model=512, use_transformer=True, nhead=8, n_layers=1, dropout=0.2).to(device)\n",
    "    params = list(fusion_head.parameters())\n",
    "    if not freeze_bases:\n",
    "        params += list(img_model.parameters()) + list(txt_model.parameters()) + list(aud_model.parameters())\n",
    "    optimizer = torch.optim.Adam(params, lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    best_val_acc = -1.0\n",
    "    best_state = None\n",
    "    train_accs, val_accs = [], []\n",
    "    for epoch in range(1, EPOCHS+1):\n",
    "        t0 = time.time()\n",
    "        fusion_head.train()\n",
    "        if not freeze_bases:\n",
    "            img_model.train(); txt_model.train(); aud_model.train()\n",
    "        running_correct = 0\n",
    "        running_total = 0\n",
    "        pbar = tqdm(train_loader, desc=f\"Epoch {epoch}/{EPOCHS}\", leave=False)\n",
    "        for batch in pbar:\n",
    "            img = batch['image'].to(device)\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            audio = batch['audio'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            e_img = img_model(img)\n",
    "            e_txt = txt_model(input_ids, attention_mask)\n",
    "            e_aud = aud_model(audio)\n",
    "            out = fusion_head(e_img, e_txt, e_aud)\n",
    "            loss = criterion(out, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            preds = out.argmax(dim=1)\n",
    "            running_total += labels.size(0)\n",
    "            running_correct += (preds == labels).sum().item()\n",
    "            train_acc = 100.0 * running_correct / running_total\n",
    "            pbar.set_postfix({\"TrainAcc\": f\"{train_acc:.2f}%\", \"Loss\": f\"{loss.item():.4f}\"})\n",
    "        train_accs.append(train_acc)\n",
    "        val_acc, cm, y_true_val, y_score_val = validate_epoch_early((img_model, txt_model, aud_model), fusion_head, val_loader, device, num_classes)\n",
    "        val_accs.append(val_acc)\n",
    "        print(f\"Epoch {epoch}/{EPOCHS} - Train Acc: {train_acc:.2f}% | Val Acc: {val_acc:.2f}% | Time: {(time.time()-t0):.1f}s\")\n",
    "        scheduler.step()\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            best_state = {\n",
    "                \"epoch\": epoch,\n",
    "                \"fusion_state\": copy.deepcopy(fusion_head.state_dict()),\n",
    "                \"img_state\": copy.deepcopy(img_model.state_dict()),\n",
    "                \"txt_state\": copy.deepcopy(txt_model.state_dict()),\n",
    "                \"aud_state\": copy.deepcopy(aud_model.state_dict()),\n",
    "                \"val_acc\": val_acc,\n",
    "                \"class_names\": class_names\n",
    "            }\n",
    "            torch.save(best_state, BEST_FUSION_PATH)\n",
    "            print(f\"  Best early-fusion model saved -> {BEST_FUSION_PATH} (Val Acc {val_acc:.2f}%)\")\n",
    "    # final plots & eval\n",
    "    plot_accuracy(train_accs, val_accs)\n",
    "    if best_state is not None:\n",
    "        print(f\"Loaded best early fusion (epoch {best_state['epoch']}, val acc {best_state['val_acc']:.2f}%)\")\n",
    "        fusion_head.load_state_dict(best_state['fusion_state'])\n",
    "        img_model.load_state_dict(best_state['img_state'], strict=False)\n",
    "        txt_model.load_state_dict(best_state['txt_state'], strict=False)\n",
    "        aud_model.load_state_dict(best_state['aud_state'], strict=False)\n",
    "    final_acc, final_cm, y_true, y_score = evaluate_full_early((img_model, txt_model, aud_model), fusion_head, val_loader, device, class_names)\n",
    "    print_classification_metrics(y_true, np.argmax(y_score, axis=1), class_names)\n",
    "    plot_confusion(final_cm, class_names)\n",
    "    try:\n",
    "        plot_multiclass_roc(y_true, y_score, class_names)\n",
    "    except Exception as e:\n",
    "        print(\"ROC plot error:\", e)\n",
    "    # Optional PSO run (disabled by default)\n",
    "    if run_pso:\n",
    "        # Lightweight PSO: small swarm & iterations by default\n",
    "        PSO_SWARM = 6\n",
    "        PSO_ITERS = 3\n",
    "        QUICK_BATCHES = 6   # number of quick mini-batch steps per particle\n",
    "        particle_metrics = run_simple_pso_evaluate((img_model, txt_model, aud_model),\n",
    "                                                   train_loader, val_loader, class_names,\n",
    "                                                   swarm_size=PSO_SWARM, iters=PSO_ITERS, device=device,\n",
    "                                                   quick_batches=QUICK_BATCHES)\n",
    "        visualize_pso_3d(particle_metrics, metric_names=(\"Val Acc (%)\", \"Precision (pct)\", \"Recall (pct)\"))\n",
    "    print(\"[DONE]\")\n",
    "\n",
    "# -----------------------------\n",
    "# Run\n",
    "# -----------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Starting Early Fusion Training\")\n",
    "    # Set run_pso=True to run the simple PSO search after training (can be slow)\n",
    "    train_early_fusion(run_pso=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a1c399",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Early Fusion Training (fixed + preload)\n",
      "Found 13674 paired samples. Classes: 7 -> ['anger', 'disgust', 'fear', 'joy', 'neutral', 'sadness', 'surprise']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preloading dataset into RAM: 100%|████████| 10939/10939 [01:50<00:00, 98.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PRELOAD] Finished preloading 10939 samples into RAM.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preloading dataset into RAM: 100%|██████████| 2735/2735 [00:29<00:00, 94.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PRELOAD] Finished preloading 2735 samples into RAM.\n",
      "[INFO] Loaded image checkpoint (partial load allowed).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_2236\\3915221423.py:888: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler(enabled=USE_AMP)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Loaded text checkpoint (partial load).\n",
      "[INFO] Loaded audio checkpoint (partial load).\n",
      "[INFO] Unfroze EfficientNetV2 last 2 stages + projection.\n",
      "[INFO] Unfroze audio last transformer layer + LSTM + projection.\n",
      "[INFO] Unfroze last 4 BERT layers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/100:   1%|          | 2/171 [00:06<07:18,  2.60s/it, TrainAcc=11.72%, Loss=2.4840]"
     ]
    }
   ],
   "source": [
    "\n",
    "# For the reason for training and getting better accuracy, this file has been modified from the original version.\n",
    "import os\n",
    "import copy\n",
    "import random\n",
    "import time\n",
    "from typing import List, Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from torchvision import transforms, models\n",
    "from PIL import Image\n",
    "\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from sklearn.preprocessing import LabelEncoder, label_binarize\n",
    "from sklearn.metrics import confusion_matrix, roc_curve, auc, accuracy_score\n",
    "from sklearn.metrics import classification_report, precision_recall_fscore_support\n",
    "from tqdm import tqdm\n",
    "import soundfile as sf\n",
    "import librosa\n",
    "from mpl_toolkits.mplot3d import Axes3D  # noqa: F401 - required for 3D projection\n",
    "\n",
    "\n",
    "CSV_PATH = r\"K:\\Code\\Project\\Research Paper\\Emotion Detection\\meld dataset\\MELD.Raw\\self\\fusion_dataset.csv\"\n",
    "IMAGE_MODEL_PATH = r\"K:\\Code\\Project\\Research Paper\\Emotion Detection\\efficientnetv3_face_emotion_final.pth\"\n",
    "TEXT_MODEL_PATH  = r\"K:\\Code\\Project\\Research Paper\\Emotion Detection\\Code\\bert_emotion_text_final.pth\"\n",
    "AUDIO_MODEL_PATH = r\"K:\\Code\\Project\\Research Paper\\Emotion Detection\\Code\\best_transformer_speech_model.pth\"\n",
    "\n",
    "# Unimodal model settings (must match your trained models)\n",
    "IMG_SIZE = 224\n",
    "BERT_MODEL_NAME = \"bert-base-multilingual-cased\"\n",
    "MAX_LEN = 64\n",
    "\n",
    "# Audio MFCC params (tweak to match your audio model)\n",
    "AUDIO_MAX_PAD = 174     # same as audio model (kept)\n",
    "AUDIO_N_MFCC = 40\n",
    "AUDIO_N_FFT = 1024\n",
    "AUDIO_HOP = 512\n",
    "AUDIO_SR = 22050\n",
    "\n",
    "# Fusion training hyperparams\n",
    "BATCH_SIZE = 64           \n",
    "EPOCHS = 100\n",
    "LR_FUSION = 1e-3          # fusion head lr\n",
    "LR_ENCODER = 2e-5        # small lr for encoders if unfrozen\n",
    "WEIGHT_DECAY = 1e-5\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "FREEZE_BASE_MODELS = True  \n",
    "UNFREEZE_BERT_LAST_N = 4   # when freeze_bases=True, unfreeze last N BERT encoder layers + LayerNorms/pooler\n",
    "GRAD_CLIP_NORM = 1.0\n",
    "USE_AMP = True            \n",
    "\n",
    "BEST_FUSION_PATH = \"./best_early_fusion_model_fixed_preload.pth\"\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available(): torch.cuda.manual_seed_all(SEED)\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "\n",
    "def load_audio(path, sr=AUDIO_SR):\n",
    "    audio, native_sr = sf.read(path)\n",
    "    if audio is None:\n",
    "        raise RuntimeError(f\"Failed reading audio: {path}\")\n",
    "    # convert stereo -> mono\n",
    "    if audio.ndim > 1:\n",
    "        audio = np.mean(audio, axis=1)\n",
    "    # resample if needed\n",
    "    if native_sr != sr:\n",
    "        audio = librosa.resample(audio, orig_sr=native_sr, target_sr=sr)\n",
    "    return audio, sr\n",
    "\n",
    "\n",
    "def extract_mfcc(file_path, sr=AUDIO_SR, n_mfcc=AUDIO_N_MFCC, max_pad_len=AUDIO_MAX_PAD,\n",
    "                 n_fft=AUDIO_N_FFT, hop_length=AUDIO_HOP):\n",
    "\n",
    "    try:\n",
    "        signal, sr = load_audio(file_path, sr)\n",
    "    except Exception:\n",
    "        # return zeros if audio load fails\n",
    "        return np.zeros((n_mfcc, max_pad_len), dtype=np.float32)\n",
    "\n",
    "    if len(signal) < 2048:\n",
    "        signal = np.pad(signal, (0, 2048 - len(signal)))\n",
    "\n",
    "    mfcc = librosa.feature.mfcc(y=signal, sr=sr, n_mfcc=n_mfcc, n_fft=n_fft, hop_length=hop_length)\n",
    "\n",
    "    # pad/crop to fixed length\n",
    "    if mfcc.shape[1] < max_pad_len:\n",
    "        mfcc = np.pad(mfcc, ((0, 0), (0, max_pad_len - mfcc.shape[1])), mode='constant')\n",
    "    else:\n",
    "        mfcc = mfcc[:, :max_pad_len]\n",
    "\n",
    "    # per-sample normalization (avoid dividing by zero)\n",
    "    mean = mfcc.mean(axis=1, keepdims=True)\n",
    "    std = mfcc.std(axis=1, keepdims=True)\n",
    "    std[std < 1e-6] = 1.0\n",
    "    mfcc = (mfcc - mean) / std\n",
    "\n",
    "    return mfcc.astype(np.float32)\n",
    "\n",
    "\n",
    "class EfficientNetV2_Embed(nn.Module):\n",
    "    def __init__(self, embed_dim=512, pretrained=True, version=\"s\"):\n",
    "        super().__init__()\n",
    "\n",
    "        # Select EfficientNet-V2 model\n",
    "        model_fn = {\n",
    "            \"s\": models.efficientnet_v2_s,\n",
    "            \"m\": models.efficientnet_v2_m,\n",
    "            \"l\": models.efficientnet_v2_l,\n",
    "        }[version]\n",
    "\n",
    "        if pretrained:\n",
    "            base = model_fn(weights=\"IMAGENET1K_V1\")\n",
    "        else:\n",
    "            base = model_fn(weights=None)\n",
    "\n",
    "     \n",
    "        in_features = base.classifier[1].in_features\n",
    "\n",
    "   \n",
    "        base.classifier = nn.Identity()\n",
    "        self.base = base\n",
    "\n",
    "   \n",
    "        self.proj = nn.Sequential(\n",
    "            nn.Linear(in_features, embed_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.3)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        feat = self.base(x)\n",
    "        return self.proj(feat)\n",
    "\n",
    "\n",
    "class BERTEncoder(nn.Module):\n",
    "    def __init__(self, model_name: str, embed_dim: int = 512, dropout: float = 0.2):\n",
    "        super().__init__()\n",
    "        self.bert = BertModel.from_pretrained(model_name)\n",
    "        self.proj = nn.Sequential(\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(self.bert.config.hidden_size, embed_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.2)\n",
    "        )\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        out = self.bert(input_ids=input_ids, attention_mask=attention_mask, return_dict=True)\n",
    "        pooled = out.pooler_output\n",
    "        emb = self.proj(pooled)\n",
    "        return emb\n",
    "\n",
    "class TransformerLSTM_Embed(nn.Module):\n",
    "    def __init__(self, embed_dim=512, d_model=128, nhead=4, num_layers=2):\n",
    "        super().__init__()\n",
    "        self.feature_proj = nn.Linear(AUDIO_N_MFCC, d_model)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead,\n",
    "                                                   dim_feedforward=256, dropout=0.3, batch_first=True)\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.lstm = nn.LSTM(d_model, 128, batch_first=True)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.proj = nn.Sequential(\n",
    "            nn.Linear(128, embed_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.2)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "       \n",
    "        x = x.squeeze(1).permute(0, 2, 1)   # (B, T, n_mfcc)\n",
    "        x = self.feature_proj(x)            # (B, T, d_model)\n",
    "        x = self.transformer(x)             # (B, T, d_model)\n",
    "        x, _ = self.lstm(x)                 # (B, T, hidden)\n",
    "        h = self.dropout(x[:, -1, :])       # (B, 128)\n",
    "        emb = self.proj(h)                  # (B, embed_dim)\n",
    "        return emb\n",
    "\n",
    "\n",
    "class MultimodalDataset(Dataset):\n",
    "    def __init__(self, df: pd.DataFrame, tokenizer: BertTokenizer, label_encoder: LabelEncoder,\n",
    "                 img_transform, audio_pad=AUDIO_MAX_PAD, preload: bool = False, preload_verbose: bool = True):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.le = label_encoder\n",
    "        self.img_transform = img_transform\n",
    "        self.audio_pad = audio_pad\n",
    "        self.preload = preload\n",
    "        self.cache = None\n",
    "        if self.preload:\n",
    "            self._preload_to_ram(verbose=preload_verbose)\n",
    "\n",
    "    def __len__(self): return len(self.df)\n",
    "\n",
    "    def _preload_to_ram(self, verbose: bool = True):\n",
    "        \"\"\"Load images (transformed), tokenized text tensors and MFCCs into memory lists.\"\"\"\n",
    "        self.cache = [None] * len(self.df)\n",
    "        iterator = range(len(self.df))\n",
    "        if verbose:\n",
    "            iterator = tqdm(iterator, desc=\"Preloading dataset into RAM\", ncols=80)\n",
    "        for idx in iterator:\n",
    "            row = self.df.loc[idx]\n",
    "            img_path = row['image_path']\n",
    "            txt = str(row['text'])\n",
    "            audio_path = row['audio_path']\n",
    "            label = int(row['label'])\n",
    "          \n",
    "            try:\n",
    "                img = Image.open(img_path).convert(\"RGB\")\n",
    "                img_t = self.img_transform(img)\n",
    "            except Exception:\n",
    "              \n",
    "                img_t = torch.zeros(3, IMG_SIZE, IMG_SIZE, dtype=torch.float32)\n",
    "           \n",
    "            try:\n",
    "                enc = self.tokenizer(txt, padding='max_length', truncation=True, max_length=MAX_LEN, return_tensors='pt')\n",
    "                input_ids = enc['input_ids'].squeeze(0)\n",
    "                attention_mask = enc['attention_mask'].squeeze(0)\n",
    "            except Exception:\n",
    "                input_ids = torch.zeros(MAX_LEN, dtype=torch.long)\n",
    "                attention_mask = torch.zeros(MAX_LEN, dtype=torch.long)\n",
    "           \n",
    "            try:\n",
    "                mfcc = extract_mfcc(audio_path, sr=AUDIO_SR, n_mfcc=AUDIO_N_MFCC, max_pad_len=self.audio_pad,\n",
    "                                    n_fft=AUDIO_N_FFT, hop_length=AUDIO_HOP)\n",
    "                mfcc_t = torch.tensor(mfcc, dtype=torch.float32).unsqueeze(0)  # (1, n_mfcc, T)\n",
    "            except Exception:\n",
    "                mfcc_t = torch.zeros(1, AUDIO_N_MFCC, self.audio_pad, dtype=torch.float32)\n",
    "            self.cache[idx] = {\n",
    "                \"image\": img_t,\n",
    "                \"input_ids\": input_ids,\n",
    "                \"attention_mask\": attention_mask,\n",
    "                \"audio\": mfcc_t,\n",
    "                \"label\": torch.tensor(label, dtype=torch.long)\n",
    "            }\n",
    "        if verbose:\n",
    "            print(f\"[PRELOAD] Finished preloading {len(self.df)} samples into RAM.\")\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.preload and (self.cache is not None):\n",
    "            item = self.cache[idx]\n",
    "            \n",
    "            return {\n",
    "                \"image\": item['image'].clone(),\n",
    "                \"input_ids\": item['input_ids'].clone(),\n",
    "                \"attention_mask\": item['attention_mask'].clone(),\n",
    "                \"audio\": item['audio'].clone(),\n",
    "                \"label\": item['label'].clone()\n",
    "            }\n",
    "\n",
    "        row = self.df.loc[idx]\n",
    "        img_path = row['image_path']\n",
    "        txt = str(row['text'])\n",
    "        audio_path = row['audio_path']\n",
    "        label = int(row['label'])\n",
    "        # Image\n",
    "        try:\n",
    "            img = Image.open(img_path).convert(\"RGB\")\n",
    "            img_t = self.img_transform(img)\n",
    "        except Exception:\n",
    "            img_t = torch.zeros(3, IMG_SIZE, IMG_SIZE)\n",
    "        # Text\n",
    "        enc = self.tokenizer(txt, padding='max_length', truncation=True, max_length=MAX_LEN, return_tensors='pt')\n",
    "        input_ids = enc['input_ids'].squeeze(0)\n",
    "        attention_mask = enc['attention_mask'].squeeze(0)\n",
    "        # Audio -> mfcc\n",
    "        try:\n",
    "            mfcc = extract_mfcc(audio_path, sr=AUDIO_SR, n_mfcc=AUDIO_N_MFCC, max_pad_len=self.audio_pad,\n",
    "                                n_fft=AUDIO_N_FFT, hop_length=AUDIO_HOP)\n",
    "            mfcc_t = torch.tensor(mfcc, dtype=torch.float32).unsqueeze(0)  # (1, n_mfcc, T)\n",
    "        except Exception:\n",
    "            mfcc_t = torch.zeros(1, AUDIO_N_MFCC, self.audio_pad, dtype=torch.float32)\n",
    "        return {\n",
    "            \"image\": img_t,\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"audio\": mfcc_t,\n",
    "            \"label\": torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "\n",
    "def plot_accuracy(train_accs, val_accs):\n",
    "    plt.figure(figsize=(7,4))\n",
    "    plt.plot(range(1,len(train_accs)+1), train_accs, marker='o', label='Train Acc')\n",
    "    plt.plot(range(1,len(val_accs)+1), val_accs, marker='o', label='Val Acc')\n",
    "    plt.xlabel(\"Epoch\"); plt.ylabel(\"Accuracy (%)\"); plt.title(\"Train vs Val Acc\")\n",
    "    plt.grid(True); plt.legend(); plt.tight_layout(); plt.show()\n",
    "\n",
    "\n",
    "def plot_confusion(cm, labels):\n",
    "    fig, ax = plt.subplots(figsize=(7,7))\n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "    ax.figure.colorbar(im, ax=ax)\n",
    "    ax.set(xticks=np.arange(len(labels)), yticks=np.arange(len(labels)),\n",
    "           xticklabels=labels, yticklabels=labels, xlabel='Predicted', ylabel='True', title='Confusion Matrix')\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha='right')\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(j, i, format(cm[i, j], 'd'), ha='center', va='center',\n",
    "                    color='white' if cm[i, j] > thresh else 'black')\n",
    "    plt.tight_layout(); plt.show()\n",
    "\n",
    "\n",
    "def plot_multiclass_roc(y_true, y_score, class_names):\n",
    "    try:\n",
    "        y_true_bin = label_binarize(y_true, classes=list(range(len(class_names))))\n",
    "        plt.figure(figsize=(8,6))\n",
    "        for i in range(y_true_bin.shape[1]):\n",
    "            fpr, tpr, _ = roc_curve(y_true_bin[:, i], y_score[:, i])\n",
    "            roc_auc = auc(fpr, tpr)\n",
    "            plt.plot(fpr, tpr, label=f\"{class_names[i]} (AUC={roc_auc:.2f})\")\n",
    "        fpr, tpr, _ = roc_curve(y_true_bin.ravel(), y_score.ravel())\n",
    "        plt.plot(fpr, tpr, label=f\"micro (AUC={auc(fpr,tpr):.2f})\", linestyle='--')\n",
    "        plt.plot([0,1],[0,1],'k--', linewidth=0.6)\n",
    "        plt.xlabel(\"FPR\"); plt.ylabel(\"TPR\"); plt.title(\"Multi-class ROC\"); plt.legend(fontsize='small'); plt.grid(True); plt.tight_layout(); plt.show()\n",
    "    except Exception as e:\n",
    "        print(\"ROC plotting failed:\", e)\n",
    "\n",
    "\n",
    "class EarlyFusion(nn.Module):\n",
    "    def __init__(self, num_classes, embed_dim=512, d_model=512,\n",
    "                 use_transformer=True, nhead=8, n_layers=1, dropout=0.2):\n",
    "        super().__init__()\n",
    "\n",
    "        self.use_transformer = use_transformer\n",
    "\n",
    "\n",
    "        if embed_dim != d_model:\n",
    "            self.proj_img = nn.Linear(embed_dim, d_model)\n",
    "            self.proj_txt = nn.Linear(embed_dim, d_model)\n",
    "            self.proj_aud = nn.Linear(embed_dim, d_model)\n",
    "        else:\n",
    "            self.proj_img = nn.Identity()\n",
    "            self.proj_txt = nn.Identity()\n",
    "            self.proj_aud = nn.Identity()\n",
    "\n",
    "\n",
    "        self.modality_type_embed = nn.Embedding(3, d_model)\n",
    "\n",
    "\n",
    "        self.pre_ln = nn.LayerNorm(d_model)\n",
    "\n",
    "        # Transformer fusion\n",
    "        if use_transformer:\n",
    "            if d_model % nhead != 0:\n",
    "                raise ValueError(f\"d_model ({d_model}) must be divisible by nhead ({nhead})\")\n",
    "\n",
    "            encoder_layer = nn.TransformerEncoderLayer(\n",
    "                d_model=d_model,\n",
    "                nhead=nhead,\n",
    "                dim_feedforward=d_model * 2,\n",
    "                dropout=dropout,\n",
    "                batch_first=True\n",
    "            )\n",
    "            self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=n_layers)\n",
    "\n",
    "            self.classifier = nn.Sequential(\n",
    "                nn.Linear(d_model, d_model),\n",
    "                nn.LayerNorm(d_model),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout),\n",
    "                nn.Linear(d_model, num_classes),\n",
    "            )\n",
    "\n",
    "        else:\n",
    "            self.classifier = nn.Sequential(\n",
    "                nn.Linear(d_model * 3, d_model),\n",
    "                nn.ReLU(),\n",
    "                nn.LayerNorm(d_model),\n",
    "                nn.Dropout(dropout),\n",
    "                nn.Linear(d_model, num_classes),\n",
    "            )\n",
    "\n",
    "\n",
    "        self.modality_scale = nn.Parameter(torch.ones(3))\n",
    "\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.zeros_(m.bias)\n",
    "\n",
    "    def forward(self, e_img, e_txt, e_aud):\n",
    "\n",
    "        # Project embeddings\n",
    "        t_img = self.proj_img(e_img)\n",
    "        t_txt = self.proj_txt(e_txt)\n",
    "        t_aud = self.proj_aud(e_aud)\n",
    "\n",
    "        scales = torch.relu(self.modality_scale)\n",
    "\n",
    "        t_img = t_img * scales[0]\n",
    "        t_txt = t_txt * scales[1]\n",
    "        t_aud = t_aud * scales[2]\n",
    "\n",
    "        if self.use_transformer:\n",
    "            tokens = torch.stack([t_img, t_txt, t_aud], dim=1)  # (B, 3, d_model)\n",
    "\n",
    "\n",
    "            B = tokens.size(0)\n",
    "            mod_ids = torch.tensor([0, 1, 2], device=tokens.device)\\\n",
    "                        .unsqueeze(0).repeat(B, 1)\n",
    "            tokens = tokens + self.modality_type_embed(mod_ids)\n",
    "\n",
    "\n",
    "            tokens = self.pre_ln(tokens)\n",
    "\n",
    "            tokens = self.transformer(tokens)\n",
    "            pooled = tokens.mean(dim=1)  # mean-pool over 3 modalities\n",
    "            logits = self.classifier(pooled)\n",
    "\n",
    "        else:\n",
    "            concat = torch.cat([t_img, t_txt, t_aud], dim=1)\n",
    "            logits = self.classifier(concat)\n",
    "\n",
    "        return logits\n",
    "\n",
    "\n",
    "def load_unimodal_models(num_classes, image_model_path, text_model_path, audio_model_path, device, embed_dim=512):\n",
    "    img_model = EfficientNetV2_Embed(embed_dim=embed_dim, pretrained=True,version=\"s\")\n",
    "    if os.path.exists(image_model_path):\n",
    "        st = torch.load(image_model_path, map_location=device)\n",
    "        try:\n",
    "            if isinstance(st, dict):\n",
    "                candidate = None\n",
    "                for k in ('model','state_dict','model_state','model_state_dict'):\n",
    "                    if k in st and isinstance(st[k], dict):\n",
    "                        candidate = st[k]; break\n",
    "                if candidate is not None:\n",
    "                    st = candidate\n",
    "                st_clean = {k:v for k,v in st.items() if not (k.startswith(\"fc.\") or k.startswith(\"classifier.\") or ('fc' in k and k.endswith('weight')))}\n",
    "                img_model.load_state_dict(st_clean, strict=False)\n",
    "            else:\n",
    "                img_model.load_state_dict(st, strict=False)\n",
    "            print(\"[INFO] Loaded image checkpoint (partial load allowed).\")\n",
    "        except Exception as e:\n",
    "            print(\"[WARN] Failed to load image checkpoint cleanly:\", e)\n",
    "    img_model.to(device)\n",
    "\n",
    "    txt_model = BERTEncoder(BERT_MODEL_NAME, embed_dim=embed_dim)\n",
    "    if os.path.exists(text_model_path):\n",
    "        st = torch.load(text_model_path, map_location=device)\n",
    "        try:\n",
    "            if isinstance(st, dict):\n",
    "                candidate = None\n",
    "                for k in ('model','state_dict','model_state','model_state_dict'):\n",
    "                    if k in st and isinstance(st[k], dict):\n",
    "                        candidate = st[k]; break\n",
    "                if candidate is not None:\n",
    "                    st = candidate\n",
    "\n",
    "                st_no_classifier = {k:v for k,v in st.items() if not k.startswith(\"classifier.\")}\n",
    "                txt_model.load_state_dict(st_no_classifier, strict=False)\n",
    "            else:\n",
    "                txt_model.load_state_dict(st, strict=False)\n",
    "            print(\"[INFO] Loaded text checkpoint (partial load).\")\n",
    "        except Exception as e:\n",
    "            print(\"[WARN] Failed to load text checkpoint:\", e)\n",
    "    txt_model.to(device)\n",
    "\n",
    "    aud_model = TransformerLSTM_Embed(embed_dim=embed_dim)\n",
    "    if os.path.exists(audio_model_path):\n",
    "        st = torch.load(audio_model_path, map_location=device)\n",
    "        try:\n",
    "            if isinstance(st, dict):\n",
    "                candidate = None\n",
    "                for k in ('model','state_dict','model_state','model_state_dict'):\n",
    "                    if k in st and isinstance(st[k], dict):\n",
    "                        candidate = st[k]; break\n",
    "                if candidate is not None:\n",
    "                    st = candidate\n",
    "                aud_model.load_state_dict(st, strict=False)\n",
    "            else:\n",
    "                aud_model.load_state_dict(st, strict=False)\n",
    "            print(\"[INFO] Loaded audio checkpoint (partial load).\")\n",
    "        except Exception as e:\n",
    "            print(\"[WARN] Failed to load audio checkpoint:\", e)\n",
    "    aud_model.to(device)\n",
    "\n",
    "    return img_model, txt_model, aud_model\n",
    "\n",
    "\n",
    "def set_requires_grad(model, requires_grad: bool):\n",
    "    for p in model.parameters():\n",
    "        p.requires_grad = requires_grad\n",
    "\n",
    "def unfreeze_efficientnet(img_model, depth=2):\n",
    "  \n",
    "\n",
    "    for p in img_model.base.parameters():\n",
    "        p.requires_grad = False\n",
    "\n",
    "    stages = [\n",
    "        img_model.base.features[2],  # Stage 3\n",
    "        img_model.base.features[3],  # Stage 4\n",
    "        img_model.base.features[4],  # Stage 5\n",
    "        img_model.base.features[5],  # Stage 6\n",
    "    ]\n",
    "\n",
    "\n",
    "    for s in stages[-depth:]:\n",
    "        for p in s.parameters():\n",
    "            p.requires_grad = True\n",
    "\n",
    "\n",
    "    for p in img_model.proj.parameters():\n",
    "        p.requires_grad = True\n",
    "\n",
    "    print(f\"[INFO] Unfroze last {depth} EfficientNetV2 stages + projection layer.\")\n",
    "\n",
    "def freeze_bert_layers(bert_model: BertModel, unfreeze_last_n: int = 4):\n",
    "\n",
    "\n",
    "    for name, p in bert_model.named_parameters():\n",
    "        p.requires_grad = False\n",
    "\n",
    "    for name, p in bert_model.named_parameters():\n",
    "        if name.startswith(\"embeddings.\") or name.startswith(\"pooler.\") or \"LayerNorm\" in name or \"layer_norm\" in name:\n",
    "            p.requires_grad = True\n",
    "\n",
    "\n",
    "    try:\n",
    "        total = bert_model.config.num_hidden_layers\n",
    "        for i in range(total - unfreeze_last_n, total):\n",
    "            prefix = f\"encoder.layer.{i}.\"\n",
    "            for name, p in bert_model.named_parameters():\n",
    "                if name.startswith(prefix):\n",
    "                    p.requires_grad = True\n",
    "    except Exception:\n",
    "\n",
    "        for name, p in bert_model.named_parameters():\n",
    "            if \"encoder.layer\" in name and any(f\"encoder.layer.{j}.\" in name for j in range(max(0, total-unfreeze_last_n), total)):\n",
    "                p.requires_grad = True\n",
    "\n",
    "\n",
    "def evaluate_full_early(model_components, fusion_head, loader, device, class_names):\n",
    "    img_model, txt_model, aud_model = model_components\n",
    "    fusion_head.eval()\n",
    "    img_model.eval(); txt_model.eval(); aud_model.eval()\n",
    "    all_true = []\n",
    "    all_pred = []\n",
    "    all_probs = []\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            img = batch['image'].to(device)\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            audio = batch['audio'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "            e_img = img_model(img)\n",
    "            e_txt = txt_model(input_ids, attention_mask)\n",
    "            e_aud = aud_model(audio)\n",
    "            out = fusion_head(e_img, e_txt, e_aud)\n",
    "            probs = F.softmax(out, dim=1)\n",
    "            preds = probs.argmax(dim=1)\n",
    "            all_true.append(labels.cpu().numpy())\n",
    "            all_pred.append(preds.cpu().numpy())\n",
    "            all_probs.append(probs.cpu().numpy())\n",
    "    y_true = np.concatenate(all_true)\n",
    "    y_pred = np.concatenate(all_pred)\n",
    "    y_score = np.vstack(all_probs)\n",
    "    acc = accuracy_score(y_true, y_pred) * 100.0\n",
    "    print(f\"[EVAL] Accuracy: {acc:.2f}%\")\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=list(range(len(class_names))))\n",
    "    return acc, cm, y_true, y_score\n",
    "\n",
    "\n",
    "def validate_epoch_early(model_components, fusion_head, val_loader, device, num_classes):\n",
    "    img_model, txt_model, aud_model = model_components\n",
    "    fusion_head.eval()\n",
    "    img_model.eval(); txt_model.eval(); aud_model.eval()\n",
    "\n",
    "    preds_list = []\n",
    "    labels_list = []\n",
    "\n",
    "\n",
    "    use_amp = True\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            img = batch['image'].to(device)\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            audio = batch['audio'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "\n",
    "\n",
    "            with torch.amp.autocast('cuda',enabled=USE_AMP):\n",
    "                e_img = img_model(img)\n",
    "                e_txt = txt_model(input_ids, attention_mask)\n",
    "                e_aud = aud_model(audio)\n",
    "                out = fusion_head(e_img, e_txt, e_aud)\n",
    "\n",
    "            preds = out.argmax(dim=1)\n",
    "\n",
    "\n",
    "            preds_list.append(preds)\n",
    "            labels_list.append(labels)\n",
    "\n",
    "\n",
    "    y_pred = torch.cat(preds_list).cpu().numpy()\n",
    "    y_true = torch.cat(labels_list).cpu().numpy()\n",
    "\n",
    "\n",
    "    acc = (y_pred == y_true).mean() * 100.0\n",
    "\n",
    "\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=list(range(num_classes)))\n",
    "\n",
    "    return acc, cm, y_true, None\n",
    "\n",
    "\n",
    "\n",
    "def print_classification_metrics(y_true, y_pred, class_names):\n",
    "    print(\"\\n========== Classification Report ==========\" )\n",
    "    print(classification_report(y_true, y_pred, target_names=class_names, digits=4))\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='weighted')\n",
    "    print(\"Weighted Precision: {:.4f}\".format(precision))\n",
    "    print(\"Weighted Recall:    {:.4f}\".format(recall))\n",
    "    print(\"Weighted F1-score:  {:.4f}\".format(f1))\n",
    "    print(\"===========================================\\n\")\n",
    "\n",
    "def visualize_pso_3d(particles, best_particle=None, metric_names=(\"Accuracy\", \"Precision\", \"Recall\")):\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "    particles = np.array(particles)\n",
    "    fig = plt.figure(figsize=(10, 7))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    if particles.size == 0:\n",
    "        print(\"[PSO VIS] no particles to display\")\n",
    "        return\n",
    "    ax.scatter(particles[:, 0], particles[:, 1], particles[:, 2], s=65, alpha=0.7)\n",
    "    if best_particle is not None:\n",
    "        best_particle = np.array(best_particle)\n",
    "        ax.scatter(best_particle[0], best_particle[1], best_particle[2],\n",
    "                   s=250, color='red', edgecolor='black', marker='o', label=\"Best Particle\")\n",
    "        ax.legend()\n",
    "    ax.set_xlabel(metric_names[0])\n",
    "    ax.set_ylabel(metric_names[1])\n",
    "    ax.set_zlabel(metric_names[2])\n",
    "    ax.set_title(\"3D PSO Per-Iteration Optimization\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def run_simple_pso_evaluate(model_components, train_loader, val_loader, class_names,\n",
    "                            swarm_size=6, iters=3, device=DEVICE, quick_batches=8):\n",
    "    print(f\"[PSO] Starting simple PSO-like search: swarm={swarm_size}, iters={iters}\")\n",
    "    img_model, txt_model, aud_model = model_components\n",
    "    all_iterations_metrics = []\n",
    "\n",
    "    particles = []\n",
    "    velocities = []\n",
    "    for _ in range(swarm_size):\n",
    "        lr = 10 ** np.random.uniform(-5, -2)\n",
    "        dropout = np.random.uniform(0.0, 0.5)\n",
    "        d_ratio = np.random.uniform(0.5, 1.0)\n",
    "        particles.append([lr, dropout, d_ratio])\n",
    "        velocities.append([0.0, 0.0, 0.0])\n",
    "    pbest = particles.copy()\n",
    "    pbest_scores = [-1.0] * swarm_size\n",
    "    gbest = None\n",
    "    gbest_score = -1.0\n",
    "    for it in range(iters):\n",
    "        print(f\"[PSO] Iteration {it+1}/{iters}\")\n",
    "        iter_metrics = []\n",
    "        best_particle_metric = None\n",
    "        best_particle_acc = -1\n",
    "        for i, p in enumerate(particles):\n",
    "            lr, dropout, d_ratio = p\n",
    "            d_model = int(512 * float(d_ratio))\n",
    "            d_model = max(4, int(d_model // 4) * 4)\n",
    "\n",
    "            nhead = 4\n",
    "            if d_model % 8 == 0:\n",
    "                nhead = 8\n",
    "            elif d_model % 4 == 0:\n",
    "                nhead = 4\n",
    "            else:\n",
    "                nhead = 1\n",
    "            fusion_head = EarlyFusion(num_classes=len(class_names), embed_dim=512, d_model=d_model,\n",
    "                                      use_transformer=True, nhead=nhead, n_layers=1, dropout=dropout).to(device)\n",
    "            opt = torch.optim.AdamW(fusion_head.parameters(), lr=lr, weight_decay=WEIGHT_DECAY)\n",
    "            criterion = nn.CrossEntropyLoss()\n",
    "            fusion_head.train()\n",
    "            img_model.eval(); txt_model.eval(); aud_model.eval()\n",
    "            batch_iter = iter(train_loader)\n",
    "\n",
    "            for b_idx in range(quick_batches):\n",
    "                try:\n",
    "                    batch = next(batch_iter)\n",
    "                except StopIteration:\n",
    "                    batch_iter = iter(train_loader)\n",
    "                    batch = next(batch_iter)\n",
    "                img = batch['image'].to(device)\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                audio = batch['audio'].to(device)\n",
    "                labels = batch['label'].to(device)\n",
    "                opt.zero_grad()\n",
    "\n",
    "                e_img = img_model(img)\n",
    "                e_txt = txt_model(input_ids, attention_mask)\n",
    "                e_aud = aud_model(audio)\n",
    "                out = fusion_head(e_img, e_txt, e_aud)\n",
    "                loss = criterion(out, labels)\n",
    "                loss.backward()\n",
    "                opt.step()\n",
    "            val_acc, _, y_true, y_score = evaluate_full_early(\n",
    "                (img_model, txt_model, aud_model),\n",
    "                fusion_head, val_loader, device, class_names\n",
    "            )\n",
    "            y_pred = np.argmax(y_score, axis=1)\n",
    "            precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "                y_true, y_pred, average='macro', zero_division=0\n",
    "            )\n",
    "            print(f\"[PSO] Particle {i} -> Acc: {val_acc:.2f} Prec: {precision:.4f} Rec: {recall:.4f}\")\n",
    "            metric_vec = [val_acc, precision*100, recall*100]\n",
    "            iter_metrics.append(metric_vec)\n",
    "            if val_acc > best_particle_acc:\n",
    "                best_particle_acc = val_acc\n",
    "                best_particle_metric = metric_vec\n",
    "            if val_acc > pbest_scores[i]:\n",
    "                pbest_scores[i] = val_acc\n",
    "                pbest[i] = p\n",
    "            if val_acc > gbest_score:\n",
    "                gbest_score = val_acc\n",
    "                gbest = p\n",
    "        all_iterations_metrics.append(iter_metrics)\n",
    "        print(f\"[PSO] Visualizing iteration {it+1}\")\n",
    "        visualize_pso_3d(particles=iter_metrics, best_particle=best_particle_metric,\n",
    "                         metric_names=(\"Val Acc (%)\", \"Precision (%)\", \"Recall (%)\"))\n",
    "\n",
    "        for i in range(swarm_size):\n",
    "            inertia = 0.5\n",
    "            cognitive = 0.8\n",
    "            social = 0.9\n",
    "            r1 = np.random.rand(3)\n",
    "            r2 = np.random.rand(3)\n",
    "            v = np.array(velocities[i])\n",
    "            pb = np.array(pbest[i])\n",
    "            gb = np.array(gbest) if gbest is not None else np.array(particles[i])\n",
    "            pos = np.array(particles[i])\n",
    "            v = inertia * v + cognitive * r1 * (pb - pos) + social * r2 * (gb - pos)\n",
    "            new_pos = pos + v\n",
    "            particles[i] = [\n",
    "                float(np.clip(new_pos[0], 1e-6, 1e-1)),\n",
    "                float(np.clip(new_pos[1], 0.0, 0.8)),\n",
    "                float(np.clip(new_pos[2], 0.3, 1.2))\n",
    "            ]\n",
    "            velocities[i] = v.tolist()\n",
    "    print(f\"[PSO] Done. Best acc found: {gbest_score:.2f} (particle hyperparams {gbest})\")\n",
    "    return all_iterations_metrics\n",
    "\n",
    "def train_early_fusion(csv_path=CSV_PATH, image_model_path=IMAGE_MODEL_PATH,\n",
    "                 text_model_path=TEXT_MODEL_PATH, audio_model_path=AUDIO_MODEL_PATH,\n",
    "                 freeze_bases=FREEZE_BASE_MODELS, device=DEVICE,\n",
    "                 run_pso=False, preload_dataset: bool = True):\n",
    "    if not os.path.exists(csv_path):\n",
    "        raise FileNotFoundError(f\"CSV mapping not found: {csv_path}\")\n",
    "    df = pd.read_csv(csv_path)\n",
    "    required_cols = {'image_path', 'text', 'audio_path', 'label'}\n",
    "    if not required_cols.issubset(set(df.columns)):\n",
    "        raise RuntimeError(f\"CSV must have columns: {required_cols}\")\n",
    "    # label encoding\n",
    "    if df['label'].dtype == object:\n",
    "        le = LabelEncoder()\n",
    "        df['label'] = le.fit_transform(df['label'].astype(str))\n",
    "        class_names = list(le.classes_)\n",
    "    else:\n",
    "        le = None\n",
    "        class_names = sorted(df['label'].unique().tolist())\n",
    "        class_names = [str(int(x)) for x in class_names]\n",
    "    num_classes = int(df['label'].nunique())\n",
    "    print(f\"Found {len(df)} paired samples. Classes: {num_classes} -> {class_names}\")\n",
    "    idxs = list(range(len(df)))\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    train_idx, val_idx = train_test_split(idxs, test_size=0.2, random_state=SEED, stratify=df['label'])\n",
    "    train_df = df.loc[train_idx].reset_index(drop=True)\n",
    "    val_df = df.loc[val_idx].reset_index(drop=True)\n",
    "    tokenizer = BertTokenizer.from_pretrained(BERT_MODEL_NAME, use_fast=True)\n",
    "    img_transform = transforms.Compose([\n",
    "        transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])\n",
    "    ])\n",
    "\n",
    "    train_ds = MultimodalDataset(train_df, tokenizer, le, img_transform, preload=preload_dataset)\n",
    "    val_ds   = MultimodalDataset(val_df, tokenizer, le, img_transform, preload=preload_dataset)\n",
    "\n",
    "\n",
    "\n",
    "    pin_mem = True if device.type == 'cuda' else False\n",
    "    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, pin_memory=pin_mem)\n",
    "    val_loader   = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, pin_memory=pin_mem)\n",
    "\n",
    "    EMBED_DIM = 512\n",
    "    img_model, txt_model, aud_model = load_unimodal_models(num_classes, image_model_path, text_model_path, audio_model_path, device, embed_dim=EMBED_DIM)\n",
    "\n",
    "    if FREEZE_BASE_MODELS:\n",
    "\n",
    "\n",
    "        set_requires_grad(img_model, False)\n",
    "        set_requires_grad(txt_model, False)\n",
    "        set_requires_grad(aud_model, False)\n",
    "\n",
    "\n",
    "        for p in img_model.base.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "        # EfficientNetV2 stages (features[2]–[5])\n",
    "        stages = [\n",
    "            img_model.base.features[2],  # Stage 3\n",
    "            img_model.base.features[3],  # Stage 4\n",
    "            img_model.base.features[4],  # Stage 5\n",
    "            img_model.base.features[5],  # Stage 6\n",
    "        ]\n",
    "\n",
    "        # unfreeze last 2 stages: Stage 5 & 6\n",
    "        for s in stages[-2:]:\n",
    "            for p in s.parameters():\n",
    "                p.requires_grad = True\n",
    "\n",
    "        # projection head always trainable\n",
    "        set_requires_grad(img_model.proj, True)\n",
    "\n",
    "        print(\"[INFO] Unfroze EfficientNetV2 last 2 stages + projection.\")\n",
    "\n",
    "        for name, p in aud_model.named_parameters():\n",
    "            if \"transformer.layers.1\" in name or \"lstm\" in name:\n",
    "                p.requires_grad = True\n",
    "\n",
    "        # audio projection always trainable\n",
    "        set_requires_grad(aud_model.proj, True)\n",
    "\n",
    "        print(\"[INFO] Unfroze audio last transformer layer + LSTM + projection.\")\n",
    "\n",
    "\n",
    "        freeze_bert_layers(txt_model.bert, UNFREEZE_BERT_LAST_N)\n",
    "        print(f\"[INFO] Unfroze last {UNFREEZE_BERT_LAST_N} BERT layers.\")\n",
    "\n",
    "    else:\n",
    "        # No freezing at all\n",
    "        set_requires_grad(img_model, True)\n",
    "        set_requires_grad(txt_model, True)\n",
    "        set_requires_grad(aud_model, True)\n",
    "        print(\"[INFO] All encoders fully trainable.\")\n",
    "\n",
    "    fusion_head = EarlyFusion(num_classes, embed_dim=EMBED_DIM, d_model=512, use_transformer=True, nhead=8, n_layers=1, dropout=0.2).to(device)   \n",
    "    fusion_params = list(fusion_head.parameters())\n",
    "    encoder_params = []\n",
    "    for m in (img_model, txt_model, aud_model):\n",
    "        for p in m.parameters():\n",
    "            if p.requires_grad:\n",
    "                encoder_params.append(p)\n",
    "    param_groups = [\n",
    "        {\"params\": fusion_params, \"lr\": LR_FUSION, \"weight_decay\": WEIGHT_DECAY},\n",
    "    ]\n",
    "    if len(encoder_params) > 0:\n",
    "        param_groups.append({\"params\": encoder_params, \"lr\": LR_ENCODER, \"weight_decay\": WEIGHT_DECAY})\n",
    "\n",
    "    optimizer = torch.optim.AdamW([\n",
    "    {\"params\": img_model.parameters(),   \"lr\": 1e-4, \"weight_decay\": 0.01},   # EfficientNetV2 (partial unfreeze)\n",
    "    {\"params\": txt_model.parameters(),  \"lr\": 6e-5, \"weight_decay\": 0.01},   # BERT (last few layers unfrozen)\n",
    "    {\"params\": aud_model.parameters(), \"lr\": 1e-4, \"weight_decay\": 0.0},    # Audio model\n",
    "    {\"params\": fusion_head.parameters(), \"lr\": 5e-4, \"weight_decay\": 0.0},    # Fusion layers\n",
    "])\n",
    "    total_steps = int(EPOCHS * len(train_loader))\n",
    "    warmup_steps = max(1, int(0.1 * total_steps))\n",
    "    def lr_lambda(current_step):\n",
    "        if current_step < warmup_steps:\n",
    "            return float(current_step) / float(max(1, warmup_steps))\n",
    "        return max(0.0, float(total_steps - current_step) / float(max(1, total_steps - warmup_steps)))\n",
    "    scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    best_val_acc = -1.0\n",
    "    best_state = None\n",
    "    train_accs, val_accs = [], []\n",
    "\n",
    "    scaler = torch.cuda.amp.GradScaler(enabled=USE_AMP)\n",
    "\n",
    "    for epoch in range(1, EPOCHS+1):\n",
    "        t0 = time.time()\n",
    "        fusion_head.train()\n",
    "        img_model.train(); txt_model.train(); aud_model.train()\n",
    "        running_correct = 0\n",
    "        running_total = 0\n",
    "        pbar = tqdm(train_loader, desc=f\"Epoch {epoch}/{EPOCHS}\", leave=False)\n",
    "        for batch in pbar:\n",
    "            img = batch['image'].to(device)\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            audio = batch['audio'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            with torch.amp.autocast('cuda',enabled=USE_AMP):\n",
    "                e_img = img_model(img)\n",
    "                e_txt = txt_model(input_ids, attention_mask)\n",
    "                e_aud = aud_model(audio)\n",
    "                out = fusion_head(e_img, e_txt, e_aud)\n",
    "                loss = criterion(out, labels)\n",
    "            \n",
    "            if USE_AMP:\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.unscale_(optimizer)\n",
    "                torch.nn.utils.clip_grad_norm_( [p for g in optimizer.param_groups for p in g['params'] if p.requires_grad], GRAD_CLIP_NORM)\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "            else:\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_( [p for g in optimizer.param_groups for p in g['params'] if p.requires_grad], GRAD_CLIP_NORM)\n",
    "                optimizer.step()\n",
    "\n",
    "        \n",
    "\n",
    "            preds = out.argmax(dim=1)\n",
    "            running_total += labels.size(0)\n",
    "            running_correct += (preds == labels).sum().item()\n",
    "            train_acc = 100.0 * running_correct / running_total\n",
    "            pbar.set_postfix({\"TrainAcc\": f\"{train_acc:.2f}%\", \"Loss\": f\"{loss.item():.4f}\"})\n",
    "        scheduler.step()\n",
    "        train_accs.append(train_acc)\n",
    "        val_acc, cm, y_true_val, y_score_val = validate_epoch_early((img_model, txt_model, aud_model), fusion_head, val_loader, device, num_classes)\n",
    "        val_accs.append(val_acc)\n",
    "        print(f\"Epoch {epoch}/{EPOCHS} - Train Acc: {train_acc:.2f}% | Val Acc: {val_acc:.2f}% | Time: {(time.time()-t0):.1f}s\")\n",
    "\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            best_state = {\n",
    "                \"epoch\": epoch,\n",
    "                \"fusion_state\": copy.deepcopy(fusion_head.state_dict()),\n",
    "                \"img_state\": copy.deepcopy(img_model.state_dict()),\n",
    "                \"txt_state\": copy.deepcopy(txt_model.state_dict()),\n",
    "                \"aud_state\": copy.deepcopy(aud_model.state_dict()),\n",
    "                \"val_acc\": val_acc,\n",
    "                \"class_names\": class_names\n",
    "            }\n",
    "            torch.save(best_state, BEST_FUSION_PATH)\n",
    "            print(f\" ✅ Best early-fusion model saved -> {BEST_FUSION_PATH} (Val Acc {val_acc:.2f}%)\")\n",
    " \n",
    "    plot_accuracy(train_accs, val_accs)\n",
    "    if best_state is not None:\n",
    "        print(f\"Loaded best early fusion (epoch {best_state['epoch']}, val acc {best_state['val_acc']:.2f}%)\")\n",
    "        fusion_head.load_state_dict(best_state['fusion_state'])\n",
    "        img_model.load_state_dict(best_state['img_state'], strict=False)\n",
    "        txt_model.load_state_dict(best_state['txt_state'], strict=False)\n",
    "        aud_model.load_state_dict(best_state['aud_state'], strict=False)\n",
    "\n",
    "    final_acc, final_cm, y_true, y_score = evaluate_full_early((img_model, txt_model, aud_model), fusion_head, val_loader, device, class_names)\n",
    "    print_classification_metrics(y_true, np.argmax(y_score, axis=1), class_names)\n",
    "    plot_confusion(final_cm, class_names)\n",
    "    try:\n",
    "        plot_multiclass_roc(y_true, y_score, class_names)\n",
    "    except Exception as e:\n",
    "        print(\"ROC plot error:\", e)\n",
    "\n",
    "    if run_pso:\n",
    "        PSO_SWARM = 6\n",
    "        PSO_ITERS = 3\n",
    "        QUICK_BATCHES = 6\n",
    "        particle_metrics = run_simple_pso_evaluate((img_model, txt_model, aud_model),\n",
    "                                                   train_loader, val_loader, class_names,\n",
    "                                                   swarm_size=PSO_SWARM, iters=PSO_ITERS, device=device,\n",
    "                                                   quick_batches=QUICK_BATCHES)\n",
    "        \n",
    "        flattened = [m for iter_batch in particle_metrics for m in iter_batch]\n",
    "        visualize_pso_3d(flattened, metric_names=(\"Val Acc (%)\", \"Precision (pct)\", \"Recall (pct)\"))\n",
    "\n",
    "    print(\"[DONE]\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Starting Early Fusion Training (fixed + preload)\")\n",
    "    \n",
    "    train_early_fusion(run_pso=True, preload_dataset=True)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
